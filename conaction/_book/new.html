<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; New Instantiations of The Trinity of Covariation – ConAction, Second Edition.</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./software.html" rel="next">
<link href="./previous.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./new.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">New Instantiations of The Trinity of Covariation</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">ConAction, Second Edition.</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefaces</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./abstract.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Abstract</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./previous.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Previous Instantiations of the Trinity of Covariation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./new.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">New Instantiations of The Trinity of Covariation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">ConAction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mtn-pine-beetle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Correlational Sufficiency of an Isolation by Distance Model of Mountain Pine Beetle in Western North America</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./discussion_conclusions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Discussion and Conclusions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./citation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Citation</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#multilinear-correlation" id="toc-multilinear-correlation" class="nav-link active" data-scroll-target="#multilinear-correlation"><span class="header-section-number">4.1</span> Multilinear Correlation</a>
  <ul class="collapse">
  <li><a href="#derivations" id="toc-derivations" class="nav-link" data-scroll-target="#derivations"><span class="header-section-number">4.1.1</span> Derivations</a></li>
  </ul></li>
  <li><a href="#interpretation" id="toc-interpretation" class="nav-link" data-scroll-target="#interpretation"><span class="header-section-number">4.2</span> Interpretation</a></li>
  <li><a href="#nightingale-correlation" id="toc-nightingale-correlation" class="nav-link" data-scroll-target="#nightingale-correlation"><span class="header-section-number">4.3</span> Nightingale Correlation</a>
  <ul class="collapse">
  <li><a href="#derivations-1" id="toc-derivations-1" class="nav-link" data-scroll-target="#derivations-1"><span class="header-section-number">4.3.1</span> Derivations</a></li>
  <li><a href="#interpretation-1" id="toc-interpretation-1" class="nav-link" data-scroll-target="#interpretation-1"><span class="header-section-number">4.3.2</span> Interpretation</a></li>
  </ul></li>
  <li><a href="#inner-correlation" id="toc-inner-correlation" class="nav-link" data-scroll-target="#inner-correlation"><span class="header-section-number">4.4</span> Inner Correlation</a>
  <ul class="collapse">
  <li><a href="#misiak-correlation" id="toc-misiak-correlation" class="nav-link" data-scroll-target="#misiak-correlation"><span class="header-section-number">4.4.1</span> Misiak Correlation</a></li>
  <li><a href="#trenevski-maleski-correlation" id="toc-trenevski-maleski-correlation" class="nav-link" data-scroll-target="#trenevski-maleski-correlation"><span class="header-section-number">4.4.2</span> Trenevski-Maleski Correlation</a></li>
  <li><a href="#interpretation-2" id="toc-interpretation-2" class="nav-link" data-scroll-target="#interpretation-2"><span class="header-section-number">4.4.3</span> Interpretation</a></li>
  </ul></li>
  <li><a href="#agnesian-operators" id="toc-agnesian-operators" class="nav-link" data-scroll-target="#agnesian-operators"><span class="header-section-number">4.5</span> Agnesian Operators</a>
  <ul class="collapse">
  <li><a href="#interpretation-3" id="toc-interpretation-3" class="nav-link" data-scroll-target="#interpretation-3"><span class="header-section-number">4.5.1</span> Interpretation</a></li>
  </ul></li>
  <li><a href="#grade-entropies" id="toc-grade-entropies" class="nav-link" data-scroll-target="#grade-entropies"><span class="header-section-number">4.6</span> Grade Entropies</a>
  <ul class="collapse">
  <li><a href="#entropy" id="toc-entropy" class="nav-link" data-scroll-target="#entropy"><span class="header-section-number">4.6.1</span> Entropy</a></li>
  <li><a href="#grades-and-partially-ordered-sets" id="toc-grades-and-partially-ordered-sets" class="nav-link" data-scroll-target="#grades-and-partially-ordered-sets"><span class="header-section-number">4.6.2</span> Grades and Partially-Ordered Sets</a></li>
  <li><a href="#graded-posets" id="toc-graded-posets" class="nav-link" data-scroll-target="#graded-posets"><span class="header-section-number">4.6.3</span> Graded Posets</a></li>
  <li><a href="#grade-entropies-1" id="toc-grade-entropies-1" class="nav-link" data-scroll-target="#grade-entropies-1"><span class="header-section-number">4.6.4</span> Grade Entropies</a></li>
  <li><a href="#interpretation-4" id="toc-interpretation-4" class="nav-link" data-scroll-target="#interpretation-4"><span class="header-section-number">4.6.5</span> Interpretation</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">4.7</span> Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">New Instantiations of The Trinity of Covariation</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This chapter introduces new instantiations of the metaphor of <em>The Trinity of Covariation</em>.</p>
<section id="multilinear-correlation" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="multilinear-correlation"><span class="header-section-number">4.1</span> Multilinear Correlation</h2>
<section id="derivations" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="derivations"><span class="header-section-number">4.1.1</span> Derivations</h3>
<p>The covariance of two random variables is conventionally defined as their centered mixed product moment. Just as the covariance in the standard sense captures something about simultaneous changes in two random variables, we explored generalizing this notion to multiple variables beyond two. We decided that a natural way to accomplish this was to compute the <em>centered mixed product moment</em> of a collection of random variables. To emphasize in this work that we are focusing on a generalization of covariance, which is bilinear, we will refer to this generalization as <em>multilinear covariance</em> as given in the following definition.</p>
<blockquote class="blockquote">
<p><strong>Definition</strong></p>
<p>Given a finite collection of real-valued random variables <span class="math inline">\(\{X_1, \cdots, X_n\}\)</span>, their <strong>multilinear covariance</strong> is defined to be the following. <span class="math display">\[\text{Cov}\left[ X_1, \cdots, X_n \right] \triangleq \mathbb{E}\left[ \prod_{j=1}^{n} \left(X_j - \mathbb{E}[X_j] \right) \right]\]</span></p>
</blockquote>
<p>This definition in standard literature bears names such as <em>centered mixed product moment</em> and <em>centered cross-product moment</em>, which are both comparably verbose to <em>multilinear covariance</em>. The qualifier <em>cross-product</em> can confuse readers into considering vector cross-products if clarifications are not given, while the former is technically accurate but communicates little intuition about what is being quantified.</p>
<p>Bilinearity is a property of covariance in the standard sense, and we wished to have multilinearity in the generalized sense. However, just because we gave a function the adjective “multilinear” does not make it multilinear in the desired sense of a multilinear map. See Definition <span class="math inline">\(\ref{def:multilinearmap}\)</span> for a definition of <em>multilinear</em> as given by <span class="citation" data-cites="Greub1978">Greub (<a href="references.html#ref-Greub1978" role="doc-biblioref">1978</a>)</span>.</p>
<blockquote class="blockquote">
<p><strong>Definition</strong> (<span class="citation" data-cites="Greub1978">Greub (<a href="references.html#ref-Greub1978" role="doc-biblioref">1978</a>)</span>)</p>
<p>Let <span class="math inline">\(E_1, \cdots, E_{p}\)</span> be a collection of vector spaces, and also let <span class="math inline">\(G\)</span> be a vector space. A map <span class="math inline">\(\phi: E_1 \times \cdots \times E_p \mapsto G\)</span> is called <strong><span class="math inline">\(p\)</span>-linear</strong> if for every <span class="math inline">\(i \in \{1, \cdots, p \}\)</span> and scalars <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> it holds that <span class="math display">\[\begin{align*}
\phi (x_1, \cdots, x_{i-1}, \alpha x_i + \beta y_i, x_{i+1}, \cdots, x_p) &amp;= \alpha \phi (x_1, \cdots, x_{i-1}, x_i, x_{i+1}, \cdots, x_p) \\
&amp;  + \beta \phi (x_1, \cdots, x_{i-1}, y_i, x_{i+1}, \cdots, x_p).
\end{align*}\]</span></p>
</blockquote>
<p>In the following proposition we claim and prove that multilinear covariance is multilinear in a mathematical sense. We took an approach of showing that the multilinear covariance of linear combinations of random variables is itself a linear combination of the multilinear covariances on those random variables.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Screen Size
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>You may need to view the following result on a larger screen to see it in its entirety.</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Proposition</strong></p>
<p><span class="math display">\[\text{Cov}\left[ \sum_{w_{1}=1}^{K_{1}} a_{w_{1}} U_{w_{1}}, \cdots, \sum_{w_{j}=1}^{K_{j}} a_{w_{j}} U_{w_{j}}, \cdots,  \sum_{w_{n}=1}^{K_{n}} a_{w_{n}} U_{w_{n}} \right]\]</span></p>
<p><span class="math display">\[=\]</span></p>
<p><span class="math display">\[\sum_{w_1=1}^{K_1} \cdots \sum_{w_j=1}^{K_j} \cdots \sum_{w_n=1}^{K_n} \left(\prod_{j=1}^{n} a_{w_j} \right) \text{Cov}\left[U_{w_1}, \cdots, U_{w_j}, \cdots, U_{w_n} \right]\]</span></p>
<p><strong>Proof</strong></p>
<p>Starting with the definition of multilinear covariance, let <span class="math inline">\(X_{j} = \sum_{w_{j}}^{K_{j}} a_{w_j}U_{w_j}\)</span> for each <span class="math inline">\(j \in \{1, \cdots, n \}\)</span>. <span class="math display">\[\begin{align*}
\text{Cov} \left[ \sum_{w_1=1}^{K_1} a_{w_1}U_{w_1}, \cdots, \sum_{w_{j}=1}^{K_{j}} a_{w_{j}} U_{w_{j}}, \cdots,  \sum_{w_n=1}^{K_n} a_{w_n}U_{w_n}\right] &amp;= \mathbb{E}\left[ \prod_{j=1}^{n} \left( \sum_{w_{j}=1}^{K_{j}} a_{w_{j}} U_{w_{j}} - \mathbb{E}\left[ \sum_{w_{j}=1}^{K_{j}} a_{w_{j}} U_{w_{j}} \right]  \right) \right] \\
&amp; = \mathbb{E}\left[ \prod_{j=1}^{n} \left( \sum_{w_{j}=1}^{K_{j}} a_{w_{j}} U_{w_{j}} - \sum_{w_{j}=1}^{K_{j}} \mathbb{E}\left[ a_{w_{j}} U_{w_{j}} \right]  \right) \right] \\
&amp; = \mathbb{E} \left[ \sum_{w_1}^{K_1} \cdots \sum_{w_j}^{K_j} \cdots \sum_{w_n}^{K_n} \prod_{j=1}^{n} \left( a_{w_j}U_{w_j} - \mathbb{E}\left[ a_{w_j}U_{w_j} \right] \right) \right] \\
&amp; = \sum_{w_1}^{K_1} \cdots \sum_{w_j}^{K_j} \cdots \sum_{w_n}^{K_n} \mathbb{E} \left[ \prod_{j=1}^{n} \left( a_{w_j}U_{w_j} - \mathbb{E}\left[ a_{w_j}U_{w_j} \right] \right) \right] \\
&amp; = \sum_{w_1}^{K_1} \cdots \sum_{w_j}^{K_j} \cdots \sum_{w_n}^{K_n} \mathbb{E} \left[ \prod_{j=1}^{n} a_{w_j} \left( U_{w_j} - \mathbb{E}\left[ U_{w_j} \right] \right) \right] \\
&amp; = \sum_{w_1}^{K_1} \cdots \sum_{w_j}^{K_j} \cdots \sum_{w_n}^{K_n} \left( \prod_{j=1}^{n} a_{w_j} \right) \mathbb{E} \left[ \prod_{j=1}^{n} \left( U_{w_j} - \mathbb{E}\left[ U_{w_j} \right] \right) \right] \\
&amp; = \sum_{w_1}^{K_1} \cdots \sum_{w_j}^{K_j} \cdots \sum_{w_n}^{K_n} \left( \prod_{j=1}^{n} a_{w_j} \right) \text{Cov} \left[ U_{w_1}, \cdots,  U_{w_j}, \cdots, U_{w_n} \right]
\end{align*}\]</span> <span class="math inline">\(\blacksquare\)</span></p>
</blockquote>
<p>Multilinear covariance generalizes covariance from a bilinear comparison of two random variables to a multilinear comparison of finitely-many random variables. This generalizes the numerator of Pearson’s product-moment correlation coefficient, which leaves finding a generalization of its denominator. Knowing that the denominator of Pearson’s correlation is the product of the standard deviations of the respective two variables compared in the numerator, such a generalization of the denominator should reduce to the product of the standard deviations in the case of only two variables. Another particularly useful property of Pearson’s correlation is its normalization onto the interval <span class="math inline">\([-1,1]\)</span> using the Cauchy-Schwartz inequality. Combining these properties into a goal, we desired to find an inequality that generalizes the Cauchy-Schwartz inequality in such a way that normalizes the multilinear covariance onto <span class="math inline">\([-1,1]\)</span>.</p>
<p><span class="citation" data-cites="Nantomah2017">Nantomah (<a href="references.html#ref-Nantomah2017" role="doc-biblioref">2017</a>)</span> provides such a generalization of the Cauchy-Schwarz inequality in the form of the <em>generalized Hölder’s inequality for sums</em> (see proposition below). They accomplish this by generalizing the absolute value of the inner product of two vectors to the sum-aggregated element-wise product of a collection of vectors, and by generalizing the product of 2-norms to the product of <span class="math inline">\(p\)</span>-norms under specific constraints. The constraints on the orders of the norms are that individually they are strictly greater than unity, and that the sum of their reciprocals equals unity.</p>
<blockquote class="blockquote">
<p><strong>Proposition</strong> (<span class="citation" data-cites="Nantomah2017">Nantomah (<a href="references.html#ref-Nantomah2017" role="doc-biblioref">2017</a>)</span>)</p>
<p>Given <span class="math inline">\(Q_{i,j} \in \mathbb{R}\ \forall i,j\)</span> where <span class="math inline">\(i \in \{ 1, \cdots, m \}\)</span> and <span class="math inline">\(j \in \{ 1, \cdots, n \}\)</span> then</p>
<p><span class="math display">\[\sum_{i=1}^{m} \left|\prod_{j=1}^{n} Q_{i,j}\right| \leq \prod_{j=1}^{n} \left( \sum_{i=1}^{m} |Q_{i,j}|^{\alpha_j} \right)^{\frac{1}{\alpha_j}}\]</span></p>
<p>provided that <span class="math inline">\(\alpha_j &gt; 1 \forall j\)</span> and that <span class="math inline">\(\sum_{j=1}^{n} \frac{1}{\alpha_j} = 1\)</span>.</p>
</blockquote>
<p>Before returning to the multilinear covariance, let us consider that a new correlation coefficient can be defined from the generalized H"older’s inequality for sums by defining a quotient of the left-hand-side of the inequality divided by the right-hand-side. This gives a function bounded to <span class="math inline">\([0,1]\)</span>. Taking such a ratio requires that none of the data vectors are the zero vector. Then by dropping the absolute value in the numerator we obtain a function bounded to <span class="math inline">\([-1,1]\)</span>. Lastly, under suitable assumptions such as integrability and the existence of a moments, we write this function using expectations as given in Definition <span class="math inline">\(\ref{def:holdercorrelation}\)</span>.</p>
<blockquote class="blockquote">
<p><strong>Definition</strong></p>
<p>Given a finite collection of real-valued random variables <span class="math inline">\(\{X_1, \cdots, X_n\}\)</span>, and <span class="math inline">\(\alpha_j \in \mathbb{R}_{&gt;1}\)</span>, their <strong>Hölder’s correlation coefficient</strong> is given by</p>
<p><span class="math display">\[\mathfrak{H} \left[ X_1, \cdots, X_n \right](\vec \alpha) \triangleq \frac{\mathbb{E} \left[ \prod_{j=1}^{n} X_j \right]}{\prod_{j=1}^{n} \sqrt[\alpha_j]{\mathbb{E} \left[ |X_j|^{\alpha_j} \right]}}\]</span></p>
<p>provided that <span class="math inline">\(\sum_{j=1}^{n} \frac{1}{\alpha_j} = 1\)</span> and that the desired moments exist.</p>
</blockquote>
<p>While H"older’s correlation coefficient as given in Definition <span class="math inline">\(\ref{def:holdercorrelation}\)</span> can be estimated from data, its primary use in this thesis is to provide a definition from which other more familiar correlation coefficients can be generalized. We begin by generalizing the reflective correlation coefficient from the bilinear case to the multilinear case.\</p>
<p>The reflective correlation coefficient is the same in mathematical form as the Pearson product-moment correlation coefficient except that only uncentered moments are used. By setting <span class="math inline">\(\alpha_j = n\)</span> for each <span class="math inline">\(j \in \{1, \cdots, n \}\)</span> we obtain a special case of Definition <span class="math inline">\(\ref{def:holdercorrelation}\)</span> which is also a multilinear generalizaton of the reflective correlation coefficient (Definition <span class="math inline">\(\ref{def:reflectivemultilinearcorrelation}\)</span>). This can be easily verified by setting <span class="math inline">\(n=2\)</span> and comparing to the reflective correlation coefficient, and it also satisfies the constraints set on <span class="math inline">\(\alpha_j\)</span> in Proposition <span class="math inline">\(\ref{thm:generalizedholdersinequalitysums}\)</span>.</p>
<blockquote class="blockquote">
<p><strong>Definition</strong></p>
<p>Given a finite collection of real-valued random variables <span class="math inline">\(\{X_1, \cdots, X_n\}\)</span>, and <span class="math inline">\(\alpha_j \in \mathbb{R}_{&gt;1}\)</span>, their <strong>multilinear reflective correlation coefficient</strong> is given by</p>
<p><span class="math display">\[R_{\text{reflective}} \left[ X_1, \cdots, X_n \right] \triangleq \frac{\mathbb{E} \left[ \prod_{j=1}^{n} X_j \right]}{\prod_{j=1}^{n} \sqrt[n]{\mathbb{E} \left[ |X_j|^{n} \right]}}\]</span></p>
<p>provided that the desired moments exist.</p>
</blockquote>
<p>Before generalizing the Pearson correlation coefficient, we introduce a definition that generalizes the standard deviation. Just as the standard deviation can be thought of as the 2-norm of the errors from the mean, we consider the <span class="math inline">\(p\)</span>-norm of the errors from the mean to be the as given in Definition <span class="math inline">\(\ref{def:minkowskideviation}\)</span>. The Nightingale deviation of order <span class="math inline">\(p\)</span> is a special case of the the power mean (sometimes called the generalized mean) (<span class="citation" data-cites="Sykora2009">Sykora (<a href="references.html#ref-Sykora2009" role="doc-biblioref">2009</a>)</span>), but it first centers the random variables and takes an absolute value.\</p>
<blockquote class="blockquote">
<p><strong>Definition</strong></p>
<p>Given a real-valued random variable <span class="math inline">\(X\)</span> whose <span class="math inline">\(p\)</span>th moment is defined, its <strong>Nightingale deviation of order <span class="math inline">\(p\)</span></strong> is given by <span class="math display">\[\text{Dev}_p[X] \triangleq \sqrt[p]{\mathbb{E}\left[\left|X - \mathbb{E}\left[ X \right]\right|^p\right]}.\]</span></p>
</blockquote>
<p>With the multilinear reflective correlation coefficient as given in Definition <span class="math inline">\(\ref{def:reflectivemultilinearcorrelation}\)</span>, a generalization of the Pearson correlation coefficient is obtained by centering the random variables by their expectation before computing the multilinear reflective correlation. The result is Definition <span class="math inline">\(\ref{def:pearsonmultilinearcorrelation}\)</span> which is abbreviated using the definitions of multilinear covariance (Definition <span class="math inline">\(\ref{def:multilinearcovariance}\)</span>) and Nightingale deviations of order <span class="math inline">\(p\)</span> (Definition <span class="math inline">\(\ref{def:minkowskideviation}\)</span>).</p>
<blockquote class="blockquote">
<p><strong>Definition</strong></p>
<p>Given a finite collection of real-valued random variables <span class="math inline">\(\{X_1, \cdots, X_n\}\)</span>, their <strong>multilinear Pearson’s correlation coefficient</strong> is defined to be</p>
<p><span class="math display">\[\text{Corr}\left[ X_1, \cdots, X_n \right] \triangleq \frac{\text{Cov} \left[ X_1, \cdots, X_n \right]}{\prod_{j=1}^{n} \text{Dev}_n[X_j]}.\]</span></p>
</blockquote>
<p>As exemplified above, making substitutions for the random variables in H"older’s correlation coefficient allows for an easy process of deriving new correlation coefficients. For example, substituting <span class="math inline">\(X_j = \sin (U_j)\)</span> produces a generalization of the used in circular statistics. Similarly, substituting <span class="math inline">\(X_j = |U_j|\)</span> obtains an . There is a general reason why we should take an interest in multiple notions of correlation which we consider in the next subsection: detecting statistical dependence.</p>
</section>
</section>
<section id="interpretation" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="interpretation"><span class="header-section-number">4.2</span> Interpretation</h2>
<p>From <span class="citation" data-cites="Athreya2006">Athreya and Lahiri (<a href="references.html#ref-Athreya2006" role="doc-biblioref">2006</a>)</span> we can consider a proposition which logically connects the notion of expectations of products of functions of random variables to the notion of statistical independence. If we choose a function for which this equality does not hold, then we have found that those variables exhibit statistical dependence. Statistical dependence entails that some events are happening more (or less) together than we would expect compared to the univariate probabilities of those events. Imprecisely, statistical dependence suggests to us that <em>something is going on</em>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Formally, <span class="citation" data-cites="Athreya2006">Athreya and Lahiri (<a href="references.html#ref-Athreya2006" role="doc-biblioref">2006</a>)</span> restrict their result to Borel measurable functions. This restriction can be easily met in real applications of statistics so we do not discuss it further here, but in theory there exists non-Borel measurable functions.</p>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Proposition</strong> (<span class="citation" data-cites="Athreya2006">Athreya and Lahiri (<a href="references.html#ref-Athreya2006" role="doc-biblioref">2006</a>)</span>)</p>
<p>Let <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span> be a probability space and let <span class="math inline">\(\{ X_1, \cdots, X_n \}\)</span>, <span class="math inline">\(2 \leq n &lt; \infty\)</span> be a collection of random variables on <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span>. Then <span class="math inline">\(\{ X_1, \cdots, X_n \}\)</span> are independent if-and-only-if</p>
<p><span class="math display">\[\mathbb{E}\left[ \prod_{j=1}^n f_j(X_j) \right] = \prod_{j=1}^n \mathbb{E} \left[ f_j(X_j)  \right]\]</span></p>
<p>for all bounded Borel measurable functions <span class="math inline">\(f_j : \mathbb{R} \mapsto \mathbb{R}\)</span>, <span class="math inline">\(\forall j \in \{1, \cdots, n \}\)</span>.</p>
</blockquote>
<p>This proposition provides a mathematically precise way of considering multilinear correlation coefficients, and now we turn to building some intuitions about the type of dependence that is described by multilinear correlations. This cannot be done for all choices of functions of random variables, but we will consider the multilinear Pearson correlation as a tractable and instructive example.</p>
<p>Since the multilinear Pearson correlation is defined on finitely-many variables, it can be difficult to understand what it describes when the number of variables is greater than three. In order to build intuition about what it describes in higher dimensions, let us first consider what it describes in two dimensions. Figure <span class="math inline">\(\ref{fig:quadrantcorr}\)</span> illustrates how points contribute weight to a Pearson correlation depending on how it is oriented about the centroid of the data.\</p>
<p>Figure <span class="math inline">\(\ref{fig:quadrantcorr}\)</span>(a) illustrates how even when two independent-and-identically distributed standard normal variables are sampled, there will almost certainly be at least a sleight inbalance between the positive and negative weights contributed. Panels (b) and (c) of Figure <span class="math inline">\(\ref{fig:quadrantcorr}\)</span> emphasize the fact that under perfect negative or positive correlation the points can be reliably found in specific quadrants. Figure <span class="math inline">\(\ref{fig:quadrantcorr}\)</span>(d) shows how non-linear relationships between the variables does not mean that there will be zero correlation. A salient point to take away from all of these panels is that the weight that a point contributes to a Pearson correlation coefficient is equal to the signed area of the rectangle that can be drawn between the centroid and the data point. For trilinear Pearson correlation coefficients this is replaced with the signed volume of a 3-dimensional box, and in the multilinear case for more than 3 variables this signed area is replaced with the signed hypervolume of an <span class="math inline">\(n\)</span>-orthotope (i.e.&nbsp;an n-dimensional box). The magnitude of these signed measures gets scaled down due to the normalization factor constituted by the product of the <span class="math inline">\(p\)</span>-norms as discussed earlier in this section.</p>
<p>The sign of the weight that a point contributes to a multilinear correlation coefficient can be predicted based on the number of variables being considered, and which orthant that point sits within. Table <span class="math inline">\(\ref{tab:orthantsign}\)</span> shows how when certain combinations of components of a point have a positive or negative sign, the resulting product will predictably have a positive or negative sign depending on whether the correlation is among 2, 3, or 4 variables.\</p>
<p>Figure <span class="math inline">\(\ref{fig:trinlinearcorrelationplots}\)</span> visualizes a trivariate analog of Figure <span class="math inline">\(\ref{fig:quadrantcorr}\)</span>. Specifically, a matrix <span class="math inline">\(\mathbf{\theta}_{1000 \times 3}\)</span> of trainable parameters were optimized by gradient descent to minimize the absolute difference between the multilinear Pearson correlation coefficient calculated on <span class="math inline">\(\mathbf{\theta}_{1000 \times 3}\)</span>, as if the columns represented random variables, from a target correlation score. Such data sets are non-unique due to the translation invariance and scaling properties of the multilinear Pearson correlation coefficient. Figure <span class="math inline">\(\ref{fig:trinlinearcorrelationplots}\)</span> conforms to Table <span class="math inline">\(\ref{tab:orthantsign}\)</span> in the sense that the directions that either minimized or maximized the multilinear trilinear correlation coefficient could be found in specific octants that were opposite in combinations of signum.\</p>
<p>A more subtle pattern arises from observing Table <span class="math inline">\(\ref{tab:orthantsign}\)</span>, which is that the of the number of variables predicts how an orthant relates to its reflection. If the number of variables is even, then the weight contributed by a point and its reflection will have the same sign. But if the number of variables is odd, then weight contributed by a point and its reflection will have opposite signs. This has an important consequence for distributions that are symmetric.\</p>
<p>If a distribution is symmetric, then its odd moments will always be zero. A symmetric distribution may or may not have zero even moments. Thus odd-arity multilinear Pearson correlation coefficients not only tell us about dependence, but can also tell us about the symmetry of the distribution via a contrapositive argument. If a distribution being symmetric implies that the odd-arity multilinear Pearson correlation coefficients are zero, and the odd-arity multilinear Pearson correlation coefficients are not zero, then the distribution in question is not symmetric. The converse argument is not true in general.\</p>
</section>
<section id="nightingale-correlation" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="nightingale-correlation"><span class="header-section-number">4.3</span> Nightingale Correlation</h2>
<p>We now introduce Nightingale correlation as an instantiation of the Trinity of Covariation that generalizes the Nightingale deviation of order <span class="math inline">\(p\)</span> (Definition <span class="math inline">\(\ref{def:minkowskideviation}\)</span>), and the absolute multilinear covariance. It provides another way of quantifying coordinated change by taking a <span class="math inline">\(p\)</span>-norm of an element-wise product of vectors.\</p>
<section id="derivations-1" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="derivations-1"><span class="header-section-number">4.3.1</span> Derivations</h3>
<p>We begin with the notion of a seminorm, see Definition <span class="math inline">\(\ref{def:seminorm}\)</span>, which is a generalization of the notion of a norm where the assumption of point separation is absent. Point separation of a function <span class="math inline">\(p:V \mapsto \mathbb{R}\)</span> is the property that <span class="math inline">\(p(x) = 0 \implies x = 0\)</span> which in the context of norms gives the intuitive property that the size of a vector is only zero when the vector is the zero vector.\</p>
<p>Here we generalize the notion of a seminorm from a function of a single vector to a function of multiple vectors as given in Definition <span class="math inline">\(\ref{def:multiseminorm}\)</span>, which we refer to as . The prefix refers to the use of multiple vectors. The approach to this generalization is to take the subadditivity and absolute homogeneity properties that hold in a seminorm, and expand the definition by taking this to be true input-wise for a multiary function.\</p>
<p>With the notion of a multiseminorm in mind as an abstract type of function, we can define a specific function that can be computed on finite vectors which satisfies these properties. This is given in Definition <span class="math inline">\(\ref{def:minkowskimultiseminorm}\)</span> to be the , which is a <span class="math inline">\(p\)</span>-norm of an elementwise product of a collection of vectors.\</p>
<p>The Minkowski multiseminorm of order <span class="math inline">\(p\)</span> is a multiseminorm. The subadditivity property can be confirmed by considering Minkowski’s inequality (i.e.&nbsp;the triangle inequality generalized to <span class="math inline">\(p\)</span>-norms), and the absolute homogeneity can be checked by scaling an arbitrary input and applying some algebra to obtain the desired result.\</p>
<p>Just as a norm can induce a metric, so can a seminorm induce a pseudometric. Likewise, one can define a metric-like function from the notion of a multiseminorm, which we define to be a . In the particular case of the Minkowski multiseminorm of order <span class="math inline">\(p\)</span> we define a function in Definition <span class="math inline">\(\ref{def:minkowskimultipseudometric}\)</span> called a which inherits the properties of a pseudometric input-wise.\</p>
<p>Taking Definition <span class="math inline">\(\ref{def:minkowskimultipseudometric}\)</span> that applies to vectors, we can define a statistical function under an assumption of <span class="math inline">\(p\)</span>-integrability that considers the deviations from the mean coordinated by an index set. This function we take to be the as given in Definition <span class="math inline">\(\ref{def:minkowskimultideviation}\)</span>.\</p>
<p>Definition <span class="math inline">\(\ref{def:minkowskimultideviation}\)</span> represents a unification of multiple notions in addition to being a generalization. We saw with Definition <span class="math inline">\(\ref{def:minkowskideviation}\)</span> giving the Nightingale deviation of order <span class="math inline">\(p\)</span> that the standard deviation can be generalized through the use of different orders of norms. Since the Nightingale covariance of order <span class="math inline">\(p\)</span> reduces to the Nightingale deviation of order <span class="math inline">\(p\)</span> when there is only one variable, the Nightingale covariance of order <span class="math inline">\(p\)</span> is a generalization of the Nightingale deviation of order <span class="math inline">\(p\)</span> and the standard deviation. The Nightingale covariance of order <span class="math inline">\(p\)</span> is also a generalization of the multilinear absolute covariance, which is obtained when <span class="math inline">\(p=1\)</span>.\</p>
<p>Similar to multilinear correlation, it is desirable to find a normalization of Nightingale covariance using a suitable inequality. The inequality in Proposition <span class="math inline">\(\ref{prop:nightinequality}\)</span> serves this purpose, which generalizes the generalized H"older’s inequality for sums given in Proposition <span class="math inline">\(\ref{thm:generalizedholdersinequalitysums}\)</span>.\</p>
<p>With Proposition <span class="math inline">\(\ref{prop:nightinequality}\)</span> we have a normalization for the Nightingale covariance which we us to define the as in Definition <span class="math inline">\(\ref{def:nightingalecorrelation}\)</span>.\</p>
<p>Having defined Nightingale correlation, we will next discuss how to interpret it.</p>
</section>
<section id="interpretation-1" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="interpretation-1"><span class="header-section-number">4.3.2</span> Interpretation</h3>
<p>The Nightingale correlation coefficient is a mathematically precise formula that intuitively encodes simultaneous change because in each instance it is quantifying how much a collection of random variables deviate together from their centroid.</p>
<p>Similar to the multilinear Pearson correlation coefficient, the Nightingale correlation coefficient can be interpreted as being weighted measures of <span class="math inline">\(n\)</span>-orthotopes drawn out between a centroid and a point. A key difference between these two statistics is that the contributing weights in the expectation of product deviations will always be positive. Consider again Figure <span class="math inline">\(\ref{fig:quadrantcorr}\)</span>, but as if all the rectangles were positive (i.e.&nbsp;red) for a visual understanding of this statistic. This distinction also means that the Nightingale correlation does not distinguish between orthants in the same way, and that a symmetric distribution will not necessarily have a zero Nightingale covariance among an odd number of variables.</p>
<p>Lastly, the Nightingale correlation coefficient uses an expectation of a product of functions of random variables, so it is interpretable in terms of Proposition <span class="math inline">\(\ref{thm:prodmomentindependence}\)</span>.</p>
<p>Next we will introduce inner correlations.</p>
</section>
</section>
<section id="inner-correlation" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="inner-correlation"><span class="header-section-number">4.4</span> Inner Correlation</h2>
<p>The notion of an inner product space (Definition <span class="math inline">\(\ref{def:innerproductspace}\)</span>) is a central concept of many disciplines of modern mathematics, and has found applications in virtually every branch of science. A familiar example to a student of linear algebra is the dot product between two vectors, while in statistics it can be found in the covariance between two random variables. They also occur frequently in functional analysis where infinite-dimensional vector spaces such as Hilbert spaces and Banach spaces are considered.\</p>
<p>We will turn to two generalizations of inner product spaces to define new correlation functions on collections of random variables.</p>
<section id="misiak-correlation" class="level3" data-number="4.4.1">
<h3 data-number="4.4.1" class="anchored" data-anchor-id="misiak-correlation"><span class="header-section-number">4.4.1</span> Misiak Correlation</h3>
<p>In 1989 Misiak introduced <span class="math inline">\(n\)</span>-inner product spaces as a natural generalization of <span class="math inline">\(2\)</span>-inner product spaces . The earlier motivations for Misiak’s work were built on a history of studying abstract <span class="math inline">\(n\)</span>-dimensional metric spaces, which we do not focus on in this work. Rather we will focus on using the definitions and propositions developed by Misiak to define a new statistc. We begin with considering a generalization of inner products given in Definition <span class="math inline">\(\ref{def:ninnerproduct}\)</span>.\</p>
<p>Definition <span class="math inline">\(\ref{def:ninnerproduct}\)</span> is quite general, and does not by itself give us specific a function that we could compute on a function or random variable. Rather <span class="math inline">\(n\)</span>-inner products represent an abstract class of functions that satisfy a list of axioms, similar to inner products themselves. Misiak provided a way of systematically defining <span class="math inline">\(n\)</span>-inner products from inner products using the notion of a determinant of a matrix of inner products as given in Proposition <span class="math inline">\(\ref{prop:misiakcorollary15}\)</span>.\</p>
<p>Proposition <span class="math inline">\(\ref{prop:misiakcorollary15}\)</span> allows us to make various choices of inner products on suitable operands. In the context of a sample we might choose for each vector to be an indexed set of deviations of the mean of instantiations of a random variable.\</p>
<p>It would be desirable to have normalized statistic onto an interval of <span class="math inline">\([-1,1]\)</span> as we achieved with multilinear correlations earlier in this chapter. This is especially the case when the scale of the statistic is not readily interpreted. Misiak provided an inequality that sets bounds on the value that an <span class="math inline">\(n\)</span>-inner product can take for a given set of vectors, which is given in Proposition <span class="math inline">\(\ref{prop:cauchyBunyakowskiinequalityarbitraryn}\)</span> as the .\</p>
<p>With the above definitions and propositions, we can construct a new statistic called (Definition <span class="math inline">\(\ref{def:misiakscorrelation}\)</span>).\</p>
</section>
<section id="trenevski-maleski-correlation" class="level3" data-number="4.4.2">
<h3 data-number="4.4.2" class="anchored" data-anchor-id="trenevski-maleski-correlation"><span class="header-section-number">4.4.2</span> Trenevski-Maleski Correlation</h3>
<p>While <span class="math inline">\(n\)</span>-inner products compare a pair of vectors or functions to a collection of other vectors or functions, a further generalization of this is possible that compares one collection of vectors or functions to another collection of vectors or functions. A was defined by , which is given in Definition <span class="math inline">\(\ref{def:trencevskimalceski}\)</span>.\</p>
<p>According to , the <span class="math inline">\(n\)</span>-inner product defined by is a special case of the generalized <span class="math inline">\(n\)</span>-inner product by the relation <span class="math display">\[(\vec{x}, \vec{y} | \vec{z}_1, \cdots, \vec{z}_{n-1}) = \langle \vec{x}, \vec{z}_1, \cdots, \vec{z}_{n-1} | \vec{y}, \vec{z}_1, \cdots, \vec{z}_{n-1} \rangle.\]</span></p>
<p>With a generalization of <span class="math inline">\(n\)</span>-inner products as given in Definition <span class="math inline">\(\ref{def:trencevskimalceski}\)</span> we desire a specific approach to defining examples of generalized <span class="math inline">\(n\)</span>-inner products. provide an analogous result to Proposition <span class="math inline">\(\ref{prop:misiakcorollary15}\)</span> in the form of Proposition <span class="math inline">\(\ref{prop:trencevskiexample}\)</span>.\</p>
<p>It is also desirable to obtain a normalization of any function that is an example of Proposition <span class="math inline">\(\ref{prop:trencevskiexample}\)</span>. Similar to Proposition <span class="math inline">\(\ref{prop:cauchyBunyakowskiinequalityarbitraryn}\)</span> proven in , provide the analogous result as given in Proposition <span class="math inline">\(\ref{prop:trencevskiinequality}\)</span> to obtain a normalization.\</p>
<p>Taking our choice of inner product to be the mixed product moment between two random variables, and utilizing it to construct a normalized statistic we obtain Definition <span class="math inline">\(\ref{def:trencevskimalceskicorrelation}\)</span>.\</p>
</section>
<section id="interpretation-2" class="level3" data-number="4.4.3">
<h3 data-number="4.4.3" class="anchored" data-anchor-id="interpretation-2"><span class="header-section-number">4.4.3</span> Interpretation</h3>
<p>In terms of the Trinity of Covariation, making a substitution such as <span class="math inline">\(U := U - \mathbb{E}[U]\)</span> entails that the resulting determinants of matrices computed via either Proposition <span class="math inline">\(\ref{prop:misiakcorollary15}\)</span> or Proposition <span class="math inline">\(\ref{prop:trencevskiexample}\)</span> will be equivalent to computing the determinant of some covariance matrix. The covariance matrix itself is a quantification of coordinated change between pairs of variables, and its determinant does so in an aggregated way. Due to our usage of expectations of random variables, we can also leverage Proposition <span class="math inline">\(\ref{thm:prodmomentindependence}\)</span> in knowing that statistical independence implies that such expectations should distribute across products of measureable functions of random variables. In the particular case of computing covariances, we know that <span class="math inline">\(X \ind Y \implies \operatorname{Cov}[X,Y] = 0\)</span>.\</p>
<p>Definition <span class="math inline">\(\ref{def:misiakcorrelation}\)</span> and Definition <span class="math inline">\(\ref{def:trencevskimalceskicorrelation}\)</span> also tell us either about linear dependence or about spanning the same subspace through Proposition <span class="math inline">\(\ref{prop:trencevskilineardependence}\)</span>.\</p>
<p>If either of the first two conditions hold in Proposition <span class="math inline">\(\ref{prop:trencevskilineardependence}\)</span> then the derived correlation coefficients will be indeterminant because both the left and the right hand sides of Proposition <span class="math inline">\(\ref{prop:trencevskiinequality}\)</span> will be zero. Thus calculating the numerator and factors of the denominator of the Misiak and Trenevski-Maleski correlation coefficients to check if linear dependence holds is recommended. In the remaining third case equality holds if the two collections of vectors span the same subspace, yielding a correlation score of <span class="math inline">\(\pm 1\)</span> depending on whether the orientation of the vectors is needed. Section 3 of provides a discussion of how such a coefficient as given in Definition <span class="math inline">\(\ref{def:trencevskimalceskicorrelation}\)</span> can be defined as a cosine of the angle between two subspaces which can be used to obtain a metric on subspaces of a vector space <span class="math inline">\(V\)</span> simply by taking the <span class="math inline">\(\arccos\)</span> to obtain the angle. They show that such an angle is invariant to choice of basis. Therefore inner correlation coefficients as we have defined in this section are a way of describing the “closeness” of two sets of random variables in a way that does not depend on scaling or translation.\</p>
</section>
</section>
<section id="agnesian-operators" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="agnesian-operators"><span class="header-section-number">4.5</span> Agnesian Operators</h2>
<p>We introduce Agnesian operators as an instantiation of the Trinity of Covariation that utilizes elementary calculus. We have named this family of operators after the Italian mathematician Maria Gaetana Agnesi (1718-1799) (Figure <span class="math inline">\(\ref{fig:mariaagnesi}\)</span>(a)). She was the first woman to be appointed as a mathematics professor (Figure <span class="math inline">\(\ref{fig:mariaagnesi}\)</span>(c)), and she was known for writing the textbook (Figure <span class="math inline">\(\ref{fig:mariaagnesi}\)</span>(b)) that covered both differential and integral calculus . Because the Agnesian operators unite the notions of derivatives and integrals into a single operator, it seems symbolically fitting that Maria Agnesi be their namesake because her work brought differential and integral calculus together in the education of young mathematicians.</p>
<div id="fig-maria-gaetana-agnesi" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-maria-gaetana-agnesi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://upload.wikimedia.org/wikipedia/commons/5/57/Maria_Gaetana_Agnesi.jpg" class="img-fluid figure-img"></p>
<figcaption>Maria Gaetana Agnesia (Public Domain)</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://upload.wikimedia.org/wikipedia/commons/1/1c/Il_frontispizio_delle_Instituzioni_analitiche_dell%27_Agnesi.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://upload.wikimedia.org/wikipedia/commons/7/7b/Il_diploma_di_nomina_dell%27_Agnesi_all%27_Universit%C3%A0_di_Bologna.png" class="img-fluid figure-img"></p>
<figcaption>Agnesi’s diploma from Università di Bologna (Public Domain)</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-maria-gaetana-agnesi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: Maria Gaetana Agnesi
</figcaption>
</figure>
</div>
<p>Derivatives are linear approximations of the slope of the line tanget to a function at a given point, providing a representation of the change part of the Trinity of Covariation. In the case of intergrals, the definite integral over <span class="math inline">\([a,b]\)</span>, <span class="math inline">\(\int_a^b \frac{df}{dt}dt = f(b) - f(a)\)</span>, models the net change the net change theorem. This idea is adopted to represent the change part of the Trinity of Covariation. We can choose collections of differentiable and/or integrable scalar functions to represent a kind of ‘structure’. The notion of ‘coordination’ can be captured in the form of scalar multiplication in which one might think of a collection of changes mutually scaling each other. Putting these notions together, Definition <span class="math inline">\(\ref{def:totalagnesian}\)</span> gives a precise definition of the (total) Agnesian operator of a given order.</p>
<blockquote class="blockquote">
<p><strong>Definition</strong></p>
<p>Let <span class="math inline">\(S = \{x_1(t),  \cdots, x_n(t) | x_j(t) \in \mathbb{R}, t \in \mathbb{R} \}\)</span> be a collection of suitably smooth or integrable functions of a real parameter <span class="math inline">\(t\)</span>, then their <strong>Agnesian of order <span class="math inline">\(k\)</span></strong> is given by <span class="math display">\[\mathcal{A}_{t}^{k} S= \begin{cases} \prod_{j=1}^{n} \frac{d^k}{dt^k}x_j(t) &amp; k &gt; 0 \\ \prod_{j=1}^{n} x_j (t) &amp; k = 0 \\ \prod_{j=1}^{n} \underbrace{\int \cdots \int}_k x_j(t)\ \underbrace{dt \cdots dt}_k &amp; k &lt; 0. \end{cases}\]</span></p>
</blockquote>
<p>Because the Agnesian is defined using set-builder notation, it is possible to construct a great variety functions with it. For a given collection of <span class="math inline">\(n\)</span> suitably smooth or integrable functions, there exists <span class="math inline">\(|\mathcal{P}(S)/\emptyset| = 2^n-1\)</span> (i.e.&nbsp;the powerset excluding the empty set) possible choices of Agnesian functionals that could be defined at some given order. For convenience of notation, we can consider Agnesian operators of scalar functions and vector functions. For a scalar function <span class="math inline">\(f(t)\)</span> we will assume that notation <span class="math inline">\(\mathcal{A}_t^k f(t)\)</span> is equivalent to <span class="math inline">\(\mathcal{A}_t^k \{ f(t) \}\)</span>. In the vector case we will take <span class="math inline">\(\mathcal{A}_t^k \vec{x}(t)\)</span> where the components of <span class="math inline">\(\vec{x}(t)\)</span> are indexed by <span class="math inline">\(j \in \{1, \cdots, n \}\)</span> to be equivalent to <span class="math inline">\(\mathcal{A}_t^k \{x_1(t), \cdots, x_n(t) \}\)</span>. These two notational conventions immediately gives us the property that <span class="math inline">\(\mathcal{A}_t^k \vec{x}(t) = \prod_{j=1}^n \mathcal{A}_t^k x_j(t)\)</span>, which is used in the proof of Proposition <span class="math inline">\(\ref{prop:agnesiansqueezeinvariant}\)</span>.</p>
<p>Let us consider the example of a helix embedded in <span class="math inline">\(\mathbb{R}^3\)</span> according to the vector equation</p>
<p><span class="math display">\[\vec{s}(t) = \begin{bmatrix}
\cos t \\
\sin t \\
t \\
\end{bmatrix}.\]</span></p>
<p>Figure <span class="math inline">\(\ref{fig:Agnesianhelixpanel}\)</span> shows the Agnesians of a helix for orders <span class="math inline">\(k \in \{-3, -2, -1, 0, 1, 2\}\)</span>, and illustrates how the order specifies different curves. The zero-order Agnesian in Figure <span class="math inline">\(\ref{fig:Agnesianhelixpanel}\)</span>(a) has reflective symmetry about <span class="math inline">\(t=0\)</span> and will oscillate between larger amplitudes as <span class="math inline">\(|t| \rightarrow \infty\)</span>. The first-order Agnesian in Figure <span class="math inline">\(\ref{fig:Agnesianhelixpanel}\)</span> is equivalent to <span class="math inline">\(\frac{1}{2} \sin 2t\)</span>, which is a reflection equivariant function (i.e.&nbsp;an odd function) in <span class="math inline">\(t\)</span>. As shown in Figure <span class="math inline">\(\ref{fig:Agnesianhelixpanel}\)</span> (c), the second-order Agnesian is zero for all time because the <span class="math inline">\(\frac{d^2 t}{dt^2} = 0\)</span> factor. Indeed, any Agnesian of this helix in the given coordinates will be zero if <span class="math inline">\(k \geq 2\)</span>. Panels (d), (e), and (f) of Figure <span class="math inline">\(\ref{fig:Agnesianhelixpanel}\)</span> show that with constants of integration taken to be zero we have an alternating pattern of odd or even Agnesians depending on the parity of the degree of the polynomial factor <span class="math inline">\(\frac{t^{k+1}}{(k+1)!}\)</span>. For <span class="math inline">\(k &lt; 0\)</span>, if <span class="math inline">\(k+1\)</span> is even then the result Agnesian is an odd function of <span class="math inline">\(t\)</span>, and if <span class="math inline">\(k+1\)</span> is even then the resulting Agnesian will be odd. Similar to what we observed with the zero-order Agnesian, we find that the negative order Agnesians of this helix will oscilate between larger amplitudes as <span class="math inline">\(|t| \rightarrow \infty\)</span>.\</p>
<p>Obtaining a scalar function of time affords us the opportunity to ask elementary questions about the function. Such questions include its domain, image, and extrema. While all of the Agnesian curves in the previous example were defined over all real numbers, this does not need to be the case in general. An example is the curve <span class="math inline">\([t, \log t]^T\)</span> which will not be defined for <span class="math inline">\(t=0\)</span>. The example of the helix also showed us that some Agnesians have local and global optima, while others have no finite global optima.\</p>
<p>In the example of the helix we assumed a given coordinate system. It is important to consider that the Agnesian of a parametric curve is not invariant to linear changes in basis. For a parametric curve <span class="math inline">\(\vec{x}(t) \in \mathbb{R}^n\)</span> and linear transformation <span class="math inline">\(T \in \mathbb{R}^{n \times n}\)</span>, it does not hold in general that <span class="math inline">\(\mathcal{A}_t^k \left[ T \vec{x}(t) \right] = \mathcal{A}_t^k \left[ \vec{x}(t) \right]\)</span>. There exist invariants for the Agnesian of parametric curves in <span class="math inline">\(\mathbb{R}^{2}\)</span> including rotations of <span class="math inline">\(\pi\)</span> radians, squeezing, and permutation. The identity matrix is a permutation matrix, and clearly leaves the curve unchanged. In Proposition <span class="math inline">\(\ref{prop:agnesiansqueezeinvariant}\)</span> we show that multidimensional squeezing is an invariant of the Agnesian operator.</p>
<blockquote class="blockquote">
<p><strong>Proposition</strong></p>
<p>Let <span class="math inline">\(\vec{x}(t) \in \mathbb{R}^n\)</span> be a parametric curve and <span class="math inline">\(D \in \mathbb{R}^{n \times n}\)</span> be a diagonal matrix with constant entries and <span class="math inline">\(\det D = 1\)</span>, then</p>
<p><span class="math display">\[\mathcal{A}_t^k \left[ D \vec{x}(t) \right] = \mathcal{A}_t^k \left[ \vec{x}(t) \right].\]</span></p>
<p><strong>Proof</strong></p>
<p>Taking <span class="math display">\[
D =
\begin{bmatrix}
   d_{1} &amp; &amp; \\
   &amp; \ddots &amp; \\
   &amp; &amp; d_{n}
\end{bmatrix}\]</span></p>
<p>and <span class="math display">\[\vec{x}(t) = \begin{bmatrix}
x_1(t) \\
\vdots \\
x_n(t)
\end{bmatrix}\]</span></p>
<p>then <span class="math display">\[D \vec{x}(t) = \begin{bmatrix}
d_1 x_1(t) \\
\vdots \\
d_n x_n(t)
\end{bmatrix}.\]</span> This entails that</p>
<p><span class="math display">\[\begin{align*}
\mathcal{A}_t^k \left[ D \vec{x}(t) \right] =&amp; \prod_{j=1}^n d_j \mathcal{A}_t^k x_j(t) \\
=&amp; \left( \prod_{j=1}^n d_j \right) \left( \prod_{j=1}^n \mathcal{A}_t^k x_j(t) \right) \\
=&amp; \det D \mathcal{A}_t^k \left[ \vec{x}(t) \right] \\
=&amp; \mathcal{A}_t^k \left[ \vec{x}(t) \right]
\end{align*}\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
</blockquote>
<p>What these invariant linear transformations have in common is that they have determinant of unity. This suggests a hypothesis that the property of being volume-preserving might be a necessary-but-insufficient condition for an operation to be Agnesian-preserving, but we do not explore this hypothesis further in this thesis.</p>
<p>The Agnesian provides a scalar function of a single parameter, which could be time, length, or some other variable. It is sometimes desirable to incorporate multiple parameters, like when there is one parameter of time and three directions of physical space. An extension to the Agnesian operator comes from considering coordinated change in multiple parameters in the form of the <em>multiagnesian</em> as given in Definition <span class="math inline">\(\ref{def:multiagnesian}\)</span>.</p>
<blockquote class="blockquote">
<p><strong>Definition</strong></p>
<p>Let <span class="math inline">\(S = \{x_1(t_1, \cdots, t_p),  \cdots, x_n(t) | x_j(t_1, \cdots, t_p) \in \mathbb{R} \land t_1, \cdots, t_p \in \mathbb{R} \}\)</span> be a collection of suitably smooth or integrable functions of a real parameters <span class="math inline">\(t_1, \cdots, t_p\)</span>, then their is given by <span class="math display">\[\begin{equation*}
\mathcal{A}_{(t_1, \cdots, t_p)}^{k} S = \begin{cases} \prod_{j=1}^{n} \prod_{w=1}^{p} \frac{d^k}{dt_{w}^k}x_j(t_1, \cdots, t_p) &amp; k &gt; 0 \\ \prod_{j=1}^{n} \prod_{w=1}^{p} x_j (t_1, \cdots, t_p) &amp; k = 0 \\ \prod_{j=1}^{n} \prod_{w=1}^{p} \underbrace{\int \cdots \int}_k x_j(t_1, \cdots, t_p)\ \underbrace{dt_{w}  \cdots dt_{w}}_k &amp; k &lt; 0 \end{cases}
\end{equation*}\]</span></p>
</blockquote>
<p>The multiagnesian operator is a natural generalization of the Agnesian operator to include multiple parameters. While the Agnesian operator can be used to study parametric curves in finite-dimensional vector spaces, the multiagnesian operator can be used to study surfaces and hypersurfaces in similar spaces. Using the commutativity and associativity of scalar multiplication, we can readily find that a multiagnesian is the product of Agnesian operators with respect to each parameter:</p>
<p><span class="math display">\[\mathcal{A}_{(t_1, \cdots, t_p)}^{k} \vec{x}(t_1, \cdots, t_p) = \prod_{w=1}^p \mathcal{A}_{t_w}^k \vec{x}(t_1, \cdots, t_p).\]</span></p>
<p>We have defined the Agnesian operator, and given some exemplification and discussion of it in mathematical language. In the next subsection we will offer some further interpretations of this operator.</p>
<section id="interpretation-3" class="level3" data-number="4.5.1">
<h3 data-number="4.5.1" class="anchored" data-anchor-id="interpretation-3"><span class="header-section-number">4.5.1</span> Interpretation</h3>
<p>The Agnesian operators can be understood in terms of mutually scaling change. Due to the product expansion in the definition of an Agnesian, it is clear that a collection of quantities are mutually scaling each other, and the order tells us what sort of quantities are performing this scaling. For positive order $ k &gt; 0$ we are considering how a collection of derivatives of a corresponding order are mutually scaling. A zero-order Agnesian quantifies how large the functions are together at some value of the parameter. And for the case of negative orders we are considering the coordinated net changes in a collection of functions with respect to the parameter. Decreasing the order below <span class="math inline">\(k=-1\)</span> leads to net changes in net changes, etc, depending on the value of <span class="math inline">\(k\)</span>.\</p>
<p>A picture that comes from taking the product of a collection of scalars is to consider an <span class="math inline">\(n\)</span>-dimensional box whose lengths are equal to the <span class="math inline">\(n\)</span> scalars being multiplied. The product of these scalars would then be the signed volume of the box. For <span class="math inline">\(n \leq 3\)</span> we can draw pictures to motivate what this looks like. Let us suppose an example where <span class="math inline">\(\{ f(t), g(t) \}\)</span> is our set of functions of time. Figure <span class="math inline">\(\ref{fig:agnesianbox}\)</span> illustrates a hypothetical planar curve whose coordinates are defined by <span class="math inline">\(\left( \mathcal{A}_t^k f(t), \mathcal{A}_t^k g(t) \right)\)</span>. For a given point on this curve there exists a rectangular region between it and the origin. The signed area of this rectangle gives the value of <span class="math inline">\(\mathcal{A}_t^k\{ f(t), g(t) \}\)</span>. The sign of the Agnesian operator follows a similiar pattern to that illustrated in Figure <span class="math inline">\(\ref{fig:quadrantcorr}\)</span> and Table <span class="math inline">\(\ref{tab:orthantsign}\)</span> describing the behaviour of multilinear correlation in the sense that multiplying scalars with particular combinations of positive and negative sign result in a scalar with a parciular sign. Unlike the multilinear correlation function, the Agnesian operator as we have defined above does not consider any random variables.\</p>
<p>We noted earlier that Agnesian operators are not invariant to linear changes in basis in general. This mathematical fact can be related back to the Trinity of Covariation: the amount of coordinated change as quantified by the Agnesian operator depends on from what perspective we are `looking’ at the structure.\</p>
<p>We have introduced the Agnesian operator as an instance of the Trinity of Covariation and as an integro-differential operator. In the next section we will introduce grade entropies as an instance of the Trinity of Covariation that connects the subjects of information theory and order theory.</p>
</section>
</section>
<section id="grade-entropies" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="grade-entropies"><span class="header-section-number">4.6</span> Grade Entropies</h2>
<p>In this section we will introduce a family of functions called . Because entropy is an often-misunderstood concept, we provide some historical context and review of mathematical definitions and properties to clarify it sufficiently for understanding the concept of grade entropy. Then we will review grade functions in the context of partially ordered sets and relate them to entropy. Finally we discuss the interpretation of grade entropies and how they relate to the Trinity of Covariation.</p>
<section id="entropy" class="level3" data-number="4.6.1">
<h3 data-number="4.6.1" class="anchored" data-anchor-id="entropy"><span class="header-section-number">4.6.1</span> Entropy</h3>
<p>The first notions that would become a formal model of entropy were developed by Rudolf Clausius (Figure <span class="math inline">\(\ref{fig:foundersentropy}\)</span>(a)), which he described as the of a body in terms of the reciprocal temperature integrated over a heat differential .</p>
<p>Entropy was formalized as a function of the number of possible microscopic states (i.e.&nbsp;microstates) in the context of statistical thermodynamics by Ludwig Boltzmann (Figure <span class="math inline">\(\ref{fig:foundersentropy}\)</span>(b)) in his efforts to relate the average kinetic energy of gas particles to the thermodynamic temperature of the gas . Each microstate represents the physical configuration of a part of a physical system, such as the orbital states of electrons in atoms, that are associated with an energy level.</p>
<blockquote class="blockquote">
<p><strong>Definition</strong></p>
<p><strong>Boltzmann’s entropy</strong> equation is given by <span class="math display">\[\begin{equation*}
S \triangleq k_B \ln \Omega
\end{equation*}\]</span> where <span class="math inline">\(S\)</span> is the (thermodynamic) entropy, <span class="math inline">\(k_B\)</span> is the Boltzmann constant taking the exact value of <span class="math inline">\(1.380649 \times 10^{-23}\ \frac{\text{J}}{\text{K}}\)</span> in units of Joules-per-Kelvin, and <span class="math inline">\(\Omega\)</span> is the number of equilibrium microstates of a system.</p>
</blockquote>
<p>Definition <span class="math inline">\(\ref{def:boltzmannentropy}\)</span> assumes that each microstate is equally probable, which is the case when a system is already in thermodynamic equilibrium. To describe the entropy of systems in which not all states are equally probable, a generalization of Definition <span class="math inline">\(\ref{def:boltzmannentropy}\)</span> given in Definition <span class="math inline">\(\ref{def:gibbsentropy}\)</span> was developed by Josiah Gibbs (Figure <span class="math inline">\(\ref{fig:foundersentropy}\)</span>(c)) (<span class="citation" data-cites="Jaynes1965">Jaynes (<a href="references.html#ref-Jaynes1965" role="doc-biblioref">1965</a>)</span>).</p>
<blockquote class="blockquote">
<p><strong>Definition</strong></p>
<p>Gibbs’s entropy equation is given by <span class="math display">\[\begin{equation*}
S \triangleq -  k_B \sum_i p_i \ln p_i
\end{equation*}\]</span> where <span class="math inline">\(S\)</span> is the (thermodynamic) entropy, <span class="math inline">\(k_B\)</span> is the Boltzmann constant as found in Boltzmann’s entropy, and <span class="math inline">\(p_i\)</span> is the probability of the <span class="math inline">\(i\)</span>th state from a finite set of physical states.</p>
</blockquote>
<p>In 1948 entropy was generalized beyond the context of Physics by Claude Shannon (Figure <span class="math inline">\(\ref{fig:foundersentropy}\)</span> (d)) to a purely mathematical construct that has applications in many domains. This generalization, sometimes called Shannon-Wiener entropy, is given in Definition <span class="math inline">\(\ref{def:shannonentropy}\)</span>. Jon von Neumann (Figure <span class="math inline">\(\ref{fig:foundersentropy}\)</span> (e)) adapted this definition to the context of quantum mechanics, which we do not explore further here.</p>
<blockquote class="blockquote">
<p><strong>Definition</strong> Shannon’s entropy equation is given by <span class="math display">\[\begin{equation*}
S \triangleq -  K \sum_i p_i \ln p_i
\end{equation*}\]</span> where <span class="math inline">\(S\)</span> is the Shannon entropy, <span class="math inline">\(K\)</span> is a proportionality constant, and <span class="math inline">\(p_i\)</span> is the probability of the <span class="math inline">\(i\)</span>th event from a discrete event space.</p>
</blockquote>
<p>The differences between Definition <span class="math inline">\(\ref{def:gibbsentropy}\)</span> and Definition <span class="math inline">\(\ref{def:shannonentropy}\)</span> are simple, subtle, but also of great importance. The first difference is explicit: we exchange the Boltzmann constant <span class="math inline">\(k_B\)</span> for an arbitrary proportionality constant <span class="math inline">\(K\)</span>. This proportionality constant is often taken to be unity in applications of Definition <span class="math inline">\(\ref{def:shannonentropy}\)</span>, however in principle it could be used to scale the log-linear factor onto a desired measurement scale. The second difference is less explicit, but of great practical significance: the probability measure can be over <em>any</em> discrete random variable.</p>
<p>Since entropy in a general sense is not about thermodynamic states <em>per se</em>, it is worth considering what it actually tells us about a discrete random variable. Often entropy is heuristically described as a “measure of disorder”, but this way of describing entropy has limitations that are outlined in <span class="citation" data-cites="Lambert2002">Lambert (<a href="references.html#ref-Lambert2002" role="doc-biblioref">2002</a>)</span>. Among them are the observations that maximal entropy distributions can generate ordered structures, and that subjective disagreement between observers can occur over what consistutes ‘disorder’.</p>
<p>Since entropy (specifically Shannon’s entropy) has a specific mathematical definition, it is worth considering one of its important properties in the form Proposition <span class="math inline">\(\ref{prop:maxentropy}\)</span>.</p>
<blockquote class="blockquote">
<p><strong>Proposition</strong></p>
<p>Given a discrete random variable <span class="math inline">\(X\)</span> with probability space <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span>, its Shannon entropy <span class="math inline">\(H(X)\)</span> satisfies</p>
<p><span class="math display">\[H(X) \leq \log |\Omega| \]</span></p>
<p>where $H(X) || $ if-and-only-if <span class="math inline">\(P\)</span> is uniform.</p>
<p><strong>Proof</strong></p>
<p>See Theorem 2.6.4 in <span class="citation" data-cites="Thomas2006">Cover and Thomas (<a href="references.html#ref-Thomas2006" role="doc-biblioref">2006</a>)</span>.</p>
</blockquote>
<p>Proposition <span class="math inline">\(\ref{prop:maxentropy}\)</span> gives us a specific way to think about what Shannon entropy quantifies: uniformity of the distribution of a discrete random variable. Abstracting entropy beyond the context of thermodynamics and taking this precise notion that quantifies uniformity will be important for understanding grade entropy.</p>
<p>Let us briefly consider an example of applying the mathematical concept of entropy outside of thermodynamics that comes from ecology. In ecology, a community is a collection of populations. In the analysis of species abundance tables it is desirable to quantify the ‘diversity’ of a community of organisms.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What is a species abundance table?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A species abundance table is a matrix whose entries are the count of how many times each species was observed in each sample.</p>
</div>
</div>
</div>
<p>Entropy is one method of quantifying diversity if we consider diversity in the sense of uniformity of species abundances. The species abundance table gives us an empirical frequency distribution with discrete support. Computing the Shannon entropy from this distribution tells us how uniform the species abundances are in the community.</p>
<p>In summary we have noted that entropy was originally motivated and developed within the context of physics, and was later generalized to any discrete probability distribution. Rather than quantifying ‘disorder’ as a synonym for ‘macroscopic messiness’, Shannon entropy tells us about the uniformity of a discrete random variable.</p>
</section>
<section id="grades-and-partially-ordered-sets" class="level3" data-number="4.6.2">
<h3 data-number="4.6.2" class="anchored" data-anchor-id="grades-and-partially-ordered-sets"><span class="header-section-number">4.6.2</span> Grades and Partially-Ordered Sets</h3>
<p>Before introducing grade entropies, we will briefly review the properties of relations. All partial orders are relations. A relation is a subset of a Cartesian product of sets. Given a collection of sets <span class="math inline">\(S_1,\cdots, S_n\)</span>, a relation <span class="math inline">\(R \subseteq S_1 \times S_2 \times \cdots \times S_{n-1} \times S_n\)</span> is said to be <em>homogenous</em> if all pairs of sets are equal. Otherwise such a relation is <em>inhomogenous</em>. Relations are also classified by the number of sets being used in the set multiplication. A <em>binary relation</em> is a subset of the Cartesian product of two sets, and more generally an <em><span class="math inline">\(n\)</span>-ary relation</em> is obtained from taking a subset of the Cartesian product of <span class="math inline">\(n\)</span> sets. In this section we focus on binary relations. Beyond these distinctions, many relations are classified by certain rules about which elements of a set are allowed to be members of the relation.</p>
<p>The rows of Table <span class="math inline">\(\ref{tab:relationproperties}\)</span> show some common types of relations or relation properties including reflexivity, symmetry, antisymmetry, asymmetry, transitivity, and connectedness. These properties in certain combinations give partial orders, strict partial orders, total orders, and strict total orders.</p>
<div id="tbl-relation-properties" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-relation-properties-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.1: Summary of common types of order relations. The listed properties are assumed to hold for all <span class="math inline">\(x, y, z\)</span> as needed from a set <span class="math inline">\(S\)</span> whose binary relation <span class="math inline">\(R\)</span> is a subset of <span class="math inline">\(S \times S\)</span>.
</figcaption>
<div aria-describedby="tbl-relation-properties-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 34%">
<col style="width: 10%">
<col style="width: 15%">
<col style="width: 9%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>Property</th>
<th>Expression</th>
<th>Partial Order</th>
<th>Strict Partial Order</th>
<th>Total Order</th>
<th>Strict Total Order</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Reflexive</td>
<td><span class="math inline">\(xRx\)</span></td>
<td>✓</td>
<td></td>
<td>✓</td>
<td></td>
</tr>
<tr class="even">
<td>Irreflexive</td>
<td><span class="math inline">\(\neg x R x\)</span></td>
<td></td>
<td>✓</td>
<td></td>
<td>✓</td>
</tr>
<tr class="odd">
<td>Symmetric</td>
<td><span class="math inline">\(x R y \implies y R x\)</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Antisymmetric</td>
<td><span class="math inline">\(x R y \land y R x \implies x = y\)</span></td>
<td>✓</td>
<td></td>
<td>✓</td>
<td></td>
</tr>
<tr class="odd">
<td>Asymmetric</td>
<td><span class="math inline">\(x R y \implies \neg y R x\)</span></td>
<td></td>
<td>✓</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Transitive</td>
<td><span class="math inline">\(x R y \land y R z \implies x R z\)</span></td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr class="odd">
<td>Connected</td>
<td><span class="math inline">\(x \neq y \implies x R y \lor y R x\)</span></td>
<td></td>
<td></td>
<td></td>
<td>✓</td>
</tr>
<tr class="even">
<td>Strongly Connected</td>
<td><span class="math inline">\(x R y \lor y R x\)</span></td>
<td></td>
<td></td>
<td>✓</td>
<td></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The familiar comparisons <span class="math inline">\(a &lt; b\)</span> and <span class="math inline">\(a \leq b\)</span> with real numbers make clear that orders exist in one dimension. But in this work we are interested in orders over sets of multidimensional points. One approach is to find a map <span class="math inline">\(f:\mathbb{R}^n \mapsto \mathbb{R}\)</span> and compare <span class="math inline">\(f(\vec{x}) &lt; f(\vec{y})\)</span> for any <span class="math inline">\(\vec{x}, \vec{y} \in \mathbb{R}^n\)</span>. A common choice for <span class="math inline">\(f\)</span> is a norm, such as the Euclidean norm. Beyond such function-induced orders, three possibilities include product orders, Pareto orders, and lexicographical orders.</p>
<p>A is satisfied by the componentwise comparisons all being satisfied. For vectors <span class="math inline">\(\vec{x}, \vec{y} \in \mathbb{R}^n\)</span>, they satisfy a strict product order if <span class="math inline">\(x_i &lt; y_i\)</span> for all <span class="math inline">\(i \in \{1, \cdots, n \}\)</span>. Similarly, they satisfy a non-strict product order if <span class="math inline">\(x_i \leq y_i\)</span> for all <span class="math inline">\(i \in \{1, \cdots, n \}\)</span>.</p>
<p>A does not have a strict or non-strict form because it already requires both strict and non-strict componentwise comparisons. For vectors <span class="math inline">\(\vec{x}, \vec{y} \in \mathbb{R}^n\)</span>, they satisfy a Pareto order if <span class="math inline">\(x_i \leq y_i\)</span> for all <span class="math inline">\(i \in \{1, \cdots, n \}\)</span> there exists <span class="math inline">\(j \in \{1, \cdots, n \}\)</span> such that <span class="math inline">\(x_j &lt; y_j\)</span>. A Pareto order is more restrictive than a non-strict product order, but less restrictive than a strict product order, in the sense of what fraction of an arbitrary finite set would satisfy the relation. Pareto orders are applied in finance (<span class="citation" data-cites="Amershi1985">Amershi (<a href="references.html#ref-Amershi1985" role="doc-biblioref">1985</a>)</span>), game theory (<span class="citation" data-cites="Aubin2014">Aubin (<a href="references.html#ref-Aubin2014" role="doc-biblioref">2014</a>)</span>), and multiobjective optimization (<span class="citation" data-cites="Emmerich2018">Emmerich and Deutz (<a href="references.html#ref-Emmerich2018" role="doc-biblioref">2018</a>)</span>).</p>
<p>A <em>lexicographical order</em> is a kind of ordering that prioritizes comparing certain components before others. An example of a lexicographical order is the use of an alphabet to order words. The alphabet itself is an order on the characters, while the lexicographical order entails looking to compare the first two letters, then the second two, etc. As an example, suppose we had the points <span class="math inline">\(a = (2,1)\)</span> and <span class="math inline">\(b=(1,3)\)</span>. Under a lexicographical ordering it holds that <span class="math inline">\(b &lt; a\)</span> because the first component is compared first, and further components are irrelevant beyond the ealiest component that breaks the tie between the points. Lexicographical orders can be strict or non-strict, and are frequently used to define the conditions under which a collection of strings are sorted.</p>
<p>Next we will give some background to the notion of grades (i.e.&nbsp;ranks) on points based on a given partial order.</p>
</section>
<section id="graded-posets" class="level3" data-number="4.6.3">
<h3 data-number="4.6.3" class="anchored" data-anchor-id="graded-posets"><span class="header-section-number">4.6.3</span> Graded Posets</h3>
<p>For the construction of the notion of grade entropies we make use of grades of points. Given a finite collection of multidimensional points, which may possibly be a statistical sample from a population, we can use a partially ordered set (i.e.&nbsp;a poset) to assign a rank to each point. A point dominanted by no other point would have a rank of 1, a point dominated by one point would have a rank of two, and a point dominated by <span class="math inline">\(k\)</span> number points would have a rank of <span class="math inline">\(k+1\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This description of assigning ranks according to point dominance is sufficient for what we later term a <em>pseudograde</em> function, and that a covering relation provides the constraint needed to obtain a <em>grade</em> function.</p>
</div>
</div>
<p>Before defining a graded partially ordered set (i.e.&nbsp;a graded poset), it is convenient to define the notion of a covering relation as given in Definition <span class="math inline">\(\ref{def:coveringrelation}\)</span>.</p>
<blockquote class="blockquote">
<p><strong>Definition</strong> (<span class="citation" data-cites="Stanley2011">Stanley (<a href="references.html#ref-Stanley2011" role="doc-biblioref">2011</a>)</span>)</p>
<p>Suppose <span class="math inline">\(S\)</span> is a set with a partial order <span class="math inline">\(\leq\)</span>, and strict partial order <span class="math inline">\(&lt;\)</span> that holds whenever <span class="math inline">\(x \leq y \land x \neq y\)</span>.</p>
<p>The <strong>covering relation</strong>, denoted <span class="math inline">\(x \lessdot y\)</span> for <span class="math inline">\(x,y \in S\)</span>, holds if <span class="math inline">\(x &lt; y\)</span> and there does not exist <span class="math inline">\(z \in S\)</span> such that <span class="math inline">\(x &lt; z &lt; y\)</span>.</p>
</blockquote>
<p>A covering relation captures an intuitive notion that certain elements are “beside each other”, which is closely related to the notion of a successor function. A successor function sends a natural number to the next natural number by incrementing by unity, while the predecessor function sends a natural number to the previous natural number by subtracting unity. For a graded poset (Definition <span class="math inline">\(\ref{def:gradedposet}\)</span>) it is assumed that if two elements are beside each other in the covering relation, then their grades should correspondingly be predecessors or successors of each other.</p>
<blockquote class="blockquote">
<p><strong>Definition</strong> (<span class="citation" data-cites="Stanley2011">Stanley (<a href="references.html#ref-Stanley2011" role="doc-biblioref">2011</a>)</span>)</p>
Let <span class="math inline">\(S\)</span> be a partially ordered set equipped with a rank function <span class="math inline">\(\rho: S \mapsto \mathbb{N}\)</span>, where <span class="math inline">\(\rho\)</span> satisfies the following:
<p>where <span class="math inline">\(\lessdot\)</span> is a covering relation on <span class="math inline">\(S\)</span>. Such a partially ordered set is called a or .</p>
</blockquote>
<p>What Definition <span class="math inline">\(\ref{def:gradedposet}\)</span> provides is a clear notion of grading a collection of points under the assumption of a partial order, that these grades preserve the order, and preserve closeness in the sense of a covering relation.</p>
<p>With these notions in place, we will now construct the notion of a grade entropy.</p>
</section>
<section id="grade-entropies-1" class="level3" data-number="4.6.4">
<h3 data-number="4.6.4" class="anchored" data-anchor-id="grade-entropies-1"><span class="header-section-number">4.6.4</span> Grade Entropies</h3>
<p>Grade entropies are about quantifying the totality of a partial order or a strict partial order. Traditionally a given order relation is total, or it isn’t total, with no consideration of how close a relation is to being total. But we suggest that it is desirable to quantify how far short a partial order falls from being a total order, which we informally refer to here as “totality”</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this work we take an information theory approach to quantifying totality, but it may also be possible to approach this notion with metric spaces.</p>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Definition</strong></p>
<p>Suppose that <span class="math inline">\(S \subset   \mathbb{R}^n\)</span> is a finite set of points of size <span class="math inline">\(|S|=m\)</span> that is also a graded poset. With a grade function <span class="math inline">\(g\)</span>, and probability mass function <span class="math inline">\(p\)</span> over <span class="math inline">\(g\)</span> we define the to be</p>
<p><span class="math display">\[H_g \triangleq - \sum_{i=1}^{k} p(g_i)  \log_b  p(g_i) \]</span></p>
<p>where <span class="math inline">\(b\)</span> is a chosen base, <span class="math inline">\(g_i\)</span> is a distinct grade assigned to any element of a partition <span class="math inline">\(S_i\)</span> of <span class="math inline">\(S\)</span>, and <span class="math inline">\(k\)</span> is the number of distinct grades assigned to elements of <span class="math inline">\(S\)</span>.</p>
</blockquote>
<p>It may be desirable to compare grade entropies computed on different systems, but the non-negative real number that results from computing grade entropy in Definition <span class="math inline">\(\ref{def:gradeentropy}\)</span> depends on <span class="math inline">\(m\)</span>. We can utilitize Proposition <span class="math inline">\(\ref{prop:maxentropy}\)</span> to define a normalized grade entropy as given in Definition <span class="math inline">\(\ref{def:normalizedgradeentropy}\)</span>.</p>
<blockquote class="blockquote">
<p><strong>Definition</strong> Given a grade entropy function <span class="math inline">\(H_g\)</span>, the <strong>normalized grade entropy</strong> is given by</p>
<p><span class="math display">\[H_{\gamma} \triangleq \frac{H_g}{\log_b |m|}\]</span></p>
<p>where <span class="math inline">\(b\)</span> is the same base used in <span class="math inline">\(H_g\)</span> and <span class="math inline">\(m\)</span> is the number of points</p>
</blockquote>
<p>The normalization in Definition <span class="math inline">\(\ref{def:normalizedgradeentropy}\)</span> provides a functional with similar properties to the grade entropy in Definition <span class="math inline">\(\ref{def:gradeentropy}\)</span>, but with the additional property of being bounded to the interval <span class="math inline">\([0,1]\)</span>. This allows comparisons of grade entropies for variables with different sizes of outcome space, and also makes it clear that the entropy has reached its maximum when the normalized grade entropy is equal to unity.</p>
<p>Note that Definition <span class="math inline">\(\ref{def:gradeentropy}\)</span> does not suppose what partial order is used, nor does it suppose which probability distribution is used. One might wish to use product orders, lexicographical orders, Pareto orders, or others. And one might wish to consider either empirical or theoretical discrete probability distributions. Grade entropies allow the researcher to choose the order relation and probability distribution that is relevant to their chosen domain.</p>
<p>One limitation of grade entropies is they do not distinguish between partially ordered sets with slightly different non-dominating pairs. Let us consider panel (b) of Figure <span class="math inline">\(\ref{fig:examplegradeentropy}\)</span> as an example. Nodes and would be assigned equal grades, and nodes and would also be assigned equal grades. If the edge <span class="math inline">\(\rightarrow\)</span> was also part of the relation this would not change the aforementioned grades, but sometimes it may be desirable to distinguish between these two versions of the lattice. This motivates a generalization of grade entropies that we term , which is achieved by first defining a <em>pseudograded poset</em> by taking Definition <span class="math inline">\(\ref{def:gradedposet}\)</span> and simply dropping the requirement that <span class="math inline">\(x,y \in S \land x \lessdot y \implies \rho(x) + 1 = \rho(y)\)</span>. The definition of a pseudograde entropy is then identical to Definition <span class="math inline">\(\ref{def:gradeentropy}\)</span> except that a pseudograde function is used instead of a grade function.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>A normalized pseudograde entropy is similarly obtained *mutatis mutandis.</p>
</div>
</div>
<p>Next we consider how to interpret what a grade entropy tells us about a finite set of points.</p>
</section>
<section id="interpretation-4" class="level3" data-number="4.6.5">
<h3 data-number="4.6.5" class="anchored" data-anchor-id="interpretation-4"><span class="header-section-number">4.6.5</span> Interpretation</h3>
<p>In this subsection we offer a general interpretation of grade entropies that should apply regardless of the choice of partial order or distribution of grades.\</p>
<p>Proposition <span class="math inline">\(\ref{prop:maxentropy}\)</span> entails that the grade entropy is maximized when the distribution of grades is uniform. If the grade entropy is maximal for a collection of <span class="math inline">\(m\)</span> points, then the probability measure for each grade is <span class="math inline">\(\frac{1}{m}\)</span> for each of the <span class="math inline">\(m\)</span> points. If we take our probability to be a normalized counting measure over the set of points, then we must conclude that there is exactly one point per grade. If every point has a distinct grade, then we can equivalently state that no two points share a grade.\</p>
<p>If no two points share a grade, then the partial order is also a total order. With the grade entropy being maximized by this uniformity, it can be considered a quantification of the totality of the partial order. Example <span class="math inline">\(\ref{fig:examplegradeentropy}\)</span> illustrates three cases including one in which the partial order is also total, a case where it is not completely total but also has some degree of totality, and a third case in which there is no totality (i.e.&nbsp;no dominance of one point over another).</p>
<p>We can further interpret grade entropies in a way that depends on the choice of order. Let us consider a product order over a finite collection of points in <span class="math inline">\(\mathbb{R}^n\)</span> as an example. If the grade entropy is zero, then for every pair of points there must exist at least one component that violates the comparison. Thus a grade entropy of zero for such a product order tells us that the collection of variables is not comonotonic. A collection of variables are comonotonic if they go up or down together whenever one them goes up or down in value.\footnote{When there exists two variables in which one always decreases as the other increases, and <em>vice versa</em>, this is called <em>antimonotonicity</em>. We are not aware of a term for the case in which some variables within a collection of variables are comonotonic while others are antimonotonic, but we suggest the term <em>anticomonotonic</em> suitably suggests this mixture. Likewise, if the grade entropy is at its maximum then the product order holds for all pairs of points, which entails perfect comonotonicity among those variables. Intermediate values of grade entropy between zero and its maximum would suggest to us a quantification of comonotonicity of a set of variables.</p>
<p>Comonotonicity is related to the Trinity of Covariation when we consider a partial order on points whose components are variables. If the order is total, then the variables must go up and down together in such a way that always satisfies the order relation. Likewise, when the order is entirely non-total the variables never go up and down together in a way that satisfies the order. The <em>change</em> part is implicit in considering the point-to-point comparisons for all points. The part comes from such points be specified by coordinates represented by the components of each tuple. The <em>structure</em> part comes from the notion of a partial order existing on the collection of points.</p>
<p>Interpreting pseudograde entropies is similar to interpreting grade entropies. It holds that a total order maximizes its value, and if no point dominates another then it will take its minimum value of zero. But pseudograde entropies may be larger than their grade entropy counterpart for a given partially ordered set.</p>
<p>Having introduced grade entropies, pseudograde entropies, and their interpretation, we will next summarize the chapter.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">4.7</span> Conclusion</h2>
<p>In this chapter we introduced multilinear correlation coefficients, Nightingale covariance, inner correlations, Agnesian operators, and grade entropies.</p>
<p>Each of these families of functions are an instantiation of the Trinity of Covariation in its own way. Mulilinear correlation relies as averaging over multiplications of random variables, and make use of a recently-proved normalization that generalized the Cauchy Schwarz inequality. Nightingale correlation is similar to multilinear correlations, but imposes non-negativity and have similar properties to seminorms and semimetrics. Inner correlations utilize a generalization of the notion of inner products to multiple variables by constructing normalized Gram determinants. Agnesian operators make direct use the notions of change as either derivatives or net changes and has applications to the notions of parametric curves and surfaces in finite dimension. Lastly, grade entropies combine elementary notions of information theory and order theory.</p>
<p>With these mathematical notions in mind, in the next chapter we introduce a software package that implements their calculation.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Amershi1985" class="csl-entry" role="listitem">
Amershi, Amin H. 1985. <span>“A Complete Analysis of Full Pareto Efficiency in Financial Markets for Arbitrary Preferences.”</span> <em>The Journal of Finance</em> 40 (4): 1235–43. <a href="https://doi.org/10.1111/j.1540-6261.1985.tb02374.x">https://doi.org/10.1111/j.1540-6261.1985.tb02374.x</a>.
</div>
<div id="ref-Athreya2006" class="csl-entry" role="listitem">
Athreya, Krishna B, and Soumen N Lahiri. 2006. <em>Measure Theory and Probability Theory</em>. 2006th ed. Springer Texts in Statistics. New York, NY: Springer.
</div>
<div id="ref-Aubin2014" class="csl-entry" role="listitem">
Aubin, Jean-Pierre. 2014. <em>Mathematical Methods of Game and Economic Theory</em>. 2nd ed. Studies in Mathematics and Its Applications. North-Holland.
</div>
<div id="ref-Thomas2006" class="csl-entry" role="listitem">
Cover, Thomas M., and Joy A Thomas. 2006. <em>Elements of Information Theory</em>. 2nd ed. Nashville, TN: John Wiley &amp; Sons.
</div>
<div id="ref-Emmerich2018" class="csl-entry" role="listitem">
Emmerich, Michael T. M., and André H. Deutz. 2018. <span>“A Tutorial on Multiobjective Optimization: Fundamentals and Evolutionary Methods.”</span> <em>Natural Computing</em> 17 (3): 585–609. <a href="https://doi.org/10.1007/s11047-018-9685-y">https://doi.org/10.1007/s11047-018-9685-y</a>.
</div>
<div id="ref-Greub1978" class="csl-entry" role="listitem">
Greub, Werner. 1978. <em>Multilinear Algebra</em>. Springer New York. <a href="https://doi.org/10.1007/978-1-4613-9425-9">https://doi.org/10.1007/978-1-4613-9425-9</a>.
</div>
<div id="ref-Jaynes1965" class="csl-entry" role="listitem">
Jaynes, E. T. 1965. <span>“Gibbs Vs Boltzmann Entropies.”</span> <em>American Journal of Physics</em> 33 (5): 391–98. <a href="https://doi.org/10.1119/1.1971557">https://doi.org/10.1119/1.1971557</a>.
</div>
<div id="ref-Lambert2002" class="csl-entry" role="listitem">
Lambert, Frank L. 2002. <span>“Disorder - a Cracked Crutch for Supporting Entropy Discussions.”</span> <em>Journal of Chemical Education</em> 79 (2): 187. <a href="https://doi.org/10.1021/ed079p187">https://doi.org/10.1021/ed079p187</a>.
</div>
<div id="ref-Nantomah2017" class="csl-entry" role="listitem">
Nantomah, Kwara. 2017. <span>“Generalized Hölder’s and Minkowski’s Inequalities for Jackson’sq-Integral and Some Applications to the Incompleteq-Gamma Function.”</span> <em>Abstract and Applied Analysis</em> 2017: 1–6. <a href="https://doi.org/10.1155/2017/9796873">https://doi.org/10.1155/2017/9796873</a>.
</div>
<div id="ref-Stanley2011" class="csl-entry" role="listitem">
Stanley, Richard P. 2011. <em>Cambridge Studies in Advanced Mathematics Enumerative Combinatorics: Series Number 49: Volume 1</em>. 2nd ed. Cambridge, England: Cambridge University Press.
</div>
<div id="ref-Sykora2009" class="csl-entry" role="listitem">
Sykora, Stanislav. 2009. <span>“Mathematical Means and Averages: Basic Properties.”</span> <em>Stan’s Library</em>, no. Volume III (July). <a href="https://doi.org/10.3247/sl3math09.001">https://doi.org/10.3247/sl3math09.001</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./previous.html" class="pagination-link" aria-label="Previous Instantiations of the Trinity of Covariation">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Previous Instantiations of the Trinity of Covariation</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./software.html" class="pagination-link" aria-label="ConAction">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">ConAction</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>