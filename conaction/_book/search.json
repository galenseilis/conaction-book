[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ConAction, Second Edition.",
    "section": "",
    "text": "Prefaces",
    "crumbs": [
      "Prefaces"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Complexity\nDefining “complexity” can seem ironic because it turns out to be a ‘complex’ endeavour. But in this section we will attempt to explain some of its character, and why it is a challenge in the sciences. The complexity of a system generally comes from a combination of two sources: the system’s state and the system’s state transition rules.\\\nFormal systems are structures like sets, sequences, vectors, scalars, matrices, tensors, graphs, hypergraphs, or any of a myriad of algebraic structures or spaces. In the simple case of a set as a system, one can think of the elements of the set as its state along with some rules for changing the elements of the set. Two contrasting examples of formal systems are cellular automata and systems of diffrential equations.\\\nFigure \\(\\ref{fig:statetransitionexample}\\) (a) shows an elementary (1-dimensional) cellular automata known as Rule 110. In such a system the entire state of the system is represented as a one-hot vector (i.e. a vector of zeros and ones). In such a system time can be thought of discretely as a count of the number of applications of the rules, which are applied simultaneously on the current state of the system to get the next state. The state transition rules dictate how a given bit is replaced with another depending on its value as well as the values of its left and right neighbours. It is possible prima facie to believe that such systems are only capable of the simplest of patterns based on the belief that simple rules imply simple behaviour. But this is not so. In 2004 Mathew Cook published a proof that Rule 110 is Turing complete (Cook (2004)), i.e. equivalent to a Turing machine, a classification indicating that Rule 110 can run any program depending on the size and configuration of its input. As a practical point of comparison, a phone or a PC can be entirely represented by a Turing machine. This includes everything from playing your favourite music or video, playing the latest computer games, calling a friend, running advanced scientific simulations, and even representing the entirety of whatever the Internet is doing right at this moment. Thus complexity really can come from simple rules with a suitable system configuration.\nStephen Wolfram has argued that some systems, including those with simple rules and relatively small state spaces, can still be what he calls “”(Wolfram (2002)). Computational irreducibility is the notion that there exists formal systems in which, even with the current state and the state transition rules on hand, there does not exist a more efficient way of knowing the future state than to run the complete sequence of iterations of the rules. Fortunately, if we consider a well-defined state with state transitions rules at time \\(t\\), we can always (within the practicalities of computation on real computers) compute what the state will be at time \\(t+\\tau\\) by iteratively applying the transition rules \\(\\tau\\) number of times. Less fortunately, there are systems that have undecidable problems in the sense that they cannot be answered in a finite number of steps.\nDiscrete (countable) systems are not the only types of formal systems that can be characterized by a current state combined with state transition rules, leading us to the notion of differential equations.\nFigure \\(\\ref{fig:statetransitionexample}\\) (b) shows the Lorenz system, which is a system of differential equations in 3 dimensions that was originally inspired by weather models . A dynamical system has a formal definition, but we will treat it informally here as a system that evolves with time according to differential equations. Like the example with elementary cellular automata, there is a well-defined state at any given point in time. But unlike these discrete state space systems, the Lorenz system has a continuous state space. This has the non-intuitive consequence that there is no “next state”, however a collection of governing equations prescribe how the system evolves through the space of states. The Lorenz system exhibits a different notion of complexity: chaos. Chaos can informally be defined as the state of the system at some future time being highly sensitive to the initial conditions that the system began with, however this may understate the severity of the sensitivity. There exists systems for which any finite precision in the measurements of the state of the system will eventually be insufficient to predict future states after a finite amount of time. This is not the same thing as randomness per se because the underlying dynamics are deterministic. Rather this is extrinsic randomness due to not knowing the state of the system in enough detail.\nWhile we often model natural systems using formal systems, they are not synonymous. Therefore it worth exemplifying some natural systems. We will describe some natural systems in which complexity makes scientific progress difficult.\nA human brain is an immensely complex organ whose state transitions are measure dependent with human decision making and behaviour. It contains roughly 85 billion neuron cells (Azevedo et al. (2009)), and some believe it has \\(\\sim 10^{15}\\) connections between them (DeWeerdt (2019)). The brain seems to function via electrical-chemical signals that travel along and between neurons, but with such large numbers of components involved it is difficult to determine precisely what any selection of neurons is actually doing.\nThere exists some 25 thousand genes in the human genome. While this number is quite modest compared to the number of neurons in the human brain, the behaviour of the human genome as a whole is still an ongoing area of research. Molecular biologists and biochemists have had immense success in mapping certain genes to biological functions, and it has helped considerably that genes code for RNA and proteins that can be studied for structure-related function. Molecular components of gene regulatory networks work via chemical diffusion. A consequence of molecules travelling via chemical diffusion is that they go everywhere within their cellular compartment and sometimes beyond. This is especially the case when other mechanisms of transport such as channel protein transport and active transport proteins play a role. It can still be difficult to predict what a given gene regulatory network will do in a cell because of unforeseen chemical interactions (Karkaria et al. (2020)).\nWhat is true of both formal and natural systems is that complexity arises from a combination of its state at some time and the state transition rules. Even if a system is deterministic, we may not have precise enough measurements, there are likely to be missing variables, and some problems are undecidable. Often our empirical measurements do not capture the whole state of the system, but rather represent statistical data about the system.\nFor example, temperature measurements sample some aggregate quantity of molecules with varying kinetic energy. Likewise with the brain, scanning technologies such as functional magnetic resonance imaging and positron emission tomography allow us to understand aggregate behaviours of neurons in terms of regions of the brain.\nThis motivates the need for mathematical models to yield insights about systems for which we do not know how to measure in precise totality. Indeed, the need for such models may be an indication that we are dealing with the current limits of our understanding. We will delve into rigorous thinking on new mathematical models of this type in later sections, but in the next section we wish to point to something of great imprecision and importance in our experience of living in a complex world: the Trinity of Covariation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ankerst, Mihael, Markus M. Breunig, Hans-peter Kriegel, and Jörg Sander.\n1999. “OPTICS: Ordering Points to Identify the Clustering\nStructure.” In, 49–60. ACM Press.\n\n\nAthreya, Krishna B, and Soumen N Lahiri. 2006. Measure Theory and\nProbability Theory. 2006th ed. Springer Texts in Statistics. New\nYork, NY: Springer.\n\n\nAzevedo, Frederico A. C., Ludmila R. B. Carvalho, Lea T. Grinberg, José\nMarcelo Farfel, Renata E. L. Ferretti, Renata E. P. Leite, Wilson Jacob\nFilho, Roberto Lent, and Suzana Herculano-Houzel. 2009. “Equal\nNumbers of Neuronal and Nonneuronal Cells Make the Human Brain an\nIsometrically Scaled-up Primate Brain.” The Journal of\nComparative Neurology 513 (5): 532–41. https://doi.org/10.1002/cne.21974.\n\n\nBaba, Kunihiro, Ritei Shibata, and Masaaki Sibuya. 2004. “Partial\nCorrelation and Conditional Correlation as Measures of Conditional\nIndependence.” Blackwell Publishing Asia Pty Ltd. 46\n(4): 657–64. https://doi.org/10.1111/j.1467-842x.2004.00360.x.\n\n\nBacaër, Nicolas. 2011. “Verhulst and the Logistic Equation\n(1838).” In A Short History of Mathematical Population\nDynamics, 35–39. Springer London. https://doi.org/10.1007/978-0-85729-115-8_6.\n\n\nBoone, Celia K., Kirsten M. Thompson, Philippe Henry, and Brent W.\nMurray. 2022. “Host Use Does Not Drive Genetic Structure of\nMountain Pine Beetles in Western North America,” June. https://doi.org/10.1101/2022.06.28.498011.\n\n\nBravais, Auguste. 1844. Analyse Mathematique Sur Les Probabilites\nDes Erreurs de Situation d’un Point. Paris Royal Printing. https://books.google.ca/books?id=PJl-mgEACAAJ.\n\n\nBurleigh, Jennifer. 2014. Field Guide to Forest Damage in British\nColumbia. Victoria, B.C: Ministry of Forests, Lands; Natural\nResource Operations.\n\n\nComaniciu, D., and P. Meer. 2002. “Mean Shift: A Robust Approach\nToward Feature Space Analysis.” IEEE\nTransactions on Pattern Analysis and Machine Intelligence 24 (5):\n603–19. https://doi.org/10.1109/34.1000236.\n\n\nCook, Mathew. 2004. “Universality in Elementary Cellular\nAutomata.” Complex Systems, 1–40.\n\n\nCorbett, L. J., P. Withey, V. A. Lantz, and T. O. Ochuodho. 2015.\n“The Economic Impact of the Mountain Pine Beetle Infestation in\nBritish Columbia: Provincial Estimates from a CGE\nAnalysis.” Forestry 89 (1): 100–105. https://doi.org/10.1093/forestry/cpv042.\n\n\nDechter, Rina, and Judea Pearl. 1985. “Generalized Best-First\nSearch Strategies and the Optimality of a*.” Journal of the\nACM 32 (3): 505–36. https://doi.org/10.1145/3828.3830.\n\n\nDeWeerdt, Sarah. 2019. “How to Map the Brain.” Nature\nOutlook 571 (7766): S6–8. https://doi.org/10.1038/d41586-019-02208-0.\n\n\nDoran, J. E., and D. Michie. 1966. “Experiments with the Graph\nTraverser Program.” Proceedings of the Royal Society of\nLondon. Series A. Mathematical and Physical Sciences 294 (1437):\n235–59. https://doi.org/10.1098/rspa.1966.0205.\n\n\nEster, Martin, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu. 1996.\n“A Density-Based Algorithm for Discovering Clusters in Large\nSpatial Databases with Noise.” In KDD.\n\n\nFisher, Ronald Aylmer. 1924. “The Distribution of the Partial\nCorrelation Coefficient.” Metron 3: 329–32. https://digital.library.adelaide.edu.au/dspace/handle/2440/15182.\n\n\nFrey, Brendan J., and Delbert Dueck. 2007. “Clustering by Passing\nMessages Between Data Points.” Science 315 (5814):\n972–76. https://doi.org/10.1126/science.1136800.\n\n\nGalton, Francis. 1888. “Co-Relations and Their Measurement,\nChiefly from Anthropometric Data.” Proceedings of the Royal\nSociety of London 45: 135–45. http://www.jstor.org/stable/114860.\n\n\nGomperz, Benjamin. 1825. “XXIV. On the Nature of the\nFunction Expressive of the Law of Human Mortality, and on a New Mode of\nDetermining the Value of Life Contingencies. In a Letter to Francis\nBaily, Esq. F. R. S. &c.” Philosophical\nTransactions of the Royal Society of London 115 (December): 513–83.\nhttps://doi.org/10.1098/rstl.1825.0026.\n\n\nGreub, Werner. 1978. Multilinear Algebra. Springer New York. https://doi.org/10.1007/978-1-4613-9425-9.\n\n\nHitchcock, Christopher, and Miklós Rédei. 2021. “Reichenbach’s Common Cause Principle.” In\nThe Stanford Encyclopedia of Philosophy, edited by\nEdward N. Zalta, Summer 2021. https://plato.stanford.edu/archives/sum2021/entries/physics-Rpcc/;\nMetaphysics Research Lab, Stanford University.\n\n\nHotelling, Harold. 1953. “New Light on the Correlation Coefficient\nand Its Transforms.” Journal of the Royal Statistical\nSociety: Series B (Methodological) 15 (2): 193–225. https://doi.org/10.1111/j.2517-6161.1953.tb00135.x.\n\n\nHouwer, Jan De. 2018. “Eelen 1980 Classical Conditioning.”\nPreprint, May. https://doi.org/10.31234/osf.io/vp5cu.\n\n\nHuber, Peter J. 1964. “Robust Estimation of a Location\nParameter.” The Annals of Mathematical Statistics 35\n(1): 73–101. https://doi.org/10.1214/aoms/1177703732.\n\n\nKarkaria, Behzad D., Neythen J. Treloar, Chris P. Barnes, and Alex J. H.\nFedorec. 2020. “From Microbial Communities to Distributed\nComputing Systems.” Frontiers Bioengineering\nBiotechnology 8 (July). https://doi.org/10.3389/fbioe.2020.00834.\n\n\nLangford, Eric, Neil Schwertman, and Margaret Owens. 2001. “Is the\nProperty of Being Positively Correlated Transitive?” The\nAmerican Statistician 55 (4): 322–25. https://doi.org/10.1198/000313001753272286.\n\n\nLawson, Charlse Lawrence. 1961. “Contribution to the Theory of\nLinear Least Maximum Approximations.” PhD thesis, University of\nCalifornia at Los Angeles.\n\n\nMill, John Stuart. 1843. A System of Logic, Ratiocinative and\nInductive. Cambridge University Press. https://doi.org/10.1017/cbo9781139149839.\n\n\nMonod, Jacques. 1949. “The Growth of Bacterial\nCultures.” Annual Review of Microbiology 3\n(1): 371–94. https://doi.org/10.1146/annurev.mi.03.100149.002103.\n\n\nNantomah, Kwara. 2017. “Generalized Hölder’s and Minkowski’s\nInequalities for Jackson’sq-Integral and Some Applications to the\nIncompleteq-Gamma Function.” Abstract and Applied\nAnalysis 2017: 1–6. https://doi.org/10.1155/2017/9796873.\n\n\nOlkin, Ingram, and John W. Pratt. 1958. “Unbiased Estimation of\nCertain Correlation Coefficients.” The Annals of Mathematical\nStatistics 29 (1): 201–11. https://doi.org/10.1214/aoms/1177706717.\n\n\nPauly, D., and G. R. Morgan. 1987. Length-Based Methods in Fisheries\nResearch. Makati, Metro Manila, Philippines Safat, Kuwait:\nInternational Center for Living Aquatic Resources Management Kuwait\nInstitute for Scientific Research.\n\n\nPeakall, Rod, and Peter E. Smouse. 2006. “Genalex 6: Genetic\nAnalysis in Excel. Population Genetic Software for Teaching and\nResearch.” Molecular Ecology Notes 6 (1): 288–95. https://doi.org/10.1111/j.1471-8286.2005.01155.x.\n\n\nPearson, Karl. 1895. “Note on Regression and Inheritance in the\nCase of Two Parents.” Proceedings of the Royal Society of\nLondon Series I 58 (January): 240–42.\n\n\nPiovani, Juan Ignacio. 2007. “The Historical Construction of\nCorrelation as a Conceptual and Operative Instrument for Empirical\nResearch.” Quality & Quantity 42 (6):\n757–77. https://doi.org/10.1007/s11135-006-9066-y.\n\n\nRichards, F. J. 1959. “A Flexible Growth Function for Empirical\nUse.” Journal of Experimental Botany 10 (2): 290–301. https://doi.org/10.1093/jxb/10.2.290.\n\n\nSaab, Victoria A., Quresh S. Latif, Mary M. Rowland, Tracey N. Johnson,\nAnna D. Chalfoun, Steven W. Buskirk, Joslin E. Heyward, and Matthew A.\nDresser. 2014. “Ecological Consequences of Mountain Pine Beetle\nOutbreaks for Wildlife in Western North American Forests.”\nForest Science 60 (3): 539–59. https://doi.org/10.5849/forsci.13-022.\n\n\nSerafino, Loris. 2016. “On the de Finetti’s Representation\nTheorem: An Evergreen (and Often Misunderstood) Result at the Foundation\nof Statistics.” http://philsci-archive.pitt.edu/12059/.\n\n\nShi, Jianbo, and J. Malik. 2000. “Normalized Cuts and Image\nSegmentation.” IEEE Transactions on Pattern Analysis and\nMachine Intelligence 22 (8): 888–905. https://doi.org/10.1109/34.868688.\n\n\nSteinhaus, Hugo. 1957. “Sur La Division Des Corps Matériels En\nParties.” Bulletin L’Académie Polonaise Des Science 4\n(12): 801–4. https://scirp.org/reference/referencespapers.aspx?referenceid=2408792.\n\n\nTabatabai, Mohammad, David Williams, and Zoran Bursac. 2005.\n“Hyperbolastic Growth Models: Theory and Application.”\nTheoretical Biology and Medical Modelling 2 (1): 14. https://doi.org/10.1186/1742-4682-2-14.\n\n\nVirtanen, Pauli, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler\nReddy, David Cournapeau, Evgeni Burovski, et al. 2020. “SciPy 1.0: Fundamental Algorithms for\nScientific Computing in Python.” Nature Methods\n17: 261–72. https://doi.org/10.1038/s41592-019-0686-2.\n\n\nWard, Joe H. 1963. “Hierarchical Grouping to Optimize an Objective\nFunction.” Journal of the American Statistical\nAssociation 58 (301): 236–44. https://doi.org/10.1080/01621459.1963.10500845.\n\n\nWeisstein, Eric W. 1999. “Wolfram MathWorld: The Web’s Most\nExtensive Mathematics Resource. From MathWorld–a Wolfram Web\nResource.” Wolfram Research, Inc. https://mathworld.wolfram.com/.\n\n\nWolfram, Stephen. 2002. A New Kind of Science. Champaign, IL:\nWolfram Media.\n\n\nWooldridge, Jeffrey. 2009. Introductory Econometrics : A Modern\nApproach. Mason, OH: South Western, Cengage Learning.\n\n\nWright, Sewall. 1921. “Correlation and Causation.”\nJournal of Agriculture Research 20: 557–85."
  },
  {
    "objectID": "abstract.html",
    "href": "abstract.html",
    "title": "1  Abstract",
    "section": "",
    "text": "Complexity poses a pervasive challenge in understanding formal and natural systems which can arise from a combination of a system’s state and state transition rules. In situations in which many aspects of a system are changing together, it is desirable to quantify how much they do so. We motivate and define five mathematical functions that can be used to quantify coordinated changes in structure. We also developed ConAction, a Python package which implements these novel mathematical tools in a way that is performant, easy to install, and easy to use. These new tools can be applied to real research problems, which we exemplified by evaluating a classic isolation by distance model for Dendroctonus ponderosae populations in western North America.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Abstract</span>"
    ]
  },
  {
    "objectID": "previous.html",
    "href": "previous.html",
    "title": "3  Previous Instantiations of the Trinity of Covariation",
    "section": "",
    "text": "3.1 Correlation\n“Correlation” is an instance of The Trinity of Covariation. It quantifies ‘change’ in the form of deviations from a mean value of a random variable. It captures a notion of ‘structure’ in the sense of quantifying something about the joint distribution of two variables. And it captures a notion of ‘coordination’ in a direct sense of paired values (i.e. points or coordinates) whose instances can be specified by an index set.\nFrancis Galton, one of the important thinkers in the history of correlation, made a statement about his calculations of ‘co-relation’ of ‘variable organs’ (given in Figure 3.1 with his portrait) that echoes the quote on causality from John Stuart Mill given in Section \\(\\ref{sec:trinityofcovariation}\\). Here, Galton is literally referring to biological and physical measurements of organs of the human body Galton (1888).\nIn this quotation Galton says, “and in the same direction”. While most students of statistics today understand the notion of negative correlation, it may have been the case that the properties that Galton studied all had positive correlation. Perhaps he did not notice the possibility of negative correlation. But it is clear from his wording of ‘variation of the one is accompanied on the average by more or less of the other’ that he is thinking of coordinated change in empirical quantities.\nThe history of correlation is complicated, with many participants that will not be mentioned here. See Figure 3.2 for some of them. For a more detailed discussion, see Piovani (2007). Correlation began with the study of error theory which endevoured to form a mathematical description of the error of models. This theory of errors led to least-squares analysis, which was pioneered by the rivals Carl Friedrich Gauss and Adrien-Marie Legendre Piovani (2007). While generations of undergraduate students have associated correlation with Karl Pearson, he neither developed the first mathematical formulation nor the interpretation of ‘co-relation’ of variables. Auguste Bravais purportedly was the first to write down a mathematically-equivalent expression to Pearson’s correlation coefficient Wright (1921), however he did not provide any interpretation of statistical association to his calculations. Rather Bravais was focused on developing joint normal distributions, and what we would now consider to be correlation was only discussed in his work as an angle between residual vectors Bravais (1844). It was Francis Galton that found an intuitive interpretation in his own tabular way of computing correlation as ‘co-relation’ Stigler1989, and it was Karl Pearson (Pearson (1895)) who later formalized the product-moment formula that students would recognize today.\n\\[\\begin{equation}\n\\text{Corr}\\left[ X, Y\\right] \\triangleq \\frac{\\mathbb{E}\\left[ \\left( X - \\mathbb{E}\\left[ X \\right] \\right) \\left( Y - \\mathbb{E}\\left[ Y \\right] \\right) \\right]}{\\sqrt{\\mathbb{E}\\left[\\left( X - \\mathbb{E}\\left[ X \\right] \\right)^2 \\right] \\mathbb{E}\\left[\\left( Y - \\mathbb{E}\\left[ Y \\right] \\right)^2 \\right]}}\n\\end{equation}\\]\nThe \\(\\mathbb{E}\\) operator in the above expression denotes the expectation operator, which can be defined as \\[\\mathbb{E}[X] \\triangleq \\int_{\\Omega} x dF\\]\nwhere \\(X\\) is a random variable, \\(\\Omega\\) is the set of outcomes for \\(X\\), and \\(F\\) is the cumulative distribution function of \\(F\\). In a sense the expectation operator can be thought of as a kind of weighted average. We will make use of the notion of the expectation multiple times throughout the thesis.\nThe Pearson correlation coefficient is one of the most used functions in applied statistics, and its numerator \\(\\operatorname{Cov}[X,Y] \\triangleq \\mathbb{E}\\left[ \\left( X - \\mathbb{E}\\left[ X \\right] \\right) \\left( Y - \\mathbb{E}\\left[ Y \\right] \\right) \\right]\\) is the of two random variables in the conventional sense. Table 3.1 lists some of the basic properties of Pearson’s correlation coefficient, along with the less-well-known result due Langford, Schwertman, and Owens (2001) that only very strong correlations can be used in a transitive-like way.\nThe notation \\(\\perp\\!\\!\\!\\!\\perp (X, Y)\\) in Table 3.1 denotes that two random variables are statistically independent. Statistical independence is a property of a collection of random variables such that their joint cumulative distribution function is equal to the product of their marginal cumulative distribution functions. The notion of statistical independence will become important in interpreting specific mathematical topics later in this thesis.\\\nAnother important statistical notion is that of the bias of an estimator. An estimator is function that can be computed on a sample to estimate the value of a population parameter. The notation \\(\\mathbb{E}_{X|\\rho}[\\hat{\\rho} - \\rho] \\neq 0\\) in Table 3.1 denotes that the average difference across samples between an estimate \\(\\hat{\\rho}\\) of the parameter population \\(\\rho\\) will not equal zero.\\\nThere exists a generalization of Pearson’s correlation coefficient that is also an instantiation of The Trinity of Covariation. It is discussed later in this thesis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Previous Instantiations of the Trinity of Covariation</span>"
    ]
  },
  {
    "objectID": "previous.html#correlation",
    "href": "previous.html#correlation",
    "title": "3  Previous Instantiations of the Trinity of Covariation",
    "section": "",
    "text": "Francis Galton (Public Domain)\n\n\n\n\n\nTwo variable organs are said to be co-related when the variation of the one is accompanied on the average by more or less of the other, and in the same direction.\n\n\n\n\n\nFigure 3.1: Portrait of Francis Galton and a quote from his work on biometrics.\n\n\n\n\n\n\n\n\n\n\n\n\nAuguste Bravais (Public Domain)\n\n\n\n\n\n\n\nFrancis Galton (Public Domain)\n\n\n\n\n\n\n\nKarl Pearson (Public Domain)\n\n\n\n\n\n\nFigure 3.2: The historical founders of the intuitive and mathematical notions that led to our modern formulations of correlation.\n\n\n\n\n\n\n\n\n\n\n\nTable 3.1: Properties of Pearson’s Product-Moment Correlation Coefficient\n\n\n\n\n\n\n\n\n\n\nName\nExpression\nComments\n\n\n\n\nTranslational invariance\n\\(\\text{Corr}\\left[ X, Y\\right] = \\text{Corr}\\left[ X + c, Y\\right]\\)\n\\(c \\in \\mathbb{R}\\)\n\n\nAbsolute Invariance of Scaling\n\\(\\text{Corr}\\left[ X, Y\\right] = \\text{Corr}\\left[ \\alpha X, Y\\right]\\)\n\\(\\alpha \\in \\mathbb{R}_{&gt;0}\\)\n\n\nSymmetric\n\\(\\text{Corr}\\left[ X, Y\\right] = \\text{Corr}\\left[ Y, X\\right]\\)\n\n\n\nBounded\n\\(-1 \\leq \\text{Corr}\\left[ X, Y\\right]  \\leq 1\\)\n\n\n\nDependence Indicator\n\\(\\perp\\!\\!\\!\\!\\perp (X, Y) \\implies \\text{Corr}\\left[ X, Y\\right] = 0\\)\nThe converse is not true.\n\n\nConditional Transitivity\n\\(\\text{Corr}\\left[ X, Y\\right]^2 + \\text{Corr}\\left[ Y,Z\\right]^2 &gt; 1 \\implies \\text{Corr}\\left[ X, Z\\right]^2 &gt; 0\\)\nLangford, Schwertman, and Owens (2001)\n\n\nEven Function\n\\(\\text{Corr}\\left[ X, Y\\right] = \\text{Corr}\\left[ -X, -Y\\right]\\)\n\n\n\nBiased\n\\(\\mathbb{E}_{X|\\rho}[\\hat{\\rho} - \\rho] \\neq 0\\)\nHotelling (1953), Olkin and Pratt (1958)\n\n\n\n\n\n\n\n\n\n\n\n\nSome Clarifications About Bias\n\n\n\n\n\nAbove we state that correlation is biased, however this has only been shown to be the case for certain families of probability distributions.\nFurther, it is important to distinguish between the estimand as the population correlation \\(\\rho\\), the estimate \\(\\hat \\rho\\), and the estimator which assigns an estimate to each sample.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Previous Instantiations of the Trinity of Covariation</span>"
    ]
  },
  {
    "objectID": "previous.html#cokurtosis-coskewness-and-other-standardized-mixed-product-moments",
    "href": "previous.html#cokurtosis-coskewness-and-other-standardized-mixed-product-moments",
    "title": "3  Previous Instantiations of the Trinity of Covariation",
    "section": "3.2 Cokurtosis, Coskewness, and Other Standardized Mixed-Product Moments",
    "text": "3.2 Cokurtosis, Coskewness, and Other Standardized Mixed-Product Moments\nPearson’s product-moment correlation is one of the more famous and historically precedented examples of standardized mixed-product moments. Such functions have the form given in Definition \\(\\ref{def:standardmixedproductmoment}\\), which can be shown to be a generalization of Definition \\(\\ref{def:pearsoncorrelation}\\).\\\nWhen \\(n=3\\) in Definition \\(\\ref{def:standardmixedproductmoment}\\) the statistic is called the (Miller (2013)) of those random variables, while if \\(n=4\\) the statistic is called the (Miller (2013)).\nThis family of statistics are instances of the Trinity of Covariation in a very similar way in which the Pearson product-moment correlation is through computing expectations of products of deviations from their univariate expectations. One of the new instances presented in this work is similar to these standardized mixed-product moments but involves a different normalization.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Previous Instantiations of the Trinity of Covariation</span>"
    ]
  },
  {
    "objectID": "previous.html#interaction-effects",
    "href": "previous.html#interaction-effects",
    "title": "3  Previous Instantiations of the Trinity of Covariation",
    "section": "3.3 Interaction Effects",
    "text": "3.3 Interaction Effects\nInteraction effects are in common usage in the context of linear models. They were motivated by the non-additivity of treatment effects . In the context of linear models an interaction effect is taken to be the linear regression coefficient of a term called an . An interaction term is a predictor defined by the product of a collection of random variables.\nLet us consider an example to clarify the concept. Let \\(X,Y,Z\\) be random variables defined in a linear model to be \\(Y = \\beta_0 + \\beta_1 X + \\beta_2 Z + \\beta_3 X Z\\). Here \\(\\beta_0\\) is an intercept term, and \\(\\beta_1\\) and \\(\\beta_2\\) represent the main effects of \\(X\\) and \\(Z\\) on \\(Y\\), respectively. The interaction term is \\(\\beta_3 XZ\\), and the interaction effect is the coefficient \\(\\beta_3\\). In other words, an interaction effect is a parameter that specifies how \\(X\\) and \\(Z\\) have a coordinated effect on \\(Y\\).\nInteraction effects are not limited to pairs of predictor variables, and different interaction effects can be included in the same model where their distinctiveness is obtained from the choice of variables `interacting’.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Previous Instantiations of the Trinity of Covariation</span>"
    ]
  },
  {
    "objectID": "previous.html#derivatives-and-differential-operators",
    "href": "previous.html#derivatives-and-differential-operators",
    "title": "3  Previous Instantiations of the Trinity of Covariation",
    "section": "3.4 Derivatives and Differential Operators",
    "text": "3.4 Derivatives and Differential Operators\nPerhaps no subject has so thoroughly studied change as that of calculus, and so it is of little surprise that instances of the Trinity of Covariation can be found within it.\nThe derivative is a central concept to the subject of calculus, and is an instance of the Trinity of Covariation. It describes how one quantity changes whenever a second quantity changes. Not only are these changes coordinated, but are by definition an instantaneous comparison.\nIn any introductory course in differential calculus, related rates problems are presented to the student. It is sometimes a student’s first introduction to applications of both calculus and the chain rule of derivatives. A classic example is that of the leaning ladder, which relates certain rates of change through the Pythagorean theorem. In terms of the metaphor, the structure is the collection of coordinates describing the bottom and top of the ladder with respect to the ground and the wall. The notion of change here is simply the derivatives, and the notion of coordination here comes from the Pythagorean theorem. Via the chain rule of derivatives, we can consider the total derivatives of the quantities of interest.\nBeyond the derivative of scalar functions are a variety of differential operators that capture collections of rates of change in a coordinated way. A function with a scalar image might also have a gradient, which describes the partial derivatives with respect to each input. The gradient is a staple of vector calculus, and modern machine learning. The Jacobian matrix is a construction of how one set of scalars change with respect to another set of scalars, which can provide a description of how when a change in coordinate system is performed it can be transformed into a change in another coordinate system. Another operator is the Hessian, which considers all of the second-order partial derivatives of a scalar-imaged function and whose eigenvalue decomposition can inform us of the convexity of a surface. These and many other differential operators extend the applicability and scope of derivatives by considering how comparisons of multiple quantities change together.\n\n\n\n\nBravais, Auguste. 1844. Analyse Mathematique Sur Les Probabilites Des Erreurs de Situation d’un Point. Paris Royal Printing. https://books.google.ca/books?id=PJl-mgEACAAJ.\n\n\nGalton, Francis. 1888. “Co-Relations and Their Measurement, Chiefly from Anthropometric Data.” Proceedings of the Royal Society of London 45: 135–45. http://www.jstor.org/stable/114860.\n\n\nHotelling, Harold. 1953. “New Light on the Correlation Coefficient and Its Transforms.” Journal of the Royal Statistical Society: Series B (Methodological) 15 (2): 193–225. https://doi.org/10.1111/j.2517-6161.1953.tb00135.x.\n\n\nLangford, Eric, Neil Schwertman, and Margaret Owens. 2001. “Is the Property of Being Positively Correlated Transitive?” The American Statistician 55 (4): 322–25. https://doi.org/10.1198/000313001753272286.\n\n\nMiller, Michael. 2013. Mathematics and Statistics for Financial Risk Management. Hoboken, NJ: John Wiley & Sons.\n\n\nOlkin, Ingram, and John W. Pratt. 1958. “Unbiased Estimation of Certain Correlation Coefficients.” The Annals of Mathematical Statistics 29 (1): 201–11. https://doi.org/10.1214/aoms/1177706717.\n\n\nPearson, Karl. 1895. “Note on Regression and Inheritance in the Case of Two Parents.” Proceedings of the Royal Society of London Series I 58 (January): 240–42.\n\n\nPiovani, Juan Ignacio. 2007. “The Historical Construction of Correlation as a Conceptual and Operative Instrument for Empirical Research.” Quality & Quantity 42 (6): 757–77. https://doi.org/10.1007/s11135-006-9066-y.\n\n\nWright, Sewall. 1921. “Correlation and Causation.” Journal of Agriculture Research 20: 557–85.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Previous Instantiations of the Trinity of Covariation</span>"
    ]
  },
  {
    "objectID": "new.html",
    "href": "new.html",
    "title": "4  New Instantiations of The Trinity of Covariation",
    "section": "",
    "text": "4.1 Multilinear Correlation",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>New Instantiations of The Trinity of Covariation</span>"
    ]
  },
  {
    "objectID": "new.html#multilinear-correlation",
    "href": "new.html#multilinear-correlation",
    "title": "4  New Instantiations of The Trinity of Covariation",
    "section": "",
    "text": "4.1.1 Derivations\nThe covariance of two random variables is conventionally defined as their centered mixed product moment. Just as the covariance in the standard sense captures something about simultaneous changes in two random variables, we explored generalizing this notion to multiple variables beyond two. We decided that a natural way to accomplish this was to compute the centered mixed product moment of a collection of random variables. To emphasize in this work that we are focusing on a generalization of covariance, which is bilinear, we will refer to this generalization as multilinear covariance as given in the following definition.\n\nDefinition\nGiven a finite collection of real-valued random variables \\(\\{X_1, \\cdots, X_n\\}\\), their multilinear covariance is defined to be the following. \\[\\text{Cov}\\left[ X_1, \\cdots, X_n \\right] \\triangleq \\mathbb{E}\\left[ \\prod_{j=1}^{n} \\left(X_j - \\mathbb{E}[X_j] \\right) \\right]\\]\n\nThis definition in standard literature bears names such as centered mixed product moment and centered cross-product moment, which are both comparably verbose to multilinear covariance. The qualifier cross-product can confuse readers into considering vector cross-products if clarifications are not given, while the former is technically accurate but communicates little intuition about what is being quantified.\nBilinearity is a property of covariance in the standard sense, and we wished to have multilinearity in the generalized sense. However, just because we gave a function the adjective “multilinear” does not make it multilinear in the desired sense of a multilinear map. See Definition \\(\\ref{def:multilinearmap}\\) for a definition of multilinear as given by Greub (1978).\n\nDefinition (Greub (1978))\nLet \\(E_1, \\cdots, E_{p}\\) be a collection of vector spaces, and also let \\(G\\) be a vector space. A map \\(\\phi: E_1 \\times \\cdots \\times E_p \\mapsto G\\) is called \\(p\\)-linear if for every \\(i \\in \\{1, \\cdots, p \\}\\) and scalars \\(\\alpha\\) and \\(\\beta\\) it holds that \\[\\begin{align*}\n\\phi (x_1, \\cdots, x_{i-1}, \\alpha x_i + \\beta y_i, x_{i+1}, \\cdots, x_p) &= \\alpha \\phi (x_1, \\cdots, x_{i-1}, x_i, x_{i+1}, \\cdots, x_p) \\\\\n&  + \\beta \\phi (x_1, \\cdots, x_{i-1}, y_i, x_{i+1}, \\cdots, x_p).\n\\end{align*}\\]\n\nIn the following proposition we claim and prove that multilinear covariance is multilinear in a mathematical sense. We took an approach of showing that the multilinear covariance of linear combinations of random variables is itself a linear combination of the multilinear covariances on those random variables.\n\n\n\n\n\n\nScreen Size\n\n\n\n\n\nYou may need to view the following result on a larger screen to see it in its entirety.\n\n\n\n\nProposition\n\\[\\text{Cov}\\left[ \\sum_{w_{1}=1}^{K_{1}} a_{w_{1}} U_{w_{1}}, \\cdots, \\sum_{w_{j}=1}^{K_{j}} a_{w_{j}} U_{w_{j}}, \\cdots,  \\sum_{w_{n}=1}^{K_{n}} a_{w_{n}} U_{w_{n}} \\right]\\]\n\\[=\\]\n\\[\\sum_{w_1=1}^{K_1} \\cdots \\sum_{w_j=1}^{K_j} \\cdots \\sum_{w_n=1}^{K_n} \\left(\\prod_{j=1}^{n} a_{w_j} \\right) \\text{Cov}\\left[U_{w_1}, \\cdots, U_{w_j}, \\cdots, U_{w_n} \\right]\\]\nProof\nStarting with the definition of multilinear covariance, let \\(X_{j} = \\sum_{w_{j}}^{K_{j}} a_{w_j}U_{w_j}\\) for each \\(j \\in \\{1, \\cdots, n \\}\\). \\[\\begin{align*}\n\\text{Cov} \\left[ \\sum_{w_1=1}^{K_1} a_{w_1}U_{w_1}, \\cdots, \\sum_{w_{j}=1}^{K_{j}} a_{w_{j}} U_{w_{j}}, \\cdots,  \\sum_{w_n=1}^{K_n} a_{w_n}U_{w_n}\\right] &= \\mathbb{E}\\left[ \\prod_{j=1}^{n} \\left( \\sum_{w_{j}=1}^{K_{j}} a_{w_{j}} U_{w_{j}} - \\mathbb{E}\\left[ \\sum_{w_{j}=1}^{K_{j}} a_{w_{j}} U_{w_{j}} \\right]  \\right) \\right] \\\\\n& = \\mathbb{E}\\left[ \\prod_{j=1}^{n} \\left( \\sum_{w_{j}=1}^{K_{j}} a_{w_{j}} U_{w_{j}} - \\sum_{w_{j}=1}^{K_{j}} \\mathbb{E}\\left[ a_{w_{j}} U_{w_{j}} \\right]  \\right) \\right] \\\\\n& = \\mathbb{E} \\left[ \\sum_{w_1}^{K_1} \\cdots \\sum_{w_j}^{K_j} \\cdots \\sum_{w_n}^{K_n} \\prod_{j=1}^{n} \\left( a_{w_j}U_{w_j} - \\mathbb{E}\\left[ a_{w_j}U_{w_j} \\right] \\right) \\right] \\\\\n& = \\sum_{w_1}^{K_1} \\cdots \\sum_{w_j}^{K_j} \\cdots \\sum_{w_n}^{K_n} \\mathbb{E} \\left[ \\prod_{j=1}^{n} \\left( a_{w_j}U_{w_j} - \\mathbb{E}\\left[ a_{w_j}U_{w_j} \\right] \\right) \\right] \\\\\n& = \\sum_{w_1}^{K_1} \\cdots \\sum_{w_j}^{K_j} \\cdots \\sum_{w_n}^{K_n} \\mathbb{E} \\left[ \\prod_{j=1}^{n} a_{w_j} \\left( U_{w_j} - \\mathbb{E}\\left[ U_{w_j} \\right] \\right) \\right] \\\\\n& = \\sum_{w_1}^{K_1} \\cdots \\sum_{w_j}^{K_j} \\cdots \\sum_{w_n}^{K_n} \\left( \\prod_{j=1}^{n} a_{w_j} \\right) \\mathbb{E} \\left[ \\prod_{j=1}^{n} \\left( U_{w_j} - \\mathbb{E}\\left[ U_{w_j} \\right] \\right) \\right] \\\\\n& = \\sum_{w_1}^{K_1} \\cdots \\sum_{w_j}^{K_j} \\cdots \\sum_{w_n}^{K_n} \\left( \\prod_{j=1}^{n} a_{w_j} \\right) \\text{Cov} \\left[ U_{w_1}, \\cdots,  U_{w_j}, \\cdots, U_{w_n} \\right]\n\\end{align*}\\] \\(\\blacksquare\\)\n\nMultilinear covariance generalizes covariance from a bilinear comparison of two random variables to a multilinear comparison of finitely-many random variables. This generalizes the numerator of Pearson’s product-moment correlation coefficient, which leaves finding a generalization of its denominator. Knowing that the denominator of Pearson’s correlation is the product of the standard deviations of the respective two variables compared in the numerator, such a generalization of the denominator should reduce to the product of the standard deviations in the case of only two variables. Another particularly useful property of Pearson’s correlation is its normalization onto the interval \\([-1,1]\\) using the Cauchy-Schwartz inequality. Combining these properties into a goal, we desired to find an inequality that generalizes the Cauchy-Schwartz inequality in such a way that normalizes the multilinear covariance onto \\([-1,1]\\).\nNantomah (2017) provides such a generalization of the Cauchy-Schwarz inequality in the form of the generalized Hölder’s inequality for sums (see proposition below). They accomplish this by generalizing the absolute value of the inner product of two vectors to the sum-aggregated element-wise product of a collection of vectors, and by generalizing the product of 2-norms to the product of \\(p\\)-norms under specific constraints. The constraints on the orders of the norms are that individually they are strictly greater than unity, and that the sum of their reciprocals equals unity.\n\nProposition (Nantomah (2017))\nGiven \\(Q_{i,j} \\in \\mathbb{R}\\ \\forall i,j\\) where \\(i \\in \\{ 1, \\cdots, m \\}\\) and \\(j \\in \\{ 1, \\cdots, n \\}\\) then\n\\[\\sum_{i=1}^{m} \\left|\\prod_{j=1}^{n} Q_{i,j}\\right| \\leq \\prod_{j=1}^{n} \\left( \\sum_{i=1}^{m} |Q_{i,j}|^{\\alpha_j} \\right)^{\\frac{1}{\\alpha_j}}\\]\nprovided that \\(\\alpha_j &gt; 1 \\forall j\\) and that \\(\\sum_{j=1}^{n} \\frac{1}{\\alpha_j} = 1\\).\n\nBefore returning to the multilinear covariance, let us consider that a new correlation coefficient can be defined from the generalized H\"older’s inequality for sums by defining a quotient of the left-hand-side of the inequality divided by the right-hand-side. This gives a function bounded to \\([0,1]\\). Taking such a ratio requires that none of the data vectors are the zero vector. Then by dropping the absolute value in the numerator we obtain a function bounded to \\([-1,1]\\). Lastly, under suitable assumptions such as integrability and the existence of a moments, we write this function using expectations as given in Definition \\(\\ref{def:holdercorrelation}\\).\n\nDefinition\nGiven a finite collection of real-valued random variables \\(\\{X_1, \\cdots, X_n\\}\\), and \\(\\alpha_j \\in \\mathbb{R}_{&gt;1}\\), their Hölder’s correlation coefficient is given by\n\\[\\mathfrak{H} \\left[ X_1, \\cdots, X_n \\right](\\vec \\alpha) \\triangleq \\frac{\\mathbb{E} \\left[ \\prod_{j=1}^{n} X_j \\right]}{\\prod_{j=1}^{n} \\sqrt[\\alpha_j]{\\mathbb{E} \\left[ |X_j|^{\\alpha_j} \\right]}}\\]\nprovided that \\(\\sum_{j=1}^{n} \\frac{1}{\\alpha_j} = 1\\) and that the desired moments exist.\n\nWhile H\"older’s correlation coefficient as given in Definition \\(\\ref{def:holdercorrelation}\\) can be estimated from data, its primary use in this thesis is to provide a definition from which other more familiar correlation coefficients can be generalized. We begin by generalizing the reflective correlation coefficient from the bilinear case to the multilinear case.\\\nThe reflective correlation coefficient is the same in mathematical form as the Pearson product-moment correlation coefficient except that only uncentered moments are used. By setting \\(\\alpha_j = n\\) for each \\(j \\in \\{1, \\cdots, n \\}\\) we obtain a special case of Definition \\(\\ref{def:holdercorrelation}\\) which is also a multilinear generalizaton of the reflective correlation coefficient (Definition \\(\\ref{def:reflectivemultilinearcorrelation}\\)). This can be easily verified by setting \\(n=2\\) and comparing to the reflective correlation coefficient, and it also satisfies the constraints set on \\(\\alpha_j\\) in Proposition \\(\\ref{thm:generalizedholdersinequalitysums}\\).\n\nDefinition\nGiven a finite collection of real-valued random variables \\(\\{X_1, \\cdots, X_n\\}\\), and \\(\\alpha_j \\in \\mathbb{R}_{&gt;1}\\), their multilinear reflective correlation coefficient is given by\n\\[R_{\\text{reflective}} \\left[ X_1, \\cdots, X_n \\right] \\triangleq \\frac{\\mathbb{E} \\left[ \\prod_{j=1}^{n} X_j \\right]}{\\prod_{j=1}^{n} \\sqrt[n]{\\mathbb{E} \\left[ |X_j|^{n} \\right]}}\\]\nprovided that the desired moments exist.\n\nBefore generalizing the Pearson correlation coefficient, we introduce a definition that generalizes the standard deviation. Just as the standard deviation can be thought of as the 2-norm of the errors from the mean, we consider the \\(p\\)-norm of the errors from the mean to be the as given in Definition \\(\\ref{def:minkowskideviation}\\). The Nightingale deviation of order \\(p\\) is a special case of the the power mean (sometimes called the generalized mean) (Sykora (2009)), but it first centers the random variables and takes an absolute value.\\\n\nDefinition\nGiven a real-valued random variable \\(X\\) whose \\(p\\)th moment is defined, its Nightingale deviation of order \\(p\\) is given by \\[\\text{Dev}_p[X] \\triangleq \\sqrt[p]{\\mathbb{E}\\left[\\left|X - \\mathbb{E}\\left[ X \\right]\\right|^p\\right]}.\\]\n\nWith the multilinear reflective correlation coefficient as given in Definition \\(\\ref{def:reflectivemultilinearcorrelation}\\), a generalization of the Pearson correlation coefficient is obtained by centering the random variables by their expectation before computing the multilinear reflective correlation. The result is Definition \\(\\ref{def:pearsonmultilinearcorrelation}\\) which is abbreviated using the definitions of multilinear covariance (Definition \\(\\ref{def:multilinearcovariance}\\)) and Nightingale deviations of order \\(p\\) (Definition \\(\\ref{def:minkowskideviation}\\)).\n\nDefinition\nGiven a finite collection of real-valued random variables \\(\\{X_1, \\cdots, X_n\\}\\), their multilinear Pearson’s correlation coefficient is defined to be\n\\[\\text{Corr}\\left[ X_1, \\cdots, X_n \\right] \\triangleq \\frac{\\text{Cov} \\left[ X_1, \\cdots, X_n \\right]}{\\prod_{j=1}^{n} \\text{Dev}_n[X_j]}.\\]\n\nAs exemplified above, making substitutions for the random variables in H\"older’s correlation coefficient allows for an easy process of deriving new correlation coefficients. For example, substituting \\(X_j = \\sin (U_j)\\) produces a generalization of the used in circular statistics. Similarly, substituting \\(X_j = |U_j|\\) obtains an . There is a general reason why we should take an interest in multiple notions of correlation which we consider in the next subsection: detecting statistical dependence.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>New Instantiations of The Trinity of Covariation</span>"
    ]
  },
  {
    "objectID": "discussion_conclusions.html",
    "href": "discussion_conclusions.html",
    "title": "7  Discussion and Conclusions",
    "section": "",
    "text": "The Trinity of Covariation is a guiding metaphor for the mathematical notions developed in this work, with the core motivation that studying coordinated changes in structure is a way of aggregating information about systems. We sought to construct more precise mathematical notions that in some sense exemplify or instantiate this metaphor. The instantiations of this metaphor developed in this thesis include multilinear correlations, Nightingale’s correlation, inner correlations, Agnesian operators, and grade entropies.\nMultilinear correlations, Nightingales’s correlation, and inner correlation were all attempts to generalize correlation from its usage in introductory statistics by accommodating multiple variables. Nightingale’s correlation focused on a generalization of norms, and inner correlation was based on a generalization of inner products. These functions are all statistics, and because statistics are functions of random variables it is also the case that statistics are themselves random variables. Any random variable has a distribution function, even if it is degenerate. In most practical cases we suspect that the distribution will not be degenerate. There is not an easy way to derive what the distribution of a statistic should be, even when a joint probability model for the input variables is given. While the distribution of summations of random variables can be approached with convolutions or Fourier transforms, the distribution of products of random variables are less easily obtained. Existing methods often require an assumption of measure independence, which is precisely one of properties that we would like to infer rather than assume. It is a further difficulty to analytically determine whether an estimator is biased or consistent without having its sampling distribution. Future research should examine approaches for considering sums of products of random variables in order to obtain distributions that do not assume independence.\nAnother limitation of some of the statistics defined in this work is the requirement that higher moments exist. Fortunately many commonly-used distributions have finite moments of all positive order. In some cases it may be possible to substitute expectations with the principal Cauchy values of the expectations, or substitue expectations with medians which are always defined for a random variable.\nThe work in this thesis focuses on considering hypotheses about particular combinations of variables. This expressiveness leads to an exponentially large class of statistical hypotheses on any finite collection of random variables. If the particular combinations of variables can be used to test specific hypotheses, then immediately we have an exponential complexity (or worse) problem. When it comes to hypothesis testing there is the problem of inflated family-wise error rate that comes from performing many Fisher-Neyman hypothesis tests. While this can be countered with multiple correction methods such as Benjamini-Hochberg corrections, such corrections also result in lower statistical power. Future research should investigate how to usefully search the space of possible hypotheses.\nThe Agnesian operators bring the Trinity of Covariation into the realm of calculus by quantifing coordinated changes that are either instantaneous, implicit, or accumulated over an interval over some parameter. Future research is warranted to explore how the Agnesian operator can be applied in areas of mathematics and science that are already using calculus, or models amenable to calculus. Some of these subjects potentially include multivariable calculus, differential geometry, linear algebra, multilinear algebra, and differential equations. The Agnesian operators do not make use of mathematical statistics, however they may find application in the subject of stochastic differential equations.\nGrade entropies and pseudograde entropies allow us to quantify the totality of partial orders. In the case of grade entropies for product orders, we learn about the comonotonicity of a collection of variables as they change value from observation-to-observation. Partial orders are ubiquitous in mathematics and the natural sciences. Some examples of partial orders include organizational trees of corporations, evolutionary trees, causal graphs , assembly histories , social hierachies, and the dependency of subroutines in a concurrent computer program.\nA product of this thesis is the ConAction Python package. It provides an easy-to-install and easy-to-use way of applying the mathematical notions developed in this thesis. This initial release has been tested for correctness and performance, documented online and in code, and there exists opportunities for improvements user feedback on GitHub.\nWe were able to evaluate the correlational sufficiency of a classic isolation-by-distance model on mountain pine beetle population data in western North America, and discovered that there exists a dependence between genetic disimilarity and geographic distance left to be explained in the presence of climate-related covariates. We gave suggestions in the previous chapter on how to further explore this, including\n\ntrying different clustering algorithms,\nusing metrics that satisfy the triangle inequality, and\nusing non-linear regression models that have vanishing isolation-by-distance effects for both large and small distances.\n\nWe also suspect that using geographical distance as arc lengths on a sphere may be less informative than distances over land that consider topography. Rather, the biological paths (i.e. the paths that an organism actually takes or tends to take) should be more useful in estimating isolation-by-distance effects but are also multivariate and more difficult to estimate.\nWhile we were able to apply the multilinear Pearson correlation coefficient to a real research problem, there is a large number of opportunities to explore applications of the ideas developed in this thesis. It can be used to detect multilinear dependence in complex systems, but it can also be used to check the correlational sufficiency of models throughout the sciences.\nIn summary, this thesis provides fundamental motivations, mathematical tools, software tools, and a demonstration of applying these tools. In order to tackle complexity in the formal and empirical sciences, we must have a diverse armamentarium. This thesis is a contribution to that armamentarium."
  },
  {
    "objectID": "citation.html",
    "href": "citation.html",
    "title": "Citation",
    "section": "",
    "text": "@mastersthesis{seilisthesis2022,\n  author  = \"Galen Seilis\",\n  title   = \"ConAction: Efficient Implementations and Applications of Functions Inspired by the Trinity of Covariation\",\n  school  = \"University of Northern British Columbia\",\n  year    = \"2022\",\n  address = \"3333 University Way, Prince George, British Columbia, V2N 4Z9, Canada\",\n  month   = \"September\",\n  doi = 10.24124/2022/59312,\n  url = https://doi.org/10.24124/2022/59312\n}"
  },
  {
    "objectID": "new.html#conclusion",
    "href": "new.html#conclusion",
    "title": "4  New Instantiations of The Trinity of Covariation",
    "section": "4.7 Conclusion",
    "text": "4.7 Conclusion\nIn this chapter we introduced multilinear correlation coefficients, Nightingale covariance, inner correlations, Agnesian operators, and grade entropies.\nEach of these families of functions are an instantiation of the Trinity of Covariation in its own way. Mulilinear correlation relies as averaging over multiplications of random variables, and make use of a recently-proved normalization that generalized the Cauchy Schwarz inequality. Nightingale correlation is similar to multilinear correlations, but imposes non-negativity and have similar properties to seminorms and semimetrics. Inner correlations utilize a generalization of the notion of inner products to multiple variables by constructing normalized Gram determinants. Agnesian operators make direct use the notions of change as either derivatives or net changes and has applications to the notions of parametric curves and surfaces in finite dimension. Lastly, grade entropies combine elementary notions of information theory and order theory.\nWith these mathematical notions in mind, in the next chapter we introduce a software package that implements their calculation.\n\n\n\n\nAmershi, Amin H. 1985. “A Complete Analysis of Full Pareto Efficiency in Financial Markets for Arbitrary Preferences.” The Journal of Finance 40 (4): 1235–43. https://doi.org/10.1111/j.1540-6261.1985.tb02374.x.\n\n\nAthreya, Krishna B, and Soumen N Lahiri. 2006. Measure Theory and Probability Theory. 2006th ed. Springer Texts in Statistics. New York, NY: Springer.\n\n\nAubin, Jean-Pierre. 2014. Mathematical Methods of Game and Economic Theory. 2nd ed. Studies in Mathematics and Its Applications. North-Holland.\n\n\nCover, Thomas M., and Joy A Thomas. 2006. Elements of Information Theory. 2nd ed. Nashville, TN: John Wiley & Sons.\n\n\nEmmerich, Michael T. M., and André H. Deutz. 2018. “A Tutorial on Multiobjective Optimization: Fundamentals and Evolutionary Methods.” Natural Computing 17 (3): 585–609. https://doi.org/10.1007/s11047-018-9685-y.\n\n\nGreub, Werner. 1978. Multilinear Algebra. Springer New York. https://doi.org/10.1007/978-1-4613-9425-9.\n\n\nJaynes, E. T. 1965. “Gibbs Vs Boltzmann Entropies.” American Journal of Physics 33 (5): 391–98. https://doi.org/10.1119/1.1971557.\n\n\nLambert, Frank L. 2002. “Disorder - a Cracked Crutch for Supporting Entropy Discussions.” Journal of Chemical Education 79 (2): 187. https://doi.org/10.1021/ed079p187.\n\n\nNantomah, Kwara. 2017. “Generalized Hölder’s and Minkowski’s Inequalities for Jackson’sq-Integral and Some Applications to the Incompleteq-Gamma Function.” Abstract and Applied Analysis 2017: 1–6. https://doi.org/10.1155/2017/9796873.\n\n\nStanley, Richard P. 2011. Cambridge Studies in Advanced Mathematics Enumerative Combinatorics: Series Number 49: Volume 1. 2nd ed. Cambridge, England: Cambridge University Press.\n\n\nSykora, Stanislav. 2009. “Mathematical Means and Averages: Basic Properties.” Stan’s Library, no. Volume III (July). https://doi.org/10.3247/sl3math09.001.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>New Instantiations of The Trinity of Covariation</span>"
    ]
  },
  {
    "objectID": "mtn-pine-beetle.html",
    "href": "mtn-pine-beetle.html",
    "title": "6  Correlational Sufficiency of an Isolation by Distance Model of Mountain Pine Beetle in Western North America",
    "section": "",
    "text": "In this chapter we describe an application of the multilinear Pearson correlation coefficient in the science of molecular ecology."
  },
  {
    "objectID": "mtn-pine-beetle.html#introduction",
    "href": "mtn-pine-beetle.html#introduction",
    "title": "6  Correlational Sufficiency of an Isolation by Distance Model of Mountain Pine Beetle in Western North America",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nFor three random variables, \\(X\\), \\(Y\\), and \\(Z\\), we can consider the proposition that \\(X\\) and \\(Y\\) are independent conditioned on \\(Z\\), denoted by \\(X \\perp\\!\\!\\!\\!\\perp Y | Z\\). If \\(Z\\) is controlled for, then \\(X\\) and \\(Y\\) should be found to be independent. Testing \\(X \\perp\\!\\!\\!\\!\\perp Y\\) while stratifying by \\(Z\\) is an approach to evaluating this proposition. Stratification can be done discretely by grouping the values of \\(X\\) and \\(Y\\) into different levels of \\(Z\\), and then estimating the measure dependence between \\(X\\) and \\(Y\\) within those groupings. While binning data into a discrete partition can introduce binning error, it can be an effective way of handling apparent discontinuities in a data set. A second approach to stratification called is to find continuous functions \\(X \\approx f(Z)\\) and \\(Y \\approx g(Z)\\), and then examine whether \\([X-f(Z)] \\perp\\!\\!\\!\\!\\perp [Y-g(Z)]\\). When \\(f\\) and \\(g\\) are linear, and we compute the Pearson correlation coefficient on their residuals, which has been called partial correlation (Fisher (1924)). Partialling with linear models will lead to the partial correlation coinciding with the conditional correlation when the random variables follow certain joint distributions such as the multivariate normal distribution (Baba, Shibata, and Sibuya (2004)), but in general a careful consideration of how the conditional expectation is being modelled is needed. These two approaches to stratifying are compatible in the sense that they can be used within the same analysis.\nMultilinear Pearson correlation can be used to indicate multilinear dependence. Since multilinear dependence is mutually exclusive with independence, we can use multilinear Pearson correlation as evidence of dependence. A multilinear Pearson correlation score of zero does not imply independence however, so the inference is one-sided. Because the multilinear Pearson correlation is defined partly in terms of expectation operators, we can condition the correlation score on other variables. In this way we can compute conditional correlations at discrete levels of a random variable, or partial and semi-partial correlation coefficients when we can model the conditional dependence with functions. We can therefore use multilinear Pearson correlation coefficients to estimate the existence of multilinear forms of conditional independence. When a collection of propositions about (in)dependence or conditional (in)dependence are tested with correlation statistics, and those correlations seem to agree with the hypothetical statements, we will call the propositions *correlationally sufficient.\nMountain pine beetle (Dendroctonus ponderosae) (Figure \\(\\ref{fig:pinebeetle}\\)(a)) is a species of bark beetle native to western Canada, the United States of America, and Mexico . It predates on a variety of tree species, and especially pine. According to a manual published by the United States Department of Agriculture Forest Service in 1977, mountain pine beetle had been observed attacking various species of pine trees including Pinus contorta, Pinus lambertiana, Pinus monticola, Pinus ponderosa, Pinus albicaulis, Pinus aristata, Pinus balfouriana, Pinus coulteri, Pinus edulis, Pinus flexilis, and Pinus monophylla (Furniss and and (1977)). Within British Columbia the most common species of host for mountain pine beetle is lodgepole pine (Pinus contorta)(Burleigh (2014)). A couple of indicators of mountain pine beetle attacks on trees include pitch tubes (Figure \\(\\ref{fig:pinebeetle}\\)(b)) which are a result of the host tree response (Burleigh (2014)), and areas of dead (especially pine) trees on a landscape (Figure \\(\\ref{fig:pinebeetle}\\)(c)). Approximately 18.3 million hectares of British Columbia were affected by a mountain pine beetle epidemic that began in the early 1990’s (Corbett et al. (2015)). This left a large negative impact on the forest industry of British Columbia, damaging over half of the merchantable pine in the province, which has been forecasted to result in a 57.37 billion dollar loss in GDP of the next few decades (Corbett et al. (2015)). The epidemic disrupted the habitat of numerous species of birds and mammals (Saab et al. (2014)). It is worthwhile to continue studying the mountain pine beetle post-epidemic to understand how to prevent and mitigate further economic and environmental disturbances from mountain pine beetle and similar forest pests.\n\n\n\n\n\n\n\nMountain pine beetle specimen (Public Domain)\n\n\n\n\n\n\n\nPitch tubes caused by beetles boring into the bark (CC BY-SA 3.0)\n\n\n\n\n\n\n\n\n\nMountain pine beetle damage in Rocky Mountain National Park in 2012 (CC BY-SA 3.0)\n\n\n\n\nFigure 6.1: Mountain pine beetle have a substantial impact on individual trees and landscapes.\n\n\nIsolation by distance (IBD) is a hypothesis that can be made about a collection of populations (i.e. a metapopulation) that will potentially interbreed . Specifically, a metapopulation exhibits IBD if its members are more genetically dissimilar the further apart they are from each other, which is motivated by the notion that mere distance between populations can serve as a gradual contribution to reproductive isolation. Mountain pine beetle has been analyzed for distinct populations of mountain pine beetle in using software such as STRUCTURE and BAPS , and was found to have a North-South clustering pattern . Subclusters within the northern and southern clusters are also supported . In evidence of four clusters (see Figure \\(\\ref{fig:geo_cluster_dist}\\)(a)) was also found, however considered the geographic scope of western Canada while considered the geographical scope of western North America. After identifying populations of mountain pine beetle, found evidence of IBD using linear regression.\\\nA question that remains under-addressed is whether host selection is independent from the genetic variation among mountain pine beetle populations. Previous studies found that allozyme variation is confounded by geography . Namely that the isolation by distance effect observed in mountain pine beetle may be confounding our understanding of dependence between host selection and genetic variability of mountain pine beetle. One approach is to perform an analysis of molecular variance conditioned on the geographical distances being small. This approach was used in where they conditioned the maximum distance to be 200 km (visualized in Figure \\(\\ref{fig:geo_cluster_dist}\\)(b)), and did not find statistically significant (\\(\\alpha=0.05\\)) results for host selection effects with this approach. Their choice of 200 km, and whether the corrections they performed (see ) were sufficient to account for the unbalanced group sizes, may or may not have affected the analysis outcome. Since no statistical procedure is without assumptions, it is desirable to perform multiple analyses under different assumptions to see if there is consilience (i.e. convergence of evidence).\nIn this application we considered whether there is correlational sufficiency of certain IBD models stratified by clusters identified in using STRUCTURE. The first hypothesis was whether the IBD models were correlationally sufficient in their exogeneity. This term, , comes from Economics. It is a hypothesis that the errors of a model are independent of the explanatory variables (Wooldridge (2009)). When this does not occur, it is an indication that the model has been improperly specified. When a model is not exogenous, it is called . The second question was whether the residuals of the IBD models appear to be independent of host selection along with potential covariates related to temperature, phase changes of water, and elevation. We also examine these hypotheses via assessing correlational sufficiency.\nIn order to do this we first hypothesized a functional relationship between the genetic dissimilarity and the geographical distance based on existing literature. summarizes slopes and intercepts of IBD models for numerous insect species in which a linear model between the log of gene flow and the log of geographic distances were computed. This motivates the use of a linear model to begin with. Similarly, suggests the modelling \\(\\frac{F_{ST}}{1- F_{ST}}\\) against the log of the geographic distances, which is also used in . We adopt the variable in our analysis to be the pairwise \\(F_{ST}\\) as calculated in the GenAlEx plugin for Excel™ (Peakall and Smouse (2006)) to represent the degree of genetic differentiation between populations."
  },
  {
    "objectID": "mtn-pine-beetle.html#dataset",
    "href": "mtn-pine-beetle.html#dataset",
    "title": "6  Correlational Sufficiency of an Isolation by Distance Model of Mountain Pine Beetle in Western North America",
    "section": "6.2 Dataset",
    "text": "6.2 Dataset\nBecause we used the same dataset as Boone et al. (2022), the underlying materials and methods are the same. However, since is currently unpublished we will give some description the dataset’s properties as they were provided to us.\nThe dataset as provided constituted a GenAlEx6.5 software analysis, a summary of the sampled sites, and an extensive collection of climate-related variables paired to the site locations. Two variables of particular importance are the pairwise Fst scores as calculated by GenAlEx6.5, and the geolocations in terms of latitude and longitude. The geolocations were used in our analysis to calculate intersite distances as the arc lengths on a sphere. The climate-related data contained monthly and annual measurements related to temperature, pressure, and precipitation.\\\nWe are grateful to Dr. Brent Murray for provided this dataset to us along with the manuscript. We are also grateful to the teams of people that participated in obtaining and curating such a large and rich dataset as acknowledged in Boone et al. (2022)."
  },
  {
    "objectID": "mtn-pine-beetle.html#data-analysis",
    "href": "mtn-pine-beetle.html#data-analysis",
    "title": "6  Correlational Sufficiency of an Isolation by Distance Model of Mountain Pine Beetle in Western North America",
    "section": "6.3 Data Analysis",
    "text": "6.3 Data Analysis\nOriginally we had intended to perform equation-by-equation ordinary least squares regression (EBE-OLS), but upon examination of the histograms (as found in Figure \\(\\ref{fig:cluster_regression}\\)) of fst_score and ln(geo_dist) it became apparent that normality and homoschedasticity assumptions were not met. Strong violations of these assumptions imply that EBE-OLS is no longer guaranteed to be unbiased or the most efficient estimator of the linear regression parameters ((ohnson2007?)). Instead we used EBE iteratively-reweighted least squares (IRLS) (Lawson (1961)) with a Huber loss (Huber (1964)) using the Python package to provide outlier-robust estimates of the regression parameters and residuals (see Appendix sections \\(\\ref{sec:ibdmodelsummaries}\\) and \\(\\ref{sec:ibdevalplots}\\) for model summaries and diagnostic plots). The fitted linear models are plotted in each panel of Figure \\(\\ref{fig:cluster_regression}\\) along with the Pearson correlation, coefficient of determination and a p-value computed with the command (Virtanen et al. (2020)). This command assumes a null hypothesis that the two variables follow a bivariate normal distribution, implying that the correlation coefficient follows a sampling density of \\(f(r) = \\frac{(1 - r^2)^{\\frac{n}{2} - 2}}{B(\\frac{1}{2}, \\frac{n}{2} - 1)}\\) where \\(r\\) is the Pearson correlation, \\(n\\) is the sample size, and \\(B\\) is the beta function . Therefore p-values on Figure \\(\\ref{fig:cluster_regression}\\) should be interpreted as the two-tailed likelihood of the data under the assumption of being sampled from this null distribution.\nBeyond mere non-normality, the data distributions in Figure \\(\\ref{fig:cluster_regression}\\) show some unusual features. One is exemplified by the NL vs SW plot, but possibly present to some degree in other pairs such as NU vs SW and within SW. There appears to be more than one grouping in the data even after clustering. This suggests that the clustering is not fully accounting for the genetic distribution of microsatellites. The second unusual feature is that some of the plots, within NL, within NU, and within SE have skewed left tails along the axis of log geographic distance. Lastly, the shape of the connected components under visual inspection seem to be hinting at a bend in the data such as in NL vs NU or within SE, which may be an indication that the relationship between fst_score and ln(geo_dist) is non-linear.\nWe wanted to include some potential covariates from the climate data that we were provided. We also wanted to evaluate whether we could reduce the dataset down to a small number of dimensions without losing much information about the original dataset. The multilinear correlation methodology we wished to use can be computed on different combinations of variables to learn about the dependence between particular combination of variables, but with over 200 variables that entails over \\(2^{200} - 201\\) possible combinations. Thus we were motivated to look for ways to reduce the set of hypotheses to examine.\\\nWe performed a principal components analysis on the standardized scores of the site-level data as part of an exploratory data analysis. Figure \\(\\ref{fig:climatepca}\\)(a) indicates that approximately 20-30 components would be needed to be kept to preserve a large majority of the variation in the data, suggested that the principal components are not efficient at compressing the data. Figures \\(\\ref{fig:climatepca}\\)(b) and \\(\\ref{fig:climatepca}\\)(c) confirms for us that the initial basis vectors point in various directions on the plane spanned by the first two principal components. Visual inspection of Figure \\(\\ref{fig:climatepca}\\) suggests the NU cluster and the SW are linearly separable on the plane, and perhaps SE from SW, but most clusters do not appear to be easily separated for other pairings.\nTo ameliorate the issue of too many candidate hypotheses, we took another approach principal components analysis. Most of the variables were explicitly about various annual, seasonal, and monthly scores related to (1) temperature and (2) phase changes in water. We annotated each of the climate variables as either having more to do temperature, phase changes of water, or neither. We labelled the group of variables related to temperature as and the group of variables to phase changes of water as . By grouping the variables semantically we expected that variation in their first principal component would be ostensibly related to their semantic label, and that subsequent calculations of correlations with these variables would also be ostensibly related to the semantic label. Because we would not be computing the multilinear correlation on the original variables tied to a single site, but rather on scores tied to pairs of sites, we chose the absolute difference in these climate variables as new features to satisfy this requirement.\nWe also stratified the climate-related variables by pairs of clusters before performing principal components analysis on them so that when we computed the correlations between climate-related principal components and the IBD model residuals, they would have the same discrete conditioning as the pairwise values. This approach may also have reduced the amount of variation in the principal components that was due to geographic distance, however we account for the possibility of the principal components being partly explained by geographic distance in our interpretation of the multilinear correlation scores. The resulting first principal components’s are labelled as and , and their explained variance ratios within each pair of clusters is summarized in Table \\(\\ref{tab:expvarpcaclimate}\\).\nTable \\(\\ref{tab:expvarpcaclimate}\\) shows that most cluster pairs had around 50-60 percent total variance of their temperature differences explained by , and about 20-30 percent total variance of their water-related phase change differences explained by . Especially for the temperature variables, this is a marked improvement over the approximately 17 percent total variation explained by the first principal component in Figure \\(\\ref{fig:climatepca}\\). While we have lost a lot of information with this compression, it adds two potential covariates related to climate to the analysis instead of over 200 potential covariates.\\\nTo asses the correlational sufficiency of the exogeneity of the IBD models, we computed the pairwise correlation between and stratified by cluster pair. See Table \\(\\ref{tab:endogencorrs}\\) for a summary of these statistics. We did not find any statistically significant (\\(\\alpha=0.05\\)) correlations after correction for multiple tests, however we noted that the pre-corrected correlation between NL and SW was significant at this level. However, the effect size of this correlation was only 0.14. Therefore the models appear to be correlationally sufficient for exogeneity according to a pairwise correlation analysis.\\\nFor the multilinear correlation analysis to estimate correlational sufficiency we used IBD model residuals (), host selection distance (), geographic log-distance (), the first temperature-difference-related principal component(), and the first phase-change-of-water-related principal component(). With these six variables there are 57 correlation hypotheses that could be tested on each cluster pair, amounting to 570, however many of them are not directly related to our research questions. And it would be desirable to minimize the number of hypotheses tested to conserve statistical power. Since we were interested in the correlational sufficiency of the IBD models, we looked at a subset of correlations that always included always included and among the considered variables. This allowed us to compute only 16 correlation coefficients per cluster pair, entailing the calculation of 160 correlation coefficients among the 10 pairs of clusters.\\\nWithin each cluster pair the multilinear Pearson correlations were calculated with an estimated p-value using a permutation test using 10 000 resamplings, which tests a null hypothesis of exchangeability. As cited by Serafino (2016), all independent and identically distributed random variables are exchangeable. Thus a strongly inexchangeable collection of random variables entails that the random variables are either dependent and/or non-identical. This is a weak condition, but a substantially-non-zero multilinear Pearson correlation score suggests dependence anyway. Some of the p-values were zero because they were observed zero times, however we know that there exists at least one permutation of the columns (i.e. the one we observed in the original data) that could have been sampled to obtain a large enough score. Therefore the p-values are somewhere inbetween zero and \\(10^{-4}\\). We assigned such zero-scores to be \\(10^{-4}\\), taking on more type II error while reducing type I error. Of the ten pairs of clusters, we found statistically-significant (\\(\\alpha=0.05\\)) correlations in six of them. Using we prepared Figure \\(\\ref{fig:setcoveribscorrelation}\\) to visualize which sets of variables had statistically significant multilinear correlation, coloured by the strength of the correlation score .\n\n\n\n\n\n\nRelationship Between IID, Exchangeable and Identical in Marginals\n\n\n\n\n\nAs cited above, is a collection of random variables are IID then they are exchangeble. Note that the converse is not true in general. A collection of random variables might be exchangeable but not IID.\nFurthermore, if a collection of random variables are exchangeable then they will have the same marginal distribution up to labelling of the variables. The converse is not true in general. It is possible to have a collection of random variables which are identical in their marginal distribution but are not exchangeable.\n\n\n\nIt is apparent from Figure \\(\\ref{fig:setcoveribscorrelation}\\) that and appear to be mutually dependent along with within each pair of clusters. Thus the correlationally sufficient exogeneity inferred with the pairwise Pearson correlations did not fully assess independence between the IBD model residuals with the geographical distances when covariates such as differences in host were considered. Rather these multilinear Pearson correlation coefficients suggest that a linear model between and does not render the genetic distances as independent from . This leaves the question of dependence between genetic distance and host selection confounded by geography. This exemplifies how multilinear correlation can be used selectively on different combinations of variables depending on the question of interest.\\\nWe also noted from Figure \\(\\ref{fig:setcoveribscorrelation}\\) that all six variables occur in only four of the six pairs of clusters included all of the variables. Namely that absolute differences in elevation () did not appear to be correlated be correlated with and within the within NL and NL vs NU clusters. We speculate based on Figure \\(\\ref{fig:elevationdist}\\) that the spatial distribution of elevations might be more similar within and between the northern clusters.\nTable \\(\\ref{tab:shortcorrresults}\\) summarizes the multilinear correlations with absolute scores greater-than-or-equal-to 0.2. The rest of the multilinear Pearson correlations are summarized in tables found in appendix section \\(\\ref{sec:summarycorrhost}\\). Notably only a short list of multilinear correlations make this list, and they are not particularly strong effects on the scale of \\([-1,1]\\)."
  },
  {
    "objectID": "mtn-pine-beetle.html#future-research",
    "href": "mtn-pine-beetle.html#future-research",
    "title": "6  Correlational Sufficiency of an Isolation by Distance Model of Mountain Pine Beetle in Western North America",
    "section": "6.4 Future Research",
    "text": "6.4 Future Research\nThe main challenge with testing the conditional independence of host selection and genetic diversity given the dependence on geography is that the dependence between genetic variation and geographical distribution is complicated. In order to account for this dependence, we believe a combination of discrete stratification (i.e. clustering or similar methods) combined with continuous stratification (i.e. regression models) will be required. We offer some discussion of how future research can improve on the modelling we presented in this work.\n\n6.4.1 Variable Extraction and Selection\nThe first aspect we will consider is which variables to use in an IBD model.\nFor variables representing abstract ideas such as “genetic variability”, “genetic differentiation”, “genetic distance”, “genetic structure” we must extract variables the from original data that operationalize our often-less-precise notions. A couple of the most common genetic disimilarity scores include pairwise Fst and Nei’s standard genetic distance score. Neither of these functions are distance functions (i.e. metrics) in the mathematicians sense, but they do have useful properties such as identity of indiscernibles and non-negativity that intuitively capture the notion of a disimilarity score'. The property that they do not have is the triangle inequality, which can be intuitively stated as ataking a detour requires more distance’ property. For example, the distance between Prince George and Vancouver must be less-than-or-equal-to the distance of going from Prince George to some other city (e.g. Edmonton) and then going to Vancouver. If we considered three genetic populations \\(A\\),\\(B\\),\\(C\\), and a distance function between them, \\(d\\), then we would find that \\(d(A,B) \\leq d(A,C) + d(C,B)\\). The triangle inequality is a property that can aid in interpretation of the results, and thus we recommend performing a literature search for alternative functions to Fst and Nei’s score that satisfy this property.\\\nIn our model of IBD we assumed that the best way to quantify the geographical distance between two points on the Earth is by the arc length on the surface of a sphere. This is certainly an excellent approximation given that the Earth’s surface is even more accurately modelled as an oblate spheroid with a small eccentricity, but we wish to suggest a different motivation for selecting a measurement of distance based on an abstract reason for why IBD should work. The underlying rationale for IBD is that populations that are further away will be more genetically dissimilar because the resources required to travel the distance is a gradual reproductive barrier. This leads us to question what other spatially-relevant factors serve as gradual barriers, and we offer some speculations. While mountain pine beetle can potentially fly for hours at a time , and even be carried by the winds at around 800 m above the tree canopy for 100 kilometers , it is not unreasonable to hypothesize that topographical features can serve as gradual barriers. The first hypothesis that we offer is to use surface distances that account for the actual topographically details such as mountains and valleys. One way to achieve this computationally would be to create a weighted graph whose vertices are spatial locations and whose weights are the 3D Euclidean distance. Figure \\(\\ref{fig:elevationsurface}\\) provides a low resolution example of such a surface using the provided georeferenced mountain pine beetle data. For such a mesh model with high enough resolution, one could consider the existence of paths through the graph between two points. The shortest path problem is a classic problem from computer science that is often solved by Dijkstra’s algorithm or extensions of it such as A* (Doran and Michie (1966), Dechter and Pearl (1985)). These shortest paths on along the surface of Earth might better reflect the graduation of reproductive isolation that exists between populations.\nOnce this local impediment notion is realized for Euclidean distance, one might imagine that more abstract network weights could be devised based on other variables including local climate variables including wind direction, local density of host species, the local density of predators, and the geographical distribution of suitable habitat. Building such surface models requires more work as variables are added and as the resolution of the spatially-embedded graph increases, but it may be an effective way of modelling the non-linear and possibly-non-smooth effects of geography on IBD as mediated through other variables.\n\n\n6.4.2 Discrete Stratification\nLet us consider the discrete stratification first. It is evident from the NL vs SW subplot in Figure \\(\\ref{fig:cluster_regression}\\) that there are separate connected components that were not identified by the STRUCTURE software package, which in that particular subplot appear to be separated primarily by axis. However, visual examination of the NU vs SW and SE vs NU subplots are more ambiguous on the existence of additional connected components and by which discriminant function (linear or non-linear) they would be separated by. This motivates finding a new partition of the data. It is not practically possible to exhaustively compute every possible partition of a modestly-sized dataset because the number of partitions on a set is equal to the Bell number for cardinality of that set. The Bell number of a set of cardinality \\(n\\) is equivalent to adding up all the Stirling numbers of the second kind up to and including \\(n\\) (Weisstein (1999)), which can be written as\n\\[B_n = \\sum_{k=0}^{n} \\left[ \\frac{1}{k!} \\sum_{i=0}^{k} (-1)^i {k \\choose i} (k-i)^n\\right].\\]\nClustering algorithms guide the data analyst toward particular choices of partitions of the data, based on a choice of algorithm, parameters, and (usually) a function quantifying similarity or dissimilarity. There are a large number of hard clustering algorithms). including K-means (Steinhaus (1957)), affinity propagation (Frey and Dueck (2007)), mean-shift (Comaniciu and Meer (2002)), spectral clustering (Shi and Malik (2000)), Ward hierarchical clustering (Ward (1963)), density-based spatial clustering of applications with noise (DBSCAN) (Ester et al. (1996)), ordering points to identify the clustering structure (OPTICS) (Ankerst et al. (1999)), Gaussian mixture models (GMM’s; see for examples), and balanced iterative reducing and clustering using hierarchies (BIRCH) . The Scikit-Learn Python package provides a useful comparison and contrast of the tradeoffs among these methods in terms of (hyper)parameters, scalability, and assumptions made by each algorithm . Including aforementioned tools such as STRUCTURE and BAPS , there are other tools in the literature as cited by which are aimed at modelling genetic structure. We suspect that many of these approaches will also require choices of hyperparameters. In our opinion, there does not exist a clustering algorithm that is suitable for all use cases and data sets. Therefore we recommend that a quantitative and qualitative exploration of which clustering algorithms best suite this microsatellite data be undergone in future research.\n\n\n6.4.3 Continuous Stratification\nNow let us consider the continuous stratification approach. After stratifying the data based on the cluster assignment, we can further substratify by a continuous function. In this thesis we tested a na\"ive hypothesis that the IBD effects between the Fst score and geographic log-distance would be linear based on a prevalence of such models in the literature. As shown in Figure \\(\\ref{fig:cluster_regression}\\), the assumption of a linear relationship between the pairwise Fst scores and the natural logarithm of the geographic (i.e. spherical arc-length) distance was poor-to-moderate in fit. The lack of excellent fit could be due to multiple factors. As mentioned above, some of the lack of model success may be attributable to not specifying the right strata. However, in subplots within NU, within SE, and within SW there appears to be substantial influence of (visually-estimated) ouliers even with the use of a Huber loss function to robustify the training. We suspect that the relationship between pairwise Fst scores and the geographic log-distance are non-linear based on visual inspection of the plots in our analysis, but we also believe there are first-principles reasons to expect non-linearity when we consider populations that are extremely close or extremely far away. When populations are relatively close compared to their mobility, we might expect that such distances explain very little genetic variation because they do not serve as a barrier to reproduction. The other extreme is when the populations are already so far away that they are very rarely interbreeding, in which case moderate variations in their geographical distance may have a smaller effect than if they were only moderately far away compared to their mobility. The notion that IBD effects have smaller sensitivity at very small distances and very large distances, but with a more sensitive range of distances in-between, suggests non-linearities such as can be found with S-shaped curves. Such curves are common in nature, from population dynamics to levels of gene transcription. Some of the S-shaped curves that might be used in future research include the Verhulst model (Bacaër (2011)), monod function (Monod (1949)), Gomperz function (Gomperz (1825)), Von Bertalanffy’s equation (Pauly and Morgan (1987)), Richard’s curve (i.e. the generalized logistic growth function) (Richards (1959)), and the hyperbolastic functions of types I, II, and III (Tabatabai, Williams, and Bursac (2005))."
  },
  {
    "objectID": "mtn-pine-beetle.html#conclusions",
    "href": "mtn-pine-beetle.html#conclusions",
    "title": "6  Correlational Sufficiency of an Isolation by Distance Model of Mountain Pine Beetle in Western North America",
    "section": "6.5 Conclusions",
    "text": "6.5 Conclusions\nIn this application we showed how multilinear Pearson correlation can be used in model evaluation. The existence of measure dependence between the residuals of the IBD models with geographical distance after partialling suggests that the models tested here are not correlationally sufficient for the exogeneity assumption. Our results suggest that there may be measure dependence between differences in host choice and pairwise Fst scores, however these correlations are still confounded by geographical distance. We provided multiple avenues for improving the IBD models in future research, and exemplified how correlational sufficiency can be used in future model evaluations.\n\n\n\n\nAnkerst, Mihael, Markus M. Breunig, Hans-peter Kriegel, and Jörg Sander. 1999. “OPTICS: Ordering Points to Identify the Clustering Structure.” In, 49–60. ACM Press.\n\n\nBaba, Kunihiro, Ritei Shibata, and Masaaki Sibuya. 2004. “Partial Correlation and Conditional Correlation as Measures of Conditional Independence.” Blackwell Publishing Asia Pty Ltd. 46 (4): 657–64. https://doi.org/10.1111/j.1467-842x.2004.00360.x.\n\n\nBacaër, Nicolas. 2011. “Verhulst and the Logistic Equation (1838).” In A Short History of Mathematical Population Dynamics, 35–39. Springer London. https://doi.org/10.1007/978-0-85729-115-8_6.\n\n\nBoone, Celia K., Kirsten M. Thompson, Philippe Henry, and Brent W. Murray. 2022. “Host Use Does Not Drive Genetic Structure of Mountain Pine Beetles in Western North America,” June. https://doi.org/10.1101/2022.06.28.498011.\n\n\nBurleigh, Jennifer. 2014. Field Guide to Forest Damage in British Columbia. Victoria, B.C: Ministry of Forests, Lands; Natural Resource Operations.\n\n\nComaniciu, D., and P. Meer. 2002. “Mean Shift: A Robust Approach Toward Feature Space Analysis.” IEEE Transactions on Pattern Analysis and Machine Intelligence 24 (5): 603–19. https://doi.org/10.1109/34.1000236.\n\n\nCorbett, L. J., P. Withey, V. A. Lantz, and T. O. Ochuodho. 2015. “The Economic Impact of the Mountain Pine Beetle Infestation in British Columbia: Provincial Estimates from a CGE Analysis.” Forestry 89 (1): 100–105. https://doi.org/10.1093/forestry/cpv042.\n\n\nDechter, Rina, and Judea Pearl. 1985. “Generalized Best-First Search Strategies and the Optimality of a*.” Journal of the ACM 32 (3): 505–36. https://doi.org/10.1145/3828.3830.\n\n\nDoran, J. E., and D. Michie. 1966. “Experiments with the Graph Traverser Program.” Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences 294 (1437): 235–59. https://doi.org/10.1098/rspa.1966.0205.\n\n\nEster, Martin, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu. 1996. “A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.” In KDD.\n\n\nFisher, Ronald Aylmer. 1924. “The Distribution of the Partial Correlation Coefficient.” Metron 3: 329–32. https://digital.library.adelaide.edu.au/dspace/handle/2440/15182.\n\n\nFrey, Brendan J., and Delbert Dueck. 2007. “Clustering by Passing Messages Between Data Points.” Science 315 (5814): 972–76. https://doi.org/10.1126/science.1136800.\n\n\nFurniss, R. L., and V. M. Carolin and. 1977. Western Forest Insects /. Dept. of Agriculture, Forest Service : https://doi.org/10.5962/bhl.title.131875.\n\n\nGomperz, Benjamin. 1825. “XXIV. On the Nature of the Function Expressive of the Law of Human Mortality, and on a New Mode of Determining the Value of Life Contingencies. In a Letter to Francis Baily, Esq. F. R. S. &c.” Philosophical Transactions of the Royal Society of London 115 (December): 513–83. https://doi.org/10.1098/rstl.1825.0026.\n\n\nHuber, Peter J. 1964. “Robust Estimation of a Location Parameter.” The Annals of Mathematical Statistics 35 (1): 73–101. https://doi.org/10.1214/aoms/1177703732.\n\n\nLawson, Charlse Lawrence. 1961. “Contribution to the Theory of Linear Least Maximum Approximations.” PhD thesis, University of California at Los Angeles.\n\n\nMonod, Jacques. 1949. “The Growth of Bacterial Cultures.” Annual Review of Microbiology 3 (1): 371–94. https://doi.org/10.1146/annurev.mi.03.100149.002103.\n\n\nPauly, D., and G. R. Morgan. 1987. Length-Based Methods in Fisheries Research. Makati, Metro Manila, Philippines Safat, Kuwait: International Center for Living Aquatic Resources Management Kuwait Institute for Scientific Research.\n\n\nPeakall, Rod, and Peter E. Smouse. 2006. “Genalex 6: Genetic Analysis in Excel. Population Genetic Software for Teaching and Research.” Molecular Ecology Notes 6 (1): 288–95. https://doi.org/10.1111/j.1471-8286.2005.01155.x.\n\n\nRichards, F. J. 1959. “A Flexible Growth Function for Empirical Use.” Journal of Experimental Botany 10 (2): 290–301. https://doi.org/10.1093/jxb/10.2.290.\n\n\nSaab, Victoria A., Quresh S. Latif, Mary M. Rowland, Tracey N. Johnson, Anna D. Chalfoun, Steven W. Buskirk, Joslin E. Heyward, and Matthew A. Dresser. 2014. “Ecological Consequences of Mountain Pine Beetle Outbreaks for Wildlife in Western North American Forests.” Forest Science 60 (3): 539–59. https://doi.org/10.5849/forsci.13-022.\n\n\nSerafino, Loris. 2016. “On the de Finetti’s Representation Theorem: An Evergreen (and Often Misunderstood) Result at the Foundation of Statistics.” http://philsci-archive.pitt.edu/12059/.\n\n\nShi, Jianbo, and J. Malik. 2000. “Normalized Cuts and Image Segmentation.” IEEE Transactions on Pattern Analysis and Machine Intelligence 22 (8): 888–905. https://doi.org/10.1109/34.868688.\n\n\nSteinhaus, Hugo. 1957. “Sur La Division Des Corps Matériels En Parties.” Bulletin L’Académie Polonaise Des Science 4 (12): 801–4. https://scirp.org/reference/referencespapers.aspx?referenceid=2408792.\n\n\nTabatabai, Mohammad, David Williams, and Zoran Bursac. 2005. “Hyperbolastic Growth Models: Theory and Application.” Theoretical Biology and Medical Modelling 2 (1): 14. https://doi.org/10.1186/1742-4682-2-14.\n\n\nVirtanen, Pauli, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, et al. 2020. “SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python.” Nature Methods 17: 261–72. https://doi.org/10.1038/s41592-019-0686-2.\n\n\nWard, Joe H. 1963. “Hierarchical Grouping to Optimize an Objective Function.” Journal of the American Statistical Association 58 (301): 236–44. https://doi.org/10.1080/01621459.1963.10500845.\n\n\nWeisstein, Eric W. 1999. “Wolfram MathWorld: The Web’s Most Extensive Mathematics Resource. From MathWorld–a Wolfram Web Resource.” Wolfram Research, Inc. https://mathworld.wolfram.com/.\n\n\nWooldridge, Jeffrey. 2009. Introductory Econometrics : A Modern Approach. Mason, OH: South Western, Cengage Learning."
  },
  {
    "objectID": "intro.html#the-trinity-of-covariation",
    "href": "intro.html#the-trinity-of-covariation",
    "title": "2  Introduction",
    "section": "2.2 The Trinity of Covariation",
    "text": "2.2 The Trinity of Covariation\nIn the analysis of data we look for how observables vary on their own, and how they vary together. Figure \\(\\ref{fig:dancing}\\) offers a metaphor for this by relating dancers to variables. In univariate statistics we are concerned with how a single variable changes or distributes on its own. In bivariate statistics we are concerned with how pairs of variables covary. Likewise, for any number of putatively dependent variables we can consider how they covary, which is one of the goals of multivariate statistics.\nThe notion that coordinated change in structures is of interest is older than modern statistics. For example, the philosohpher John Stuart Mill spoke of the importance of coordinated change in his thinking on causality . This is similar to Reichenbach’s common cause principle which states that if two events \\(A\\) and \\(B\\) hold the relation \\(P(A \\cap B) &gt; P(A)P(B)\\) and yet neither causes the other, then there exists a common cause \\(C\\) such that \\(A\\) and \\(B\\) are conditionally independent on \\(C\\) (Hitchcock and Rédei (2021)).\n\n\n\n\n\n\n\n\nJohn Stuart Mill (Public Domain)\n\n\n\n\n\nQuotation (Mill (1843))\nWhatever phenomenon varies in any manner whenever another phenomenon varies in some particular manner, is either a cause or an effect of that phenomenon, or is connected with it through some fact of causation.\n\n\n\n\n\nFigure 2.1: Portrait of John Stuart Mill and a quote from his work on causal inference and reasoning.\n\n\n\nPerhaps the notion of coordinated change in structures is even older than recorded history. Humans and other animals are capable of learning via operant conditioning, which involves an association of a (i.e. a change) with a reward/punishment (i.e. another change) (Houwer (2018)). While learning this way is very general, it can lead to false pattern recognition in the form of apophenia including pareidolia, and agenticity. This establishes a precedent of coordinated changes in structures being an old, if not ancient, notion.\nA metaphor can be based on this possibly-ancient notion which can be used as a muse for developing more precise mathematical and software tools. But first we would like to highlight both the challenges and potential of using metaphors as a starting point for idea generation.\nA quote that nicely captures one of the concerns of metaphor-driven idea generation was stated by Eric Weinstein speaking to Roger Penrose on the mathematical foundations of modern physics .\\\nOne of the ways in which this quote captures the challenge of using metaphors to inspire mathematical ideas is that metaphors are often ambiguous. They often have multiple interpretations, thus making any persuits of a unique instantiation of a metaphor to be na\"ive. However, this property can be used as a feature' rather than abug’ of idea generation. Based on Figure \\(\\ref{fig:exampleinstantiations}\\), consider that numerous imprecise notions such as “probability”, “network centrality”, “central tendency”, and “change” have multiple instantiations. In the case of network centrality measures there are far more of them than would fit in Figure \\(\\ref{fig:exampleinstantiations}\\). Rather than being concerned about finding a multiplicity of interpretations, we suggest exploiting them for different use cases.\nMotivated by the historical existence of a notion of coordinated change in structures, and expecting some multiplicity of interpretations, here is a simple metaphor: The Trinity of Covariation (Figure \\(\\ref{fig:trinityofcovariation}\\)).\nReflecting on the metaphor, “covariation” as “coordinated changes in structure” can be used as a tool for creative thinking. In relation to this metaphor, the following questions can be considered when doing open-ended idea generation:\nIn this chapter a frame of thinking was introduced in the form of a metaphor, and from it we wish to define more precise notions that help us understand complex systems. The coming chapters discuss both new and existing instances of this metaphor.\n\n\n\n\nAzevedo, Frederico A. C., Ludmila R. B. Carvalho, Lea T. Grinberg, José Marcelo Farfel, Renata E. L. Ferretti, Renata E. P. Leite, Wilson Jacob Filho, Roberto Lent, and Suzana Herculano-Houzel. 2009. “Equal Numbers of Neuronal and Nonneuronal Cells Make the Human Brain an Isometrically Scaled-up Primate Brain.” The Journal of Comparative Neurology 513 (5): 532–41. https://doi.org/10.1002/cne.21974.\n\n\nCook, Mathew. 2004. “Universality in Elementary Cellular Automata.” Complex Systems, 1–40.\n\n\nDeWeerdt, Sarah. 2019. “How to Map the Brain.” Nature Outlook 571 (7766): S6–8. https://doi.org/10.1038/d41586-019-02208-0.\n\n\nHitchcock, Christopher, and Miklós Rédei. 2021. “Reichenbach’s Common Cause Principle.” In The Stanford Encyclopedia of Philosophy, edited by Edward N. Zalta, Summer 2021. https://plato.stanford.edu/archives/sum2021/entries/physics-Rpcc/; Metaphysics Research Lab, Stanford University.\n\n\nHouwer, Jan De. 2018. “Eelen 1980 Classical Conditioning.” Preprint, May. https://doi.org/10.31234/osf.io/vp5cu.\n\n\nKarkaria, Behzad D., Neythen J. Treloar, Chris P. Barnes, and Alex J. H. Fedorec. 2020. “From Microbial Communities to Distributed Computing Systems.” Frontiers Bioengineering Biotechnology 8 (July). https://doi.org/10.3389/fbioe.2020.00834.\n\n\nMill, John Stuart. 1843. A System of Logic, Ratiocinative and Inductive. Cambridge University Press. https://doi.org/10.1017/cbo9781139149839.\n\n\nWolfram, Stephen. 2002. A New Kind of Science. Champaign, IL: Wolfram Media.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "new.html#grade-entropies",
    "href": "new.html#grade-entropies",
    "title": "4  New Instantiations of The Trinity of Covariation",
    "section": "4.6 Grade Entropies",
    "text": "4.6 Grade Entropies\nIn this section we will introduce a family of functions called . Because entropy is an often-misunderstood concept, we provide some historical context and review of mathematical definitions and properties to clarify it sufficiently for understanding the concept of grade entropy. Then we will review grade functions in the context of partially ordered sets and relate them to entropy. Finally we discuss the interpretation of grade entropies and how they relate to the Trinity of Covariation.\n\n4.6.1 Entropy\nThe first notions that would become a formal model of entropy were developed by Rudolf Clausius (Figure \\(\\ref{fig:foundersentropy}\\)(a)), which he described as the of a body in terms of the reciprocal temperature integrated over a heat differential .\nEntropy was formalized as a function of the number of possible microscopic states (i.e. microstates) in the context of statistical thermodynamics by Ludwig Boltzmann (Figure \\(\\ref{fig:foundersentropy}\\)(b)) in his efforts to relate the average kinetic energy of gas particles to the thermodynamic temperature of the gas . Each microstate represents the physical configuration of a part of a physical system, such as the orbital states of electrons in atoms, that are associated with an energy level.\n\nDefinition\nBoltzmann’s entropy equation is given by \\[\\begin{equation*}\nS \\triangleq k_B \\ln \\Omega\n\\end{equation*}\\] where \\(S\\) is the (thermodynamic) entropy, \\(k_B\\) is the Boltzmann constant taking the exact value of \\(1.380649 \\times 10^{-23}\\ \\frac{\\text{J}}{\\text{K}}\\) in units of Joules-per-Kelvin, and \\(\\Omega\\) is the number of equilibrium microstates of a system.\n\nDefinition \\(\\ref{def:boltzmannentropy}\\) assumes that each microstate is equally probable, which is the case when a system is already in thermodynamic equilibrium. To describe the entropy of systems in which not all states are equally probable, a generalization of Definition \\(\\ref{def:boltzmannentropy}\\) given in Definition \\(\\ref{def:gibbsentropy}\\) was developed by Josiah Gibbs (Figure \\(\\ref{fig:foundersentropy}\\)(c)) (Jaynes (1965)).\n\nDefinition\nGibbs’s entropy equation is given by \\[\\begin{equation*}\nS \\triangleq -  k_B \\sum_i p_i \\ln p_i\n\\end{equation*}\\] where \\(S\\) is the (thermodynamic) entropy, \\(k_B\\) is the Boltzmann constant as found in Boltzmann’s entropy, and \\(p_i\\) is the probability of the \\(i\\)th state from a finite set of physical states.\n\nIn 1948 entropy was generalized beyond the context of Physics by Claude Shannon (Figure \\(\\ref{fig:foundersentropy}\\) (d)) to a purely mathematical construct that has applications in many domains. This generalization, sometimes called Shannon-Wiener entropy, is given in Definition \\(\\ref{def:shannonentropy}\\). Jon von Neumann (Figure \\(\\ref{fig:foundersentropy}\\) (e)) adapted this definition to the context of quantum mechanics, which we do not explore further here.\n\nDefinition Shannon’s entropy equation is given by \\[\\begin{equation*}\nS \\triangleq -  K \\sum_i p_i \\ln p_i\n\\end{equation*}\\] where \\(S\\) is the Shannon entropy, \\(K\\) is a proportionality constant, and \\(p_i\\) is the probability of the \\(i\\)th event from a discrete event space.\n\nThe differences between Definition \\(\\ref{def:gibbsentropy}\\) and Definition \\(\\ref{def:shannonentropy}\\) are simple, subtle, but also of great importance. The first difference is explicit: we exchange the Boltzmann constant \\(k_B\\) for an arbitrary proportionality constant \\(K\\). This proportionality constant is often taken to be unity in applications of Definition \\(\\ref{def:shannonentropy}\\), however in principle it could be used to scale the log-linear factor onto a desired measurement scale. The second difference is less explicit, but of great practical significance: the probability measure can be over any discrete random variable.\nSince entropy in a general sense is not about thermodynamic states per se, it is worth considering what it actually tells us about a discrete random variable. Often entropy is heuristically described as a “measure of disorder”, but this way of describing entropy has limitations that are outlined in Lambert (2002). Among them are the observations that maximal entropy distributions can generate ordered structures, and that subjective disagreement between observers can occur over what consistutes ‘disorder’.\nSince entropy (specifically Shannon’s entropy) has a specific mathematical definition, it is worth considering one of its important properties in the form Proposition \\(\\ref{prop:maxentropy}\\).\n\nProposition\nGiven a discrete random variable \\(X\\) with probability space \\((\\Omega, \\mathcal{F}, P)\\), its Shannon entropy \\(H(X)\\) satisfies\n\\[H(X) \\leq \\log |\\Omega| \\]\nwhere $H(X) || $ if-and-only-if \\(P\\) is uniform.\nProof\nSee Theorem 2.6.4 in Cover and Thomas (2006).\n\nProposition \\(\\ref{prop:maxentropy}\\) gives us a specific way to think about what Shannon entropy quantifies: uniformity of the distribution of a discrete random variable. Abstracting entropy beyond the context of thermodynamics and taking this precise notion that quantifies uniformity will be important for understanding grade entropy.\nLet us briefly consider an example of applying the mathematical concept of entropy outside of thermodynamics that comes from ecology. In ecology, a community is a collection of populations. In the analysis of species abundance tables it is desirable to quantify the ‘diversity’ of a community of organisms.\n\n\n\n\n\n\nWhat is a species abundance table?\n\n\n\n\n\nA species abundance table is a matrix whose entries are the count of how many times each species was observed in each sample.\n\n\n\nEntropy is one method of quantifying diversity if we consider diversity in the sense of uniformity of species abundances. The species abundance table gives us an empirical frequency distribution with discrete support. Computing the Shannon entropy from this distribution tells us how uniform the species abundances are in the community.\nIn summary we have noted that entropy was originally motivated and developed within the context of physics, and was later generalized to any discrete probability distribution. Rather than quantifying ‘disorder’ as a synonym for ‘macroscopic messiness’, Shannon entropy tells us about the uniformity of a discrete random variable.\n\n\n4.6.2 Grades and Partially-Ordered Sets\nBefore introducing grade entropies, we will briefly review the properties of relations. All partial orders are relations. A relation is a subset of a Cartesian product of sets. Given a collection of sets \\(S_1,\\cdots, S_n\\), a relation \\(R \\subseteq S_1 \\times S_2 \\times \\cdots \\times S_{n-1} \\times S_n\\) is said to be homogenous if all pairs of sets are equal. Otherwise such a relation is inhomogenous. Relations are also classified by the number of sets being used in the set multiplication. A binary relation is a subset of the Cartesian product of two sets, and more generally an \\(n\\)-ary relation is obtained from taking a subset of the Cartesian product of \\(n\\) sets. In this section we focus on binary relations. Beyond these distinctions, many relations are classified by certain rules about which elements of a set are allowed to be members of the relation.\nThe rows of Table \\(\\ref{tab:relationproperties}\\) show some common types of relations or relation properties including reflexivity, symmetry, antisymmetry, asymmetry, transitivity, and connectedness. These properties in certain combinations give partial orders, strict partial orders, total orders, and strict total orders.\n\n\n\nTable 4.1: Summary of common types of order relations. The listed properties are assumed to hold for all \\(x, y, z\\) as needed from a set \\(S\\) whose binary relation \\(R\\) is a subset of \\(S \\times S\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nProperty\nExpression\nPartial Order\nStrict Partial Order\nTotal Order\nStrict Total Order\n\n\n\n\nReflexive\n\\(xRx\\)\n✓\n\n✓\n\n\n\nIrreflexive\n\\(\\neg x R x\\)\n\n✓\n\n✓\n\n\nSymmetric\n\\(x R y \\implies y R x\\)\n\n\n\n\n\n\nAntisymmetric\n\\(x R y \\land y R x \\implies x = y\\)\n✓\n\n✓\n\n\n\nAsymmetric\n\\(x R y \\implies \\neg y R x\\)\n\n✓\n\n\n\n\nTransitive\n\\(x R y \\land y R z \\implies x R z\\)\n✓\n✓\n✓\n✓\n\n\nConnected\n\\(x \\neq y \\implies x R y \\lor y R x\\)\n\n\n\n✓\n\n\nStrongly Connected\n\\(x R y \\lor y R x\\)\n\n\n✓\n\n\n\n\n\n\n\nThe familiar comparisons \\(a &lt; b\\) and \\(a \\leq b\\) with real numbers make clear that orders exist in one dimension. But in this work we are interested in orders over sets of multidimensional points. One approach is to find a map \\(f:\\mathbb{R}^n \\mapsto \\mathbb{R}\\) and compare \\(f(\\vec{x}) &lt; f(\\vec{y})\\) for any \\(\\vec{x}, \\vec{y} \\in \\mathbb{R}^n\\). A common choice for \\(f\\) is a norm, such as the Euclidean norm. Beyond such function-induced orders, three possibilities include product orders, Pareto orders, and lexicographical orders.\nA is satisfied by the componentwise comparisons all being satisfied. For vectors \\(\\vec{x}, \\vec{y} \\in \\mathbb{R}^n\\), they satisfy a strict product order if \\(x_i &lt; y_i\\) for all \\(i \\in \\{1, \\cdots, n \\}\\). Similarly, they satisfy a non-strict product order if \\(x_i \\leq y_i\\) for all \\(i \\in \\{1, \\cdots, n \\}\\).\nA does not have a strict or non-strict form because it already requires both strict and non-strict componentwise comparisons. For vectors \\(\\vec{x}, \\vec{y} \\in \\mathbb{R}^n\\), they satisfy a Pareto order if \\(x_i \\leq y_i\\) for all \\(i \\in \\{1, \\cdots, n \\}\\) there exists \\(j \\in \\{1, \\cdots, n \\}\\) such that \\(x_j &lt; y_j\\). A Pareto order is more restrictive than a non-strict product order, but less restrictive than a strict product order, in the sense of what fraction of an arbitrary finite set would satisfy the relation. Pareto orders are applied in finance (Amershi (1985)), game theory (Aubin (2014)), and multiobjective optimization (Emmerich and Deutz (2018)).\nA lexicographical order is a kind of ordering that prioritizes comparing certain components before others. An example of a lexicographical order is the use of an alphabet to order words. The alphabet itself is an order on the characters, while the lexicographical order entails looking to compare the first two letters, then the second two, etc. As an example, suppose we had the points \\(a = (2,1)\\) and \\(b=(1,3)\\). Under a lexicographical ordering it holds that \\(b &lt; a\\) because the first component is compared first, and further components are irrelevant beyond the ealiest component that breaks the tie between the points. Lexicographical orders can be strict or non-strict, and are frequently used to define the conditions under which a collection of strings are sorted.\nNext we will give some background to the notion of grades (i.e. ranks) on points based on a given partial order.\n\n\n4.6.3 Graded Posets\nFor the construction of the notion of grade entropies we make use of grades of points. Given a finite collection of multidimensional points, which may possibly be a statistical sample from a population, we can use a partially ordered set (i.e. a poset) to assign a rank to each point. A point dominanted by no other point would have a rank of 1, a point dominated by one point would have a rank of two, and a point dominated by \\(k\\) number points would have a rank of \\(k+1\\).\n\n\n\n\n\n\nNote\n\n\n\nThis description of assigning ranks according to point dominance is sufficient for what we later term a pseudograde function, and that a covering relation provides the constraint needed to obtain a grade function.\n\n\nBefore defining a graded partially ordered set (i.e. a graded poset), it is convenient to define the notion of a covering relation as given in Definition \\(\\ref{def:coveringrelation}\\).\n\nDefinition (Stanley (2011))\nSuppose \\(S\\) is a set with a partial order \\(\\leq\\), and strict partial order \\(&lt;\\) that holds whenever \\(x \\leq y \\land x \\neq y\\).\nThe covering relation, denoted \\(x \\lessdot y\\) for \\(x,y \\in S\\), holds if \\(x &lt; y\\) and there does not exist \\(z \\in S\\) such that \\(x &lt; z &lt; y\\).\n\nA covering relation captures an intuitive notion that certain elements are “beside each other”, which is closely related to the notion of a successor function. A successor function sends a natural number to the next natural number by incrementing by unity, while the predecessor function sends a natural number to the previous natural number by subtracting unity. For a graded poset (Definition \\(\\ref{def:gradedposet}\\)) it is assumed that if two elements are beside each other in the covering relation, then their grades should correspondingly be predecessors or successors of each other.\n\nDefinition (Stanley (2011))\nLet \\(S\\) be a partially ordered set equipped with a rank function \\(\\rho: S \\mapsto \\mathbb{N}\\), where \\(\\rho\\) satisfies the following:\nwhere \\(\\lessdot\\) is a covering relation on \\(S\\). Such a partially ordered set is called a or .\n\nWhat Definition \\(\\ref{def:gradedposet}\\) provides is a clear notion of grading a collection of points under the assumption of a partial order, that these grades preserve the order, and preserve closeness in the sense of a covering relation.\nWith these notions in place, we will now construct the notion of a grade entropy.\n\n\n4.6.4 Grade Entropies\nGrade entropies are about quantifying the totality of a partial order or a strict partial order. Traditionally a given order relation is total, or it isn’t total, with no consideration of how close a relation is to being total. But we suggest that it is desirable to quantify how far short a partial order falls from being a total order, which we informally refer to here as “totality”\n\n\n\n\n\n\nNote\n\n\n\nIn this work we take an information theory approach to quantifying totality, but it may also be possible to approach this notion with metric spaces.\n\n\n\nDefinition\nSuppose that \\(S \\subset   \\mathbb{R}^n\\) is a finite set of points of size \\(|S|=m\\) that is also a graded poset. With a grade function \\(g\\), and probability mass function \\(p\\) over \\(g\\) we define the to be\n\\[H_g \\triangleq - \\sum_{i=1}^{k} p(g_i)  \\log_b  p(g_i) \\]\nwhere \\(b\\) is a chosen base, \\(g_i\\) is a distinct grade assigned to any element of a partition \\(S_i\\) of \\(S\\), and \\(k\\) is the number of distinct grades assigned to elements of \\(S\\).\n\nIt may be desirable to compare grade entropies computed on different systems, but the non-negative real number that results from computing grade entropy in Definition \\(\\ref{def:gradeentropy}\\) depends on \\(m\\). We can utilitize Proposition \\(\\ref{prop:maxentropy}\\) to define a normalized grade entropy as given in Definition \\(\\ref{def:normalizedgradeentropy}\\).\n\nDefinition Given a grade entropy function \\(H_g\\), the normalized grade entropy is given by\n\\[H_{\\gamma} \\triangleq \\frac{H_g}{\\log_b |m|}\\]\nwhere \\(b\\) is the same base used in \\(H_g\\) and \\(m\\) is the number of points\n\nThe normalization in Definition \\(\\ref{def:normalizedgradeentropy}\\) provides a functional with similar properties to the grade entropy in Definition \\(\\ref{def:gradeentropy}\\), but with the additional property of being bounded to the interval \\([0,1]\\). This allows comparisons of grade entropies for variables with different sizes of outcome space, and also makes it clear that the entropy has reached its maximum when the normalized grade entropy is equal to unity.\nNote that Definition \\(\\ref{def:gradeentropy}\\) does not suppose what partial order is used, nor does it suppose which probability distribution is used. One might wish to use product orders, lexicographical orders, Pareto orders, or others. And one might wish to consider either empirical or theoretical discrete probability distributions. Grade entropies allow the researcher to choose the order relation and probability distribution that is relevant to their chosen domain.\nOne limitation of grade entropies is they do not distinguish between partially ordered sets with slightly different non-dominating pairs. Let us consider panel (b) of Figure \\(\\ref{fig:examplegradeentropy}\\) as an example. Nodes and would be assigned equal grades, and nodes and would also be assigned equal grades. If the edge \\(\\rightarrow\\) was also part of the relation this would not change the aforementioned grades, but sometimes it may be desirable to distinguish between these two versions of the lattice. This motivates a generalization of grade entropies that we term , which is achieved by first defining a pseudograded poset by taking Definition \\(\\ref{def:gradedposet}\\) and simply dropping the requirement that \\(x,y \\in S \\land x \\lessdot y \\implies \\rho(x) + 1 = \\rho(y)\\). The definition of a pseudograde entropy is then identical to Definition \\(\\ref{def:gradeentropy}\\) except that a pseudograde function is used instead of a grade function.\n\n\n\n\n\n\nNote\n\n\n\nA normalized pseudograde entropy is similarly obtained *mutatis mutandis.\n\n\nNext we consider how to interpret what a grade entropy tells us about a finite set of points.\n\n\n4.6.5 Interpretation\nIn this subsection we offer a general interpretation of grade entropies that should apply regardless of the choice of partial order or distribution of grades.\\\nProposition \\(\\ref{prop:maxentropy}\\) entails that the grade entropy is maximized when the distribution of grades is uniform. If the grade entropy is maximal for a collection of \\(m\\) points, then the probability measure for each grade is \\(\\frac{1}{m}\\) for each of the \\(m\\) points. If we take our probability to be a normalized counting measure over the set of points, then we must conclude that there is exactly one point per grade. If every point has a distinct grade, then we can equivalently state that no two points share a grade.\\\nIf no two points share a grade, then the partial order is also a total order. With the grade entropy being maximized by this uniformity, it can be considered a quantification of the totality of the partial order. Example \\(\\ref{fig:examplegradeentropy}\\) illustrates three cases including one in which the partial order is also total, a case where it is not completely total but also has some degree of totality, and a third case in which there is no totality (i.e. no dominance of one point over another).\nWe can further interpret grade entropies in a way that depends on the choice of order. Let us consider a product order over a finite collection of points in \\(\\mathbb{R}^n\\) as an example. If the grade entropy is zero, then for every pair of points there must exist at least one component that violates the comparison. Thus a grade entropy of zero for such a product order tells us that the collection of variables is not comonotonic. A collection of variables are comonotonic if they go up or down together whenever one them goes up or down in value.\\footnote{When there exists two variables in which one always decreases as the other increases, and vice versa, this is called antimonotonicity. We are not aware of a term for the case in which some variables within a collection of variables are comonotonic while others are antimonotonic, but we suggest the term anticomonotonic suitably suggests this mixture. Likewise, if the grade entropy is at its maximum then the product order holds for all pairs of points, which entails perfect comonotonicity among those variables. Intermediate values of grade entropy between zero and its maximum would suggest to us a quantification of comonotonicity of a set of variables.\nComonotonicity is related to the Trinity of Covariation when we consider a partial order on points whose components are variables. If the order is total, then the variables must go up and down together in such a way that always satisfies the order relation. Likewise, when the order is entirely non-total the variables never go up and down together in a way that satisfies the order. The change part is implicit in considering the point-to-point comparisons for all points. The part comes from such points be specified by coordinates represented by the components of each tuple. The structure part comes from the notion of a partial order existing on the collection of points.\nInterpreting pseudograde entropies is similar to interpreting grade entropies. It holds that a total order maximizes its value, and if no point dominates another then it will take its minimum value of zero. But pseudograde entropies may be larger than their grade entropy counterpart for a given partially ordered set.\nHaving introduced grade entropies, pseudograde entropies, and their interpretation, we will next summarize the chapter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>New Instantiations of The Trinity of Covariation</span>"
    ]
  },
  {
    "objectID": "new.html#agnesian-operators",
    "href": "new.html#agnesian-operators",
    "title": "4  New Instantiations of The Trinity of Covariation",
    "section": "4.5 Agnesian Operators",
    "text": "4.5 Agnesian Operators\nWe introduce Agnesian operators as an instantiation of the Trinity of Covariation that utilizes elementary calculus. We have named this family of operators after the Italian mathematician Maria Gaetana Agnesi (1718-1799) (Figure \\(\\ref{fig:mariaagnesi}\\)(a)). She was the first woman to be appointed as a mathematics professor (Figure \\(\\ref{fig:mariaagnesi}\\)(c)), and she was known for writing the textbook (Figure \\(\\ref{fig:mariaagnesi}\\)(b)) that covered both differential and integral calculus . Because the Agnesian operators unite the notions of derivatives and integrals into a single operator, it seems symbolically fitting that Maria Agnesi be their namesake because her work brought differential and integral calculus together in the education of young mathematicians.\n\n\n\n\n\n\n\n\nMaria Gaetana Agnesia (Public Domain)\n\n\n\n\n\n\n\n\n\n\nAgnesi’s diploma from Università di Bologna (Public Domain)\n\n\n\n\n\n\nFigure 4.1: Maria Gaetana Agnesi\n\n\n\nDerivatives are linear approximations of the slope of the line tanget to a function at a given point, providing a representation of the change part of the Trinity of Covariation. In the case of intergrals, the definite integral over \\([a,b]\\), \\(\\int_a^b \\frac{df}{dt}dt = f(b) - f(a)\\), models the net change the net change theorem. This idea is adopted to represent the change part of the Trinity of Covariation. We can choose collections of differentiable and/or integrable scalar functions to represent a kind of ‘structure’. The notion of ‘coordination’ can be captured in the form of scalar multiplication in which one might think of a collection of changes mutually scaling each other. Putting these notions together, Definition \\(\\ref{def:totalagnesian}\\) gives a precise definition of the (total) Agnesian operator of a given order.\n\nDefinition\nLet \\(S = \\{x_1(t),  \\cdots, x_n(t) | x_j(t) \\in \\mathbb{R}, t \\in \\mathbb{R} \\}\\) be a collection of suitably smooth or integrable functions of a real parameter \\(t\\), then their Agnesian of order \\(k\\) is given by \\[\\mathcal{A}_{t}^{k} S= \\begin{cases} \\prod_{j=1}^{n} \\frac{d^k}{dt^k}x_j(t) & k &gt; 0 \\\\ \\prod_{j=1}^{n} x_j (t) & k = 0 \\\\ \\prod_{j=1}^{n} \\underbrace{\\int \\cdots \\int}_k x_j(t)\\ \\underbrace{dt \\cdots dt}_k & k &lt; 0. \\end{cases}\\]\n\nBecause the Agnesian is defined using set-builder notation, it is possible to construct a great variety functions with it. For a given collection of \\(n\\) suitably smooth or integrable functions, there exists \\(|\\mathcal{P}(S)/\\emptyset| = 2^n-1\\) (i.e. the powerset excluding the empty set) possible choices of Agnesian functionals that could be defined at some given order. For convenience of notation, we can consider Agnesian operators of scalar functions and vector functions. For a scalar function \\(f(t)\\) we will assume that notation \\(\\mathcal{A}_t^k f(t)\\) is equivalent to \\(\\mathcal{A}_t^k \\{ f(t) \\}\\). In the vector case we will take \\(\\mathcal{A}_t^k \\vec{x}(t)\\) where the components of \\(\\vec{x}(t)\\) are indexed by \\(j \\in \\{1, \\cdots, n \\}\\) to be equivalent to \\(\\mathcal{A}_t^k \\{x_1(t), \\cdots, x_n(t) \\}\\). These two notational conventions immediately gives us the property that \\(\\mathcal{A}_t^k \\vec{x}(t) = \\prod_{j=1}^n \\mathcal{A}_t^k x_j(t)\\), which is used in the proof of Proposition \\(\\ref{prop:agnesiansqueezeinvariant}\\).\nLet us consider the example of a helix embedded in \\(\\mathbb{R}^3\\) according to the vector equation\n\\[\\vec{s}(t) = \\begin{bmatrix}\n\\cos t \\\\\n\\sin t \\\\\nt \\\\\n\\end{bmatrix}.\\]\nFigure \\(\\ref{fig:Agnesianhelixpanel}\\) shows the Agnesians of a helix for orders \\(k \\in \\{-3, -2, -1, 0, 1, 2\\}\\), and illustrates how the order specifies different curves. The zero-order Agnesian in Figure \\(\\ref{fig:Agnesianhelixpanel}\\)(a) has reflective symmetry about \\(t=0\\) and will oscillate between larger amplitudes as \\(|t| \\rightarrow \\infty\\). The first-order Agnesian in Figure \\(\\ref{fig:Agnesianhelixpanel}\\) is equivalent to \\(\\frac{1}{2} \\sin 2t\\), which is a reflection equivariant function (i.e. an odd function) in \\(t\\). As shown in Figure \\(\\ref{fig:Agnesianhelixpanel}\\) (c), the second-order Agnesian is zero for all time because the \\(\\frac{d^2 t}{dt^2} = 0\\) factor. Indeed, any Agnesian of this helix in the given coordinates will be zero if \\(k \\geq 2\\). Panels (d), (e), and (f) of Figure \\(\\ref{fig:Agnesianhelixpanel}\\) show that with constants of integration taken to be zero we have an alternating pattern of odd or even Agnesians depending on the parity of the degree of the polynomial factor \\(\\frac{t^{k+1}}{(k+1)!}\\). For \\(k &lt; 0\\), if \\(k+1\\) is even then the result Agnesian is an odd function of \\(t\\), and if \\(k+1\\) is even then the resulting Agnesian will be odd. Similar to what we observed with the zero-order Agnesian, we find that the negative order Agnesians of this helix will oscilate between larger amplitudes as \\(|t| \\rightarrow \\infty\\).\\\nObtaining a scalar function of time affords us the opportunity to ask elementary questions about the function. Such questions include its domain, image, and extrema. While all of the Agnesian curves in the previous example were defined over all real numbers, this does not need to be the case in general. An example is the curve \\([t, \\log t]^T\\) which will not be defined for \\(t=0\\). The example of the helix also showed us that some Agnesians have local and global optima, while others have no finite global optima.\\\nIn the example of the helix we assumed a given coordinate system. It is important to consider that the Agnesian of a parametric curve is not invariant to linear changes in basis. For a parametric curve \\(\\vec{x}(t) \\in \\mathbb{R}^n\\) and linear transformation \\(T \\in \\mathbb{R}^{n \\times n}\\), it does not hold in general that \\(\\mathcal{A}_t^k \\left[ T \\vec{x}(t) \\right] = \\mathcal{A}_t^k \\left[ \\vec{x}(t) \\right]\\). There exist invariants for the Agnesian of parametric curves in \\(\\mathbb{R}^{2}\\) including rotations of \\(\\pi\\) radians, squeezing, and permutation. The identity matrix is a permutation matrix, and clearly leaves the curve unchanged. In Proposition \\(\\ref{prop:agnesiansqueezeinvariant}\\) we show that multidimensional squeezing is an invariant of the Agnesian operator.\n\nProposition\nLet \\(\\vec{x}(t) \\in \\mathbb{R}^n\\) be a parametric curve and \\(D \\in \\mathbb{R}^{n \\times n}\\) be a diagonal matrix with constant entries and \\(\\det D = 1\\), then\n\\[\\mathcal{A}_t^k \\left[ D \\vec{x}(t) \\right] = \\mathcal{A}_t^k \\left[ \\vec{x}(t) \\right].\\]\nProof\nTaking \\[\nD =\n\\begin{bmatrix}\n   d_{1} & & \\\\\n   & \\ddots & \\\\\n   & & d_{n}\n\\end{bmatrix}\\]\nand \\[\\vec{x}(t) = \\begin{bmatrix}\nx_1(t) \\\\\n\\vdots \\\\\nx_n(t)\n\\end{bmatrix}\\]\nthen \\[D \\vec{x}(t) = \\begin{bmatrix}\nd_1 x_1(t) \\\\\n\\vdots \\\\\nd_n x_n(t)\n\\end{bmatrix}.\\] This entails that\n\\[\\begin{align*}\n\\mathcal{A}_t^k \\left[ D \\vec{x}(t) \\right] =& \\prod_{j=1}^n d_j \\mathcal{A}_t^k x_j(t) \\\\\n=& \\left( \\prod_{j=1}^n d_j \\right) \\left( \\prod_{j=1}^n \\mathcal{A}_t^k x_j(t) \\right) \\\\\n=& \\det D \\mathcal{A}_t^k \\left[ \\vec{x}(t) \\right] \\\\\n=& \\mathcal{A}_t^k \\left[ \\vec{x}(t) \\right]\n\\end{align*}\\]\n\\(\\blacksquare\\)\n\nWhat these invariant linear transformations have in common is that they have determinant of unity. This suggests a hypothesis that the property of being volume-preserving might be a necessary-but-insufficient condition for an operation to be Agnesian-preserving, but we do not explore this hypothesis further in this thesis.\nThe Agnesian provides a scalar function of a single parameter, which could be time, length, or some other variable. It is sometimes desirable to incorporate multiple parameters, like when there is one parameter of time and three directions of physical space. An extension to the Agnesian operator comes from considering coordinated change in multiple parameters in the form of the multiagnesian as given in Definition \\(\\ref{def:multiagnesian}\\).\n\nDefinition\nLet \\(S = \\{x_1(t_1, \\cdots, t_p),  \\cdots, x_n(t) | x_j(t_1, \\cdots, t_p) \\in \\mathbb{R} \\land t_1, \\cdots, t_p \\in \\mathbb{R} \\}\\) be a collection of suitably smooth or integrable functions of a real parameters \\(t_1, \\cdots, t_p\\), then their is given by \\[\\begin{equation*}\n\\mathcal{A}_{(t_1, \\cdots, t_p)}^{k} S = \\begin{cases} \\prod_{j=1}^{n} \\prod_{w=1}^{p} \\frac{d^k}{dt_{w}^k}x_j(t_1, \\cdots, t_p) & k &gt; 0 \\\\ \\prod_{j=1}^{n} \\prod_{w=1}^{p} x_j (t_1, \\cdots, t_p) & k = 0 \\\\ \\prod_{j=1}^{n} \\prod_{w=1}^{p} \\underbrace{\\int \\cdots \\int}_k x_j(t_1, \\cdots, t_p)\\ \\underbrace{dt_{w}  \\cdots dt_{w}}_k & k &lt; 0 \\end{cases}\n\\end{equation*}\\]\n\nThe multiagnesian operator is a natural generalization of the Agnesian operator to include multiple parameters. While the Agnesian operator can be used to study parametric curves in finite-dimensional vector spaces, the multiagnesian operator can be used to study surfaces and hypersurfaces in similar spaces. Using the commutativity and associativity of scalar multiplication, we can readily find that a multiagnesian is the product of Agnesian operators with respect to each parameter:\n\\[\\mathcal{A}_{(t_1, \\cdots, t_p)}^{k} \\vec{x}(t_1, \\cdots, t_p) = \\prod_{w=1}^p \\mathcal{A}_{t_w}^k \\vec{x}(t_1, \\cdots, t_p).\\]\nWe have defined the Agnesian operator, and given some exemplification and discussion of it in mathematical language. In the next subsection we will offer some further interpretations of this operator.\n\n4.5.1 Interpretation\nThe Agnesian operators can be understood in terms of mutually scaling change. Due to the product expansion in the definition of an Agnesian, it is clear that a collection of quantities are mutually scaling each other, and the order tells us what sort of quantities are performing this scaling. For positive order $ k &gt; 0$ we are considering how a collection of derivatives of a corresponding order are mutually scaling. A zero-order Agnesian quantifies how large the functions are together at some value of the parameter. And for the case of negative orders we are considering the coordinated net changes in a collection of functions with respect to the parameter. Decreasing the order below \\(k=-1\\) leads to net changes in net changes, etc, depending on the value of \\(k\\).\\\nA picture that comes from taking the product of a collection of scalars is to consider an \\(n\\)-dimensional box whose lengths are equal to the \\(n\\) scalars being multiplied. The product of these scalars would then be the signed volume of the box. For \\(n \\leq 3\\) we can draw pictures to motivate what this looks like. Let us suppose an example where \\(\\{ f(t), g(t) \\}\\) is our set of functions of time. Figure \\(\\ref{fig:agnesianbox}\\) illustrates a hypothetical planar curve whose coordinates are defined by \\(\\left( \\mathcal{A}_t^k f(t), \\mathcal{A}_t^k g(t) \\right)\\). For a given point on this curve there exists a rectangular region between it and the origin. The signed area of this rectangle gives the value of \\(\\mathcal{A}_t^k\\{ f(t), g(t) \\}\\). The sign of the Agnesian operator follows a similiar pattern to that illustrated in Figure \\(\\ref{fig:quadrantcorr}\\) and Table \\(\\ref{tab:orthantsign}\\) describing the behaviour of multilinear correlation in the sense that multiplying scalars with particular combinations of positive and negative sign result in a scalar with a parciular sign. Unlike the multilinear correlation function, the Agnesian operator as we have defined above does not consider any random variables.\\\nWe noted earlier that Agnesian operators are not invariant to linear changes in basis in general. This mathematical fact can be related back to the Trinity of Covariation: the amount of coordinated change as quantified by the Agnesian operator depends on from what perspective we are `looking’ at the structure.\\\nWe have introduced the Agnesian operator as an instance of the Trinity of Covariation and as an integro-differential operator. In the next section we will introduce grade entropies as an instance of the Trinity of Covariation that connects the subjects of information theory and order theory.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>New Instantiations of The Trinity of Covariation</span>"
    ]
  },
  {
    "objectID": "new.html#interpretation",
    "href": "new.html#interpretation",
    "title": "4  New Instantiations of The Trinity of Covariation",
    "section": "4.2 Interpretation",
    "text": "4.2 Interpretation\nFrom Athreya and Lahiri (2006) we can consider a proposition which logically connects the notion of expectations of products of functions of random variables to the notion of statistical independence. If we choose a function for which this equality does not hold, then we have found that those variables exhibit statistical dependence. Statistical dependence entails that some events are happening more (or less) together than we would expect compared to the univariate probabilities of those events. Imprecisely, statistical dependence suggests to us that something is going on.\n\n\n\n\n\n\nNote\n\n\n\nFormally, Athreya and Lahiri (2006) restrict their result to Borel measurable functions. This restriction can be easily met in real applications of statistics so we do not discuss it further here, but in theory there exists non-Borel measurable functions.\n\n\n\nProposition (Athreya and Lahiri (2006))\nLet \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and let \\(\\{ X_1, \\cdots, X_n \\}\\), \\(2 \\leq n &lt; \\infty\\) be a collection of random variables on \\((\\Omega, \\mathcal{F}, P)\\). Then \\(\\{ X_1, \\cdots, X_n \\}\\) are independent if-and-only-if\n\\[\\mathbb{E}\\left[ \\prod_{j=1}^n f_j(X_j) \\right] = \\prod_{j=1}^n \\mathbb{E} \\left[ f_j(X_j)  \\right]\\]\nfor all bounded Borel measurable functions \\(f_j : \\mathbb{R} \\mapsto \\mathbb{R}\\), \\(\\forall j \\in \\{1, \\cdots, n \\}\\).\n\nThis proposition provides a mathematically precise way of considering multilinear correlation coefficients, and now we turn to building some intuitions about the type of dependence that is described by multilinear correlations. This cannot be done for all choices of functions of random variables, but we will consider the multilinear Pearson correlation as a tractable and instructive example.\nSince the multilinear Pearson correlation is defined on finitely-many variables, it can be difficult to understand what it describes when the number of variables is greater than three. In order to build intuition about what it describes in higher dimensions, let us first consider what it describes in two dimensions. Figure \\(\\ref{fig:quadrantcorr}\\) illustrates how points contribute weight to a Pearson correlation depending on how it is oriented about the centroid of the data.\\\nFigure \\(\\ref{fig:quadrantcorr}\\)(a) illustrates how even when two independent-and-identically distributed standard normal variables are sampled, there will almost certainly be at least a sleight inbalance between the positive and negative weights contributed. Panels (b) and (c) of Figure \\(\\ref{fig:quadrantcorr}\\) emphasize the fact that under perfect negative or positive correlation the points can be reliably found in specific quadrants. Figure \\(\\ref{fig:quadrantcorr}\\)(d) shows how non-linear relationships between the variables does not mean that there will be zero correlation. A salient point to take away from all of these panels is that the weight that a point contributes to a Pearson correlation coefficient is equal to the signed area of the rectangle that can be drawn between the centroid and the data point. For trilinear Pearson correlation coefficients this is replaced with the signed volume of a 3-dimensional box, and in the multilinear case for more than 3 variables this signed area is replaced with the signed hypervolume of an \\(n\\)-orthotope (i.e. an n-dimensional box). The magnitude of these signed measures gets scaled down due to the normalization factor constituted by the product of the \\(p\\)-norms as discussed earlier in this section.\nThe sign of the weight that a point contributes to a multilinear correlation coefficient can be predicted based on the number of variables being considered, and which orthant that point sits within. Table \\(\\ref{tab:orthantsign}\\) shows how when certain combinations of components of a point have a positive or negative sign, the resulting product will predictably have a positive or negative sign depending on whether the correlation is among 2, 3, or 4 variables.\\\nFigure \\(\\ref{fig:trinlinearcorrelationplots}\\) visualizes a trivariate analog of Figure \\(\\ref{fig:quadrantcorr}\\). Specifically, a matrix \\(\\mathbf{\\theta}_{1000 \\times 3}\\) of trainable parameters were optimized by gradient descent to minimize the absolute difference between the multilinear Pearson correlation coefficient calculated on \\(\\mathbf{\\theta}_{1000 \\times 3}\\), as if the columns represented random variables, from a target correlation score. Such data sets are non-unique due to the translation invariance and scaling properties of the multilinear Pearson correlation coefficient. Figure \\(\\ref{fig:trinlinearcorrelationplots}\\) conforms to Table \\(\\ref{tab:orthantsign}\\) in the sense that the directions that either minimized or maximized the multilinear trilinear correlation coefficient could be found in specific octants that were opposite in combinations of signum.\\\nA more subtle pattern arises from observing Table \\(\\ref{tab:orthantsign}\\), which is that the of the number of variables predicts how an orthant relates to its reflection. If the number of variables is even, then the weight contributed by a point and its reflection will have the same sign. But if the number of variables is odd, then weight contributed by a point and its reflection will have opposite signs. This has an important consequence for distributions that are symmetric.\\\nIf a distribution is symmetric, then its odd moments will always be zero. A symmetric distribution may or may not have zero even moments. Thus odd-arity multilinear Pearson correlation coefficients not only tell us about dependence, but can also tell us about the symmetry of the distribution via a contrapositive argument. If a distribution being symmetric implies that the odd-arity multilinear Pearson correlation coefficients are zero, and the odd-arity multilinear Pearson correlation coefficients are not zero, then the distribution in question is not symmetric. The converse argument is not true in general.\\",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>New Instantiations of The Trinity of Covariation</span>"
    ]
  },
  {
    "objectID": "new.html#nightingale-correlation",
    "href": "new.html#nightingale-correlation",
    "title": "4  New Instantiations of The Trinity of Covariation",
    "section": "4.3 Nightingale Correlation",
    "text": "4.3 Nightingale Correlation\nWe now introduce Nightingale correlation as an instantiation of the Trinity of Covariation that generalizes the Nightingale deviation of order \\(p\\) (Definition \\(\\ref{def:minkowskideviation}\\)), and the absolute multilinear covariance. It provides another way of quantifying coordinated change by taking a \\(p\\)-norm of an element-wise product of vectors.\\\n\n4.3.1 Derivations\nWe begin with the notion of a seminorm, see Definition \\(\\ref{def:seminorm}\\), which is a generalization of the notion of a norm where the assumption of point separation is absent. Point separation of a function \\(p:V \\mapsto \\mathbb{R}\\) is the property that \\(p(x) = 0 \\implies x = 0\\) which in the context of norms gives the intuitive property that the size of a vector is only zero when the vector is the zero vector.\\\nHere we generalize the notion of a seminorm from a function of a single vector to a function of multiple vectors as given in Definition \\(\\ref{def:multiseminorm}\\), which we refer to as . The prefix refers to the use of multiple vectors. The approach to this generalization is to take the subadditivity and absolute homogeneity properties that hold in a seminorm, and expand the definition by taking this to be true input-wise for a multiary function.\\\nWith the notion of a multiseminorm in mind as an abstract type of function, we can define a specific function that can be computed on finite vectors which satisfies these properties. This is given in Definition \\(\\ref{def:minkowskimultiseminorm}\\) to be the , which is a \\(p\\)-norm of an elementwise product of a collection of vectors.\\\nThe Minkowski multiseminorm of order \\(p\\) is a multiseminorm. The subadditivity property can be confirmed by considering Minkowski’s inequality (i.e. the triangle inequality generalized to \\(p\\)-norms), and the absolute homogeneity can be checked by scaling an arbitrary input and applying some algebra to obtain the desired result.\\\nJust as a norm can induce a metric, so can a seminorm induce a pseudometric. Likewise, one can define a metric-like function from the notion of a multiseminorm, which we define to be a . In the particular case of the Minkowski multiseminorm of order \\(p\\) we define a function in Definition \\(\\ref{def:minkowskimultipseudometric}\\) called a which inherits the properties of a pseudometric input-wise.\\\nTaking Definition \\(\\ref{def:minkowskimultipseudometric}\\) that applies to vectors, we can define a statistical function under an assumption of \\(p\\)-integrability that considers the deviations from the mean coordinated by an index set. This function we take to be the as given in Definition \\(\\ref{def:minkowskimultideviation}\\).\\\nDefinition \\(\\ref{def:minkowskimultideviation}\\) represents a unification of multiple notions in addition to being a generalization. We saw with Definition \\(\\ref{def:minkowskideviation}\\) giving the Nightingale deviation of order \\(p\\) that the standard deviation can be generalized through the use of different orders of norms. Since the Nightingale covariance of order \\(p\\) reduces to the Nightingale deviation of order \\(p\\) when there is only one variable, the Nightingale covariance of order \\(p\\) is a generalization of the Nightingale deviation of order \\(p\\) and the standard deviation. The Nightingale covariance of order \\(p\\) is also a generalization of the multilinear absolute covariance, which is obtained when \\(p=1\\).\\\nSimilar to multilinear correlation, it is desirable to find a normalization of Nightingale covariance using a suitable inequality. The inequality in Proposition \\(\\ref{prop:nightinequality}\\) serves this purpose, which generalizes the generalized H\"older’s inequality for sums given in Proposition \\(\\ref{thm:generalizedholdersinequalitysums}\\).\\\nWith Proposition \\(\\ref{prop:nightinequality}\\) we have a normalization for the Nightingale covariance which we us to define the as in Definition \\(\\ref{def:nightingalecorrelation}\\).\\\nHaving defined Nightingale correlation, we will next discuss how to interpret it.\n\n\n4.3.2 Interpretation\nThe Nightingale correlation coefficient is a mathematically precise formula that intuitively encodes simultaneous change because in each instance it is quantifying how much a collection of random variables deviate together from their centroid.\nSimilar to the multilinear Pearson correlation coefficient, the Nightingale correlation coefficient can be interpreted as being weighted measures of \\(n\\)-orthotopes drawn out between a centroid and a point. A key difference between these two statistics is that the contributing weights in the expectation of product deviations will always be positive. Consider again Figure \\(\\ref{fig:quadrantcorr}\\), but as if all the rectangles were positive (i.e. red) for a visual understanding of this statistic. This distinction also means that the Nightingale correlation does not distinguish between orthants in the same way, and that a symmetric distribution will not necessarily have a zero Nightingale covariance among an odd number of variables.\nLastly, the Nightingale correlation coefficient uses an expectation of a product of functions of random variables, so it is interpretable in terms of Proposition \\(\\ref{thm:prodmomentindependence}\\).\nNext we will introduce inner correlations.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>New Instantiations of The Trinity of Covariation</span>"
    ]
  },
  {
    "objectID": "new.html#inner-correlation",
    "href": "new.html#inner-correlation",
    "title": "4  New Instantiations of The Trinity of Covariation",
    "section": "4.4 Inner Correlation",
    "text": "4.4 Inner Correlation\nThe notion of an inner product space (Definition \\(\\ref{def:innerproductspace}\\)) is a central concept of many disciplines of modern mathematics, and has found applications in virtually every branch of science. A familiar example to a student of linear algebra is the dot product between two vectors, while in statistics it can be found in the covariance between two random variables. They also occur frequently in functional analysis where infinite-dimensional vector spaces such as Hilbert spaces and Banach spaces are considered.\\\nWe will turn to two generalizations of inner product spaces to define new correlation functions on collections of random variables.\n\n4.4.1 Misiak Correlation\nIn 1989 Misiak introduced \\(n\\)-inner product spaces as a natural generalization of \\(2\\)-inner product spaces . The earlier motivations for Misiak’s work were built on a history of studying abstract \\(n\\)-dimensional metric spaces, which we do not focus on in this work. Rather we will focus on using the definitions and propositions developed by Misiak to define a new statistc. We begin with considering a generalization of inner products given in Definition \\(\\ref{def:ninnerproduct}\\).\\\nDefinition \\(\\ref{def:ninnerproduct}\\) is quite general, and does not by itself give us specific a function that we could compute on a function or random variable. Rather \\(n\\)-inner products represent an abstract class of functions that satisfy a list of axioms, similar to inner products themselves. Misiak provided a way of systematically defining \\(n\\)-inner products from inner products using the notion of a determinant of a matrix of inner products as given in Proposition \\(\\ref{prop:misiakcorollary15}\\).\\\nProposition \\(\\ref{prop:misiakcorollary15}\\) allows us to make various choices of inner products on suitable operands. In the context of a sample we might choose for each vector to be an indexed set of deviations of the mean of instantiations of a random variable.\\\nIt would be desirable to have normalized statistic onto an interval of \\([-1,1]\\) as we achieved with multilinear correlations earlier in this chapter. This is especially the case when the scale of the statistic is not readily interpreted. Misiak provided an inequality that sets bounds on the value that an \\(n\\)-inner product can take for a given set of vectors, which is given in Proposition \\(\\ref{prop:cauchyBunyakowskiinequalityarbitraryn}\\) as the .\\\nWith the above definitions and propositions, we can construct a new statistic called (Definition \\(\\ref{def:misiakscorrelation}\\)).\\\n\n\n4.4.2 Trenevski-Maleski Correlation\nWhile \\(n\\)-inner products compare a pair of vectors or functions to a collection of other vectors or functions, a further generalization of this is possible that compares one collection of vectors or functions to another collection of vectors or functions. A was defined by , which is given in Definition \\(\\ref{def:trencevskimalceski}\\).\\\nAccording to , the \\(n\\)-inner product defined by is a special case of the generalized \\(n\\)-inner product by the relation \\[(\\vec{x}, \\vec{y} | \\vec{z}_1, \\cdots, \\vec{z}_{n-1}) = \\langle \\vec{x}, \\vec{z}_1, \\cdots, \\vec{z}_{n-1} | \\vec{y}, \\vec{z}_1, \\cdots, \\vec{z}_{n-1} \\rangle.\\]\nWith a generalization of \\(n\\)-inner products as given in Definition \\(\\ref{def:trencevskimalceski}\\) we desire a specific approach to defining examples of generalized \\(n\\)-inner products. provide an analogous result to Proposition \\(\\ref{prop:misiakcorollary15}\\) in the form of Proposition \\(\\ref{prop:trencevskiexample}\\).\\\nIt is also desirable to obtain a normalization of any function that is an example of Proposition \\(\\ref{prop:trencevskiexample}\\). Similar to Proposition \\(\\ref{prop:cauchyBunyakowskiinequalityarbitraryn}\\) proven in , provide the analogous result as given in Proposition \\(\\ref{prop:trencevskiinequality}\\) to obtain a normalization.\\\nTaking our choice of inner product to be the mixed product moment between two random variables, and utilizing it to construct a normalized statistic we obtain Definition \\(\\ref{def:trencevskimalceskicorrelation}\\).\\\n\n\n4.4.3 Interpretation\nIn terms of the Trinity of Covariation, making a substitution such as \\(U := U - \\mathbb{E}[U]\\) entails that the resulting determinants of matrices computed via either Proposition \\(\\ref{prop:misiakcorollary15}\\) or Proposition \\(\\ref{prop:trencevskiexample}\\) will be equivalent to computing the determinant of some covariance matrix. The covariance matrix itself is a quantification of coordinated change between pairs of variables, and its determinant does so in an aggregated way. Due to our usage of expectations of random variables, we can also leverage Proposition \\(\\ref{thm:prodmomentindependence}\\) in knowing that statistical independence implies that such expectations should distribute across products of measureable functions of random variables. In the particular case of computing covariances, we know that \\(X \\ind Y \\implies \\operatorname{Cov}[X,Y] = 0\\).\\\nDefinition \\(\\ref{def:misiakcorrelation}\\) and Definition \\(\\ref{def:trencevskimalceskicorrelation}\\) also tell us either about linear dependence or about spanning the same subspace through Proposition \\(\\ref{prop:trencevskilineardependence}\\).\\\nIf either of the first two conditions hold in Proposition \\(\\ref{prop:trencevskilineardependence}\\) then the derived correlation coefficients will be indeterminant because both the left and the right hand sides of Proposition \\(\\ref{prop:trencevskiinequality}\\) will be zero. Thus calculating the numerator and factors of the denominator of the Misiak and Trenevski-Maleski correlation coefficients to check if linear dependence holds is recommended. In the remaining third case equality holds if the two collections of vectors span the same subspace, yielding a correlation score of \\(\\pm 1\\) depending on whether the orientation of the vectors is needed. Section 3 of provides a discussion of how such a coefficient as given in Definition \\(\\ref{def:trencevskimalceskicorrelation}\\) can be defined as a cosine of the angle between two subspaces which can be used to obtain a metric on subspaces of a vector space \\(V\\) simply by taking the \\(\\arccos\\) to obtain the angle. They show that such an angle is invariant to choice of basis. Therefore inner correlation coefficients as we have defined in this section are a way of describing the “closeness” of two sets of random variables in a way that does not depend on scaling or translation.\\",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>New Instantiations of The Trinity of Covariation</span>"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "5  ConAction",
    "section": "",
    "text": "We believe that the mathematical functions developed in this thesis are far more likely to be used if there exists a software package that implements them. This is the purpose of ConAction, a Python package we developed as part of this thesis. We chose the Python programming language for its rapid development times, package management system, mathematical libraries, portability, and support for performance and feature enhancements. In this package the user will find tools for computing instances of the Trinity of Covariation on data matrices, numerically on Python functions, or symbolically on computer algebra expressions. This diversity of implementations facilitates ConAction being useful to both theoreticians and practitioners.\nIn this chapter we cover the installation of ConAction, its dependencies, the main features, our testing approach, the algorithmic complexity of the implemented algorithms, some run time performance analysis, our approach to documentation of ConAction, and our plans for future support."
  },
  {
    "objectID": "software.html#installation",
    "href": "software.html#installation",
    "title": "5  ConAction",
    "section": "5.1 Installation",
    "text": "5.1 Installation\nIn order to reduce the technical skills required to set up ConAction, it must be easy to install. This is really important because many users give up on using a package if it requires considerable configuration. We also wanted the installation process to be the same on any platform. For these reasons, we use the Python package management system known as Pip. Pip makes it easy to install packages from the Python Package Index (PyPI (2003)). Figure \\(\\ref{fig:installation}\\) shows how ConAction can be installed with a single command.\nBecause Pip is available on MacOS, Windows, and various distributions of Linux, the command given in Figure \\(\\ref{fig:installation}\\) can be run on practically any desktop. Thus this approach to installation is simple to perform and cross-platform."
  },
  {
    "objectID": "software.html#dependencies",
    "href": "software.html#dependencies",
    "title": "5  ConAction",
    "section": "5.2 Dependencies",
    "text": "5.2 Dependencies\nConAction has dependencies in order to avoid reinventing solutions to solved problems, and to reduce development time. The main submodules of ConAction and their dependencies are given in Figure \\(\\ref{fig:dep_tree}\\).\nThe supported submodules numparam, estimators, sampling, and symparam all depend upon numpy for either numerical and/or array operations. The scipy package provides the singular values of a correlation matrix and integration techniques in numparam as well as data ranking functions estimators. The tqdm package provides progress bars for some of the functions found in estimators and sampling that potentially take a long time. The pathos package is used for in numparam and estimators to parallelize certain loops (McKerns and Aivazis (2010)). We preferred the pathos package over the multiprocessing package in the standard Python library because it uses dill instead of pickle to serialize objects, and dill covers a wider range of cases including nested functions (McKerns et al. (2011)). The networkx package is used in computing grade entropies by providing graph algorithms and data structures related to the lattice representation of a partial order."
  },
  {
    "objectID": "software.html#features-by-module",
    "href": "software.html#features-by-module",
    "title": "5  ConAction",
    "section": "5.3 Features by Module",
    "text": "5.3 Features by Module\nThe ConAction package is organized into multiple sub-modules to support different features (Figure \\(\\ref{fig:conactionmodules}\\)). The same function names are used for a given mathematical function in different modules to make it easier for users to remember which command they want. But this also means that users of ConAction must also adhere quite strictly to the instructions in the Python Enhancement Proposal version 8 (PEP8) to avoid wildcard imports of the form from <module> import * because names can be overwritten in the namespace (Rossum, Warsaw, and Coghlan (2001)). Rather we intend for users to import a submodule of ConAction, and use dot notation to specify the function they wish to use.\nThe estimators module contains many of the functions that can be computed on data matrices (i.e. tabular data), and thus can be thought of as statistical estimators for most intents and purposes. Table \\(\\ref{tab:conactionfunctions}\\) is a summary of the available functions in this submodule.\nThe example in Figure \\(\\ref{code:estimatorexample}\\) shows how the multilinear Pearson correlation coefficient can be estimated from data. Any of the functions in Table \\(\\ref{tab:conactionfunctions}\\) can be called in a similar fashion. \\\nThe numparam submodule contains similar functions to estimators, but the purpose and scope differs. While estimators contains estimators to be computed on data matrices, whereas numparam submodule assumes that a mathematical function is known and that numerical integration of the function is desired.\nThe submodule is most similar to the submodule in its scope. But rather than numerically integrating the statistic as is done in , computes the integrals symbolically.\nAt this time both and assumes a uniform probability measure, however future development will look at allowing user-specified probability measures.\\\nThe module allows contains resampling methods used in Chapter \\(\\ref{chap:mtnpinebeetle}\\).\\\nNext we will consider how ConAction has been tested."
  },
  {
    "objectID": "software.html#testing",
    "href": "software.html#testing",
    "title": "5  ConAction",
    "section": "5.4 Testing",
    "text": "5.4 Testing\nSoftware testing is an important part of ensuring software quality. Due to the mathematically precise nature of the features of ConAction, it is readily testable by setting up function inputs and checking whether the correct (usually numerical) output is obtained.\nThe Python standard library includes the doctest module which served as the primary mode of testing this first release of ConAction. Every function in the Conaction code base has a docstring under the function name, including an example input and output. The doctest module can be imported, and running doctest.testmod() function will go through every such example for every function to check if the correct output is given.\nAs unforeseen cases are found by users, this will require further testing outside the scope of the examples used in the documentation strings. Future testing will make use of the unittest module which is built-in to the standard Python library to perform unit tests.\nThus ConAction has undergone basic testing to ensure a minimal viable product, but can be further tested when unforeseen cases arise."
  },
  {
    "objectID": "software.html#algorithmic-complexity",
    "href": "software.html#algorithmic-complexity",
    "title": "5  ConAction",
    "section": "5.5 Algorithmic Complexity",
    "text": "5.5 Algorithmic Complexity\nIn this section we will overview the theoretical scalability of the algorithms implemented in ConAction.\n\n5.5.1 Estimators\nThe submodule has numerous functions that compute expectation operators. When computed on a sample, we took any expectation \\(\\mathbb{E}[U]\\) of a random variable \\(U\\) to be estimated by\n\\[\\bar{u} = \\frac{1}{m} \\sum_{i=1}^m u_i\\]\nwhere \\(\\bar{u}\\) is the sample mean of a sample \\(\\{ u_1, \\cdots, u_m \\}\\). Computing such an expectation requires \\(\\mathcal{O}(m)\\) time and space, but since this is often computed for each variable in a collection of \\(n\\) random variables the complexity often comes to \\(\\mathcal{O}(mn)\\) time. While multilinear correlations and Nightingale’s correlation involve more operations than just computing a sample average for each variable, none of them change the resulting complexity when computing using BEDMAS order of operations.\\\nIn computing the Misiak correlation and Trenevski-Maleski correlation we always took the inner product to be the dot product when computed in the submodule. Most of the steps of computing these coefficients are also in \\(\\mathcal{O}(mn)\\) of time which is due to computing the inner products of all pairs of variables to obtain a Gram matrix. But the overall time complexity will be that of the algorithm used to compute the determinant of the Gram matrix. We used the function which computes the determinant using a \\(\\mathcal{O}(n^3)\\) algorithm based on performing a LU decomposition.\\\nComputing the partial Agnesian of non-negative order \\(k \\geq 0\\) has a complexity in terms of the sample size \\(m\\), the number of variables \\(n\\) (not inlcuding the parameter \\(t\\)), and the order \\(k\\). Instead of computing the partial derivatives , they are approximated using\n\\[\\frac{\\partial x_j(t)}{\\partial t} \\approx \\frac{x_{i,j} - x_{i-1,j}}{t_i - t_{i-1}}\\]\nwhere \\(j \\in \\{ 1, \\cdots, n \\}\\) and \\(i \\in \\{ 1, \\cdots, m \\}\\). It requires \\(\\mathcal{O}(m)\\) time to compute this for a single variable, and consequently requires \\(\\mathcal{O}(mn)\\) time for all the variables. Repeating this calculation \\(k\\) times suggests that the overall time complexity is \\(\\mathcal{O}(mnk)\\).\\\nCalculating grade entropy involves comparing each point to each other point, suggesting at least \\(\\mathcal{O}(m^2)\\) time complexity is required, to construct a directed acyclic graph (DAG) of which points dominate other points. But since each point has \\(n\\) components to be compared in a componentwise fashion, the time complexity for constructing the DAG is actually \\(\\mathcal{O}(m^2n)\\). We then perform a transitive reduction on the constructed DAG to convert it to a lattice of the partial order. showed that the time complexity of performing a transitive reduction of a DAG requires the same complexity as a matrix multiplication, which we can assume to be somewhat better than \\(\\mathcal{O}(m^3)\\) depending on the implmentation. Our implementation of grade entropy uses the function. At first glance the source code for has only two nested for loops, but within the inner loop there exists a set comprehension in place of a third for loop. Next a topological sort is performed, which requires \\(\\mathcal{O}(|V| + |E|)\\) where \\(V\\) and \\(E\\) are the vertex and edge sets of the lattice, respectively . We know that \\(|V| = m\\) and that \\(|E| \\leq |V \\times V| = m^2\\), suggesting that we are adding no more than \\(\\mathcal{O}(m^2)\\) which is already a factor of \\(\\mathcal{O}(m^2n)\\) and is dominated by the \\(\\mathcal{O}(m^3)\\) time complexity of computing a transitive reduction. Thus the topological sort in our implementation does not add to the complexity. The remaining steps of looping over the nodes in topological order, counting the distinct grades, and computing the entropy are all linear complexity or dominated by linear complexity, and thus do not change the time complexity in terms of either \\(m\\) or \\(n\\).\\\nThe algorithm we implemented for computing a pseudograde entropy based on a strict product order has a better time complexity than that of our grade entropy implementation because it does not require performing a transitive reduction. Similarly to grade entropy, every point is compared to each other point in a componentwise fashion giving a \\(\\mathcal{O}(m^2n)\\) time complexity in order to occupy an \\(m \\times m\\) adjacency matrix. Summing this matrix along one of the index sets provides an array of the pseudogrades of the points. Counting the distinct pseudogrades from this array, normalizing them into probabilities, and computing the entropy all take linear-or-better time and therefore do not further change the complexity.\n\n\n5.5.2 NumParam and SymParam\nThe complexity of the functions computed in numparam and symparam are more difficult to consider than those in the estimator module because (possibly non-constant) functions are considered rather than a data matrix.\nEach function scales linearly in the number of functions it takes as input just as its counterpart in the estimators submodule scales linearly in the number of variables. But since we are no longer considering a sample of size \\(m\\) but rather a collection of functions either at a point or over an interval, the scalability in terms of \\(m\\) does not apply. In the case of the Agnesian operators, both differentiation and integration are considered whereas the various correlation coefficients all require integration.\nWe refer the interested reader to scipy.integrate.quad in the SciPy documentation for further reading of how the integration in numparam works, and to the the SymPy documentation for information about how integrals are symbolically computed in symparam."
  },
  {
    "objectID": "software.html#numparam-and-symparam",
    "href": "software.html#numparam-and-symparam",
    "title": "5  ConAction",
    "section": "5.6 NumParam and SymParam",
    "text": "5.6 NumParam and SymParam\nThe complexity of the functions computed in and are more difficult to consider than those in the module because (possibly non-constant) functions are considered rather than a data matrix.\\\nEach function scales linearly in the number of functions it takes as input just as its counterpart in the submodule scales linearly in the number of variables. But since we are no longer considering a sample of size \\(m\\) but rather a collection of functions either at a point or over an interval, the scalability in terms of \\(m\\) does not apply. In the case of the Agnesian operators, both differentiation and integration are considered whereas the various correlation coefficients all require integration.\\\nWe refer the interested reader to in the SciPy documentation for further reading of how the integration in works, and to the the SymPy documentation for information about how integrals are symbolically computed in ."
  },
  {
    "objectID": "software.html#performance",
    "href": "software.html#performance",
    "title": "5  ConAction",
    "section": "5.6 Performance",
    "text": "5.6 Performance\nRun time performance tests were used to ensure that (1) the code ran in a reasonable amount of time, and (2) compare to a pure Python implementation that we called “naïve” in that it did not utilize certain optimization strategies. Putting aside the choice of algorithm, there were two main approaches we took to implementing functions in ConAction to improve the run time performance.\nThe first was to avoid dynamic typing when it is safe to assume static types. This is exemplified by Figure \\(\\ref{fig:intextperformance}\\)(a) where the multilinear correlation coefficient was computed on increasing sample sizes with either dynamic typing (naive.py) or static typing (estimators.py). Using static typing can provide an order of magnitude or more improvement in rune times, which is used heavily as a strategy for improving the performance of functions in .\nThe second strategy to improve performance was to parellelize any nested loops. We chose not to parallelize every function because many of them would run slower due to the process (i.e. thread) management overhead. In the case of the pseudograde_entropy there exists a nested for loop, and parallelizing the inner loop had a marked improvement in performance for large input sizes. Figure \\(\\ref{fig:intextperformance}\\)(b) illustrates two effects of changing the number of threads working on computing the pseudograde entropy using the function. The first effect is that there is increasing overhead as the number of threads increases, which is most evident for the smaller sample sizes where using fewer threads was actually faster. The second effect is the improved scalability of using more threads, which can be inferred from Figure \\(\\ref{fig:intextperformance}\\)(b) by noticing that using 64 threads gave a nearly flat response to increasing the sample size up to \\(10^4\\) from \\(2\\) whereas using 1 or 2 threads had a noticable increase in runtime by an order of magnitude.\nOther minor improvements were made in the implementation. For example, multilinear correlation coefficients and Nightingale’s correlation coefficient involve computing \\(n\\)th roots in their denominator, which were computed only once by taking advantage of the distributive property of multiplication that entails \\(\\sqrt[n]{\\prod_{j=1}^n x_j} = \\prod_{j=1}^n \\sqrt[n]{x_j}\\). This does not affect the algorithmic complexity classes these functions have as the sample size or number of variables increases, but it is expected to slightly improve run time and numerical precision.\nThere are also optional ways to improve the performance of ConAction either by using auxillary packages or by changing the data type of the input data.\nBecause the makes heavy use of the NumPy package, we can use the Numba package which is designed to interroperate with NumPy to increase its performance. Numba provides options for parallelism, just-in-time compiling, and ahead-of-time compiling. Numba is not a dependency for ConAction because at the time of writing the latest version of Numba was not compatible with the latest version of NumPy under Python 3.10, but this may change in the future.\nWe can also make some adjustments for when the input data matrix is very large and/or sparse. The function can be used to create a virtual memory representation of an array that can be used like any ordinary NumPy array, including as a data matrix in ConAction. Using virtual memory involves using hard drive space, so the increase in apparent memory comes at the cost of run time because of the added input-output (IO) between memory and the hard drive. Likewise, the submodule of SciPy contains classes for sparse matrices that can also be used like ordinary NumPy arrays in ConAction, which can dramatically improve the effective short-term storage space.\nIn this section we discussed how ConAction has some built-in performance enhancements, and the availability of tools for increasing performance when coding large or sparse data matrices."
  },
  {
    "objectID": "software.html#documentation",
    "href": "software.html#documentation",
    "title": "5  ConAction",
    "section": "5.7 Documentation",
    "text": "5.7 Documentation\nWhile being easy to install and use are essential requirements for any scientific computing package, it is likewise important that the software package is well-communicated. One of the ways that a software package can be communicated is through effective documentation.\nIn this thesis we used two forms of documentation in a integrated way thanks to modern package development tools.\nThe first for was to include documentation strings within every function and class definition so that programmers inspecting the code can understand the gist of what is entailed in the code. This form of documentation can be found throughout the original source files if inspected, however calling the built-in Python function will also display these documentation string.\nThe second form of documentation was generated from the former by a combination of tools including Sphinx and Autodoc, which allowed us to automatically generate HTML documentation based on a combination of the documentation strings we wrote and some additional restructured text files we provided. This improves the maintainability of the software by ensuring that changes need to only be made in one or two places when a change to the source code is made. The documentation is hosted at , but it can also be built locally by going to the path and calling the command."
  },
  {
    "objectID": "software.html#future-support",
    "href": "software.html#future-support",
    "title": "5  ConAction",
    "section": "5.8 Future Support",
    "text": "5.8 Future Support\nWe will continue to support the ConAction Python package through its GitHub repository after the completion of this thesis. This includes addressing issues raised by users about the correctness and performance of the code, considering feature requests, and keeping the package compatible with supported versions of dependencies."
  },
  {
    "objectID": "software.html#conclusion",
    "href": "software.html#conclusion",
    "title": "5  ConAction",
    "section": "5.9 Conclusion",
    "text": "5.9 Conclusion\nConAction is a Python package which provides researchers and data analysts with functions for computing various notions of coordinated change in structures. While ConAction is not yet a mature software due to only recently being released to the public, it has undergone testing for performance and correctness, it is documented, and has a framework for further development and support.\nThe starting set of features in ConAction it suitable for supporting simulation studies, empirical research, and theoretical research.\n\n\n\n\nMcKerns, Michael M., and Michael A. G. Aivazis. 2010. “Pathos: A Framework for Heterogenous Computing.” GitHub. https://uqfoundation.github.io/project/pathos.\n\n\nMcKerns, Michael M., Leif Strand, Tim Sullivan, Alta Fang, and Michael A. G. Aivazis. 2011. “Building a Framework for Predictive Science.” Proceedings of the 10th Python in Science Conference.\n\n\nPyPI. 2003. “Python Package Index - PyPI.” Python Software Foundation. https://pypi.org/.\n\n\nRossum, Guido van, Barry Warsaw, and Nick Coghlan. 2001. “Style Guide for Python Code.” PEP 8. Python Software Foundation. https://www.python.org/dev/peps/pep-0008/."
  },
  {
    "objectID": "index.html#preface-to-the-first-edition",
    "href": "index.html#preface-to-the-first-edition",
    "title": "ConAction, Second Edition.",
    "section": "Preface to the First Edition",
    "text": "Preface to the First Edition\nThroughout the writing of this thesis I have received support and assistance that should be acknowledged.\nMy thanks to Dr. Alex Aravind who initially took me on as a student in 2020. He was instrumental in guiding me in the computer science aspects of this project. While he was my supervisor he always told me to keep going and enjoy the process. I have tried my best to stay true to his advice throughout this thesis.\nI must equally thank Dr. Edward Dobrowolski for taking me on as a student in 2021 when Dr. Aravind unexpectedly became unable to continue as my supervisor. His encouragement and guidance have been invaluable in embracing my strengths in this thesis project. I have benefited from his facility with natural languages.\nI also thank my committee members Dr. Brent Murray and Dr. Mohammad El Smaily for both encouraging and challenging me. Their ernest enquiries to understand my work have pushed me to think more clearly.\nI wish to show gratitude to particular members of the UNBC faculty. I thank Dr. Stephen Rader, Dr. Margot Mandy, and Dr. Alia Hamieh for their advice and support. I thank Dr. Andy Wan for allowing me to sit in on his Mathematics of Machine Learning course, and I thank both Dr. Andy Wan and Dr. Geoffrey McGregor for always supporting my participation in the UNBC Interdisciplinary Weekly Seminar Series as a speaker and as an audience participant.\nThanks is also due to my friends and colleagues in the former Aravind research group including Dylan Fossl, Daniel Kopf, Conan Veitch, and Daniel O’Reilly for their sincerity and support in a confusing loss of Dr. Aravind as a supervisor.\nAnd I thank my others friends who inspired or encouraged me during this thesis including Simon Harris, Amy Jimmo, Catherine Sprangers, Alix Schebel, and Richard Nhan.\nMost of all I thank my family including Aaron Seilis, Pat Seilis, Mark Seilis, Flo Dew, and Derwyn Dew for their unwavering support. My grandfather, Derwyn Dew, passed away in January of 2022 due to complications of esophageal cancer. He is loved and missed.",
    "crumbs": [
      "Prefaces"
    ]
  },
  {
    "objectID": "index.html#preface-to-the-second-edition",
    "href": "index.html#preface-to-the-second-edition",
    "title": "ConAction, Second Edition.",
    "section": "Preface to the Second Edition",
    "text": "Preface to the Second Edition\nThis second edition was motived by learning about the existence of the Quarto document preparation system. As an exercise I have migrated my thesis to an online book format. Even after completing my thesis and graduating with a masters in Computer Science, I keep busy learning.\nI still feel gratitude to all the people who helped me get this far.\nDeveloping this second edition has also given me the opportunity to address errata in the first edition and add clarifications or additional information.",
    "crumbs": [
      "Prefaces"
    ]
  }
]