% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={ConAction, Second Edition.},
  pdfauthor={Galen Seilis},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{ConAction, Second Edition.}
\author{Galen Seilis}
\date{2022-09-16}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{Prefaces}\label{prefaces}
\addcontentsline{toc}{chapter}{Prefaces}

\markboth{Prefaces}{Prefaces}

\section*{Preface to the First
Edition}\label{preface-to-the-first-edition}
\addcontentsline{toc}{section}{Preface to the First Edition}

\markright{Preface to the First Edition}

Throughout the writing of this thesis I have received support and
assistance that should be acknowledged.

My thanks to Dr.~Alex Aravind who initially took me on as a student in
2020. He was instrumental in guiding me in the computer science aspects
of this project. While he was my supervisor he always told me to keep
going and enjoy the process. I have tried my best to stay true to his
advice throughout this thesis.

I must equally thank Dr.~Edward Dobrowolski for taking me on as a
student in 2021 when Dr.~Aravind unexpectedly became unable to continue
as my supervisor. His encouragement and guidance have been invaluable in
embracing my strengths in this thesis project. I have benefited from his
facility with natural languages.

I also thank my committee members Dr.~Brent Murray and Dr.~Mohammad El
Smaily for both encouraging and challenging me. Their ernest enquiries
to understand my work have pushed me to think more clearly.

I wish to show gratitude to particular members of the UNBC faculty. I
thank Dr.~Stephen Rader, Dr.~Margot Mandy, and Dr.~Alia Hamieh for their
advice and support. I thank Dr.~Andy Wan for allowing me to sit in on
his Mathematics of Machine Learning course, and I thank both Dr.~Andy
Wan and Dr.~Geoffrey McGregor for always supporting my participation in
the UNBC Interdisciplinary Weekly Seminar Series as a speaker and as an
audience participant.

Thanks is also due to my friends and colleagues in the former Aravind
research group including Dylan Fossl, Daniel Kopf, Conan Veitch, and
Daniel O'Reilly for their sincerity and support in a confusing loss of
Dr.~Aravind as a supervisor.

And I thank my others friends who inspired or encouraged me during this
thesis including Simon Harris, Amy Jimmo, Catherine Sprangers, Alix
Schebel, and Richard Nhan.

Most of all I thank my family including Aaron Seilis, Pat Seilis, Mark
Seilis, Flo Dew, and Derwyn Dew for their unwavering support. My
grandfather, Derwyn Dew, passed away in January of 2022 due to
complications of esophageal cancer. He is loved and missed.

\section*{Preface to the Second
Edition}\label{preface-to-the-second-edition}
\addcontentsline{toc}{section}{Preface to the Second Edition}

\markright{Preface to the Second Edition}

This second edition was motived by learning about the existence of the
Quarto document preparation system. As an exercise I have migrated my
thesis to an online book format. Even after completing my thesis and
graduating with a masters in Computer Science, I keep busy learning.

I still feel gratitude to all the people who helped me get this far.

Developing this second edition has also given me the opportunity to
address errata in the first edition and add clarifications or additional
information.

\bookmarksetup{startatroot}

\chapter{Abstract}\label{abstract}

Complexity poses a pervasive challenge in understanding formal and
natural systems which can arise from a combination of a system's state
and state transition rules. In situations in which many aspects of a
system are changing together, it is desirable to quantify how much they
do so. We motivate and define five mathematical functions that can be
used to quantify coordinated changes in structure. We also developed
ConAction, a Python package which implements these novel mathematical
tools in a way that is performant, easy to install, and easy to use.
These new tools can be applied to real research problems, which we
exemplified by evaluating a classic isolation by distance model for
\emph{Dendroctonus ponderosae} populations in western North America.

\bookmarksetup{startatroot}

\chapter{Introduction}\label{introduction}

\section{Complexity}\label{complexity}

Defining ``complexity'' can seem ironic because it turns out to be a
`complex' endeavour. But in this section we will attempt to explain some
of its character, and why it is a challenge in the sciences. The
complexity of a system generally comes from a combination of two
sources: the system's state and the system's state transition
rules.\textbackslash{}

Formal systems are structures like sets, sequences, vectors, scalars,
matrices, tensors, graphs, hypergraphs, or any of a myriad of algebraic
structures or spaces. In the simple case of a set as a system, one can
think of the elements of the set as its state along with some rules for
changing the elements of the set. Two contrasting examples of formal
systems are cellular automata and systems of diffrential
equations.\textbackslash{}

Figure \ref{fig:statetransitionexample} (a) shows an elementary
(1-dimensional) cellular automata known as Rule 110. In such a system
the entire state of the system is represented as a one-hot vector
(i.e.~a vector of zeros and ones). In such a system time can be thought
of discretely as a count of the number of applications of the rules,
which are applied simultaneously on the current state of the system to
get the next state. The state transition rules dictate how a given bit
is replaced with another depending on its value as well as the values of
its left and right neighbours. It is possible \emph{prima facie} to
believe that such systems are only capable of the simplest of patterns
based on the belief that simple rules imply simple behaviour. But this
is not so. In 2004 Mathew Cook published a proof that Rule 110 is Turing
complete (Cook (2004)), i.e.~equivalent to a Turing machine, a
classification indicating that Rule 110 can run any program depending on
the size and configuration of its
input.\footnote{A system would have to be infinitely-large to be Turing complete. If Rule 110 elementary automata were given an infinitely-large input this would be apt, however it still speaks to the ability of a system to represent complex patterns if it is a finite approximation to a Turing machine.}
As a practical point of comparison, a phone or a PC can be entirely
represented by a Turing machine. This includes everything from playing
your favourite music or video, playing the latest computer games,
calling a friend, running advanced scientific simulations, and even
representing the entirety of whatever the Internet is doing right at
this moment. Thus complexity really \textbf{can} come from simple rules
with a suitable system configuration.

Stephen Wolfram has argued that some systems, including those with
simple rules and relatively small state spaces, can still be what he
calls ``\textit{computationally irreducible}''(Wolfram (2002)).
Computational irreducibility is the notion that there exists formal
systems in which, even with the current state and the state transition
rules on hand, there does not exist a more efficient way of knowing the
future state than to run the complete sequence of iterations of the
rules. Fortunately, if we consider a well-defined state with state
transitions rules at time \(t\), we can always (within the
practicalities of computation on real computers) compute what the state
will be at time \(t+\tau\) by iteratively applying the transition rules
\(\tau\) number of times. Less fortunately, there are systems that have
undecidable problems in the sense that they cannot be answered in a
finite number of steps.

Discrete (countable) systems are not the only types of formal systems
that can be characterized by a current state combined with state
transition rules, leading us to the notion of differential equations.

\begin{figure}[H]
    \begin{center}
    \begin{tabular}{cc}
    (a) & (b) \\
        \includegraphics[scale=0.2]{rule110.png} & \includegraphics[scale=0.2]{lorenzsystem.png}  \\
        \ccPublicDomain\ Public Domain & \ccPublicDomain\ Public Domain \\
        \small $\{(1,1,1), (1,0,0), (0,0,0)\} \mapsto 0$ & $\frac{dx}{dt} = \sigma (y-x)$ \\
        \small $\{(1,1,0), (1,0,1), (0,1,1), (0,1,0), (0,0,1)\} \mapsto 1$ & $\frac{dy}{dt} =x (\rho - z) - y$ \\
         & $\frac{dz}{dt} = xy - \beta z$ \\
    \end{tabular}
    \end{center}
    \caption{Examples of systems with a well-defined state with state transition rules. (a) Rule 110 elementary cellular automata. (b) Lorenz dynamical system.}
    
\end{figure}

Figure \ref{fig:statetransitionexample} (b) shows the Lorenz system,
which is a system of differential equations in 3 dimensions that was
originally inspired by weather models \cite{Lorenz1963}. A
\emph{dynamical system} has a formal definition, but we will treat it
informally here as a system that evolves with time according to
differential equations. Like the example with elementary cellular
automata, there is a well-defined state at any given point in time. But
unlike these discrete state space systems, the Lorenz system has a
continuous state space. This has the non-intuitive consequence that
there is no ``next state'', however a collection of governing equations
prescribe how the system evolves through the space of states. The Lorenz
system exhibits a different notion of complexity: chaos. Chaos can
informally be defined as the state of the system at some future time
being highly sensitive to the initial conditions that the system began
with, however this may understate the severity of the sensitivity. There
exists systems for which \emph{any} finite precision in the measurements
of the state of the system will eventually be insufficient to predict
future states after a finite amount of time. This is not the same thing
as randomness \emph{per se} because the underlying dynamics are
deterministic. Rather this is extrinsic randomness due to not knowing
the state of the system in enough
detail.\footnote{The only systems that might be intrinsically random are those described by wave functions in quantum mechanics, however even this inference has doubts that have been raised by @Hossenfelder2020a, @Hossenfelder2020b, and @Hooft2014.}

While we often model natural systems using formal systems, they are not
synonymous. Therefore it worth exemplifying some natural systems. We
will describe some natural systems in which complexity makes scientific
progress difficult.

A human brain is an immensely complex organ whose state transitions are
measure dependent with human decision making and behaviour. It contains
roughly 85 billion neuron cells (Azevedo et al. (2009)), and some
believe it has \(\sim 10^{15}\) connections between them (DeWeerdt
(2019)). The brain seems to function via electrical-chemical signals
that travel along and between neurons, but with such large numbers of
components involved it is difficult to determine precisely what any
selection of neurons is actually doing.

There exists some 25 thousand genes in the human genome. While this
number is quite modest compared to the number of neurons in the human
brain, the behaviour of the human genome as a whole is still an ongoing
area of research. Molecular biologists and biochemists have had immense
success in mapping certain genes to biological functions, and it has
helped considerably that genes code for RNA and proteins that can be
studied for structure-related function. Molecular components of gene
regulatory networks work via chemical diffusion. A consequence of
molecules travelling via chemical diffusion is that they go everywhere
within their cellular compartment and sometimes beyond. This is
especially the case when other mechanisms of transport such as channel
protein transport and active transport proteins play a role. It can
still be difficult to predict what a given gene regulatory network will
do in a cell because of unforeseen chemical interactions (Karkaria et
al. (2020)).

What is true of both formal and natural systems is that complexity
arises from a combination of its state at some time and the state
transition rules. Even if a system is deterministic, we may not have
precise enough measurements, there are likely to be missing variables,
and some problems are undecidable. Often our empirical measurements do
not capture the whole state of the system, but rather represent
statistical data about the system.

For example, temperature measurements sample some aggregate quantity of
molecules with varying kinetic
energy.\footnote{In thermodynamics, temperature is formally defined as $T$ in the equality $\frac{1}{T} = \frac{\partial S}{\partial E}$ where $S$ is the system's entropy and $E$ is the system's internal energy. The change in internal energy, $\partial E$, can come in the form of heat. Heat is related to exchange of kinetic energy of particles through random thermal motion.}
Likewise with the brain, scanning technologies such as functional
magnetic resonance imaging and positron emission tomography allow us to
understand aggregate behaviours of neurons in terms of \emph{regions} of
the brain.

This motivates the need for mathematical models to yield insights about
systems for which we do not know how to measure in precise totality.
Indeed, the need for such models may be an indication that we are
dealing with the current limits of our understanding. We will delve into
rigorous thinking on new mathematical models of this type in later
sections, but in the next section we wish to point to something of great
imprecision and importance in our experience of living in a complex
world: the Trinity of Covariation.

\section{The Trinity of Covariation}\label{the-trinity-of-covariation}

In the analysis of data we look for how observables vary on their own,
and how they vary together. Figure \ref{fig:dancing} offers a metaphor
for this by relating dancers to variables. In univariate statistics we
are concerned with how a single variable changes or distributes on its
own. In bivariate statistics we are concerned with how pairs of
variables covary. Likewise, for any number of putatively dependent
variables we can consider how they covary, which is one of the goals of
multivariate statistics.

\begin{figure}[H]
        \begin{center}
            \begin{tabular}{ccc}
            (a) & (b) & (c) \\
            \includegraphics[scale=0.65]{singledancer.jpg} & \includegraphics[scale=0.13]{twodancers.jpeg} & \includegraphics[scale=0.181]{threedancers.jpg} \\
    \tiny \href{https://en.wikipedia.org/wiki/File:Nandini\_Ghosal.jpg}{CC-BY-SA-2.0} & \tiny \href{https://en.wikipedia.org/wiki/File:NutcrackerSnowPas.JPG}{CC-BY-SA-4.0} & \tiny \href{https://en.wikipedia.org/wiki/File:Irish\_dancers\_in\_team\_costume,\_Davis\_Academy,\_USA.jpg}{\ccPublicDomain\ Public Domain} \\
    \tiny  \href{https://www.flickr.com/people/43518209@N00}{Bala Sivakumar} & \tiny \href{https://en.wikipedia.org/wiki/User:Lambtron}{Jim Lamberson $|$ Wikimedia Commons} &  \\
            \end{tabular}
        \end{center}
    \caption{'Dancing variables' is a metaphor relating statistical variables to dancers. (a) A single dancer as a univariate system. (b) Two dancers as a bivariate system. (c) Three dancers as a trivariate system.}
    
\end{figure}

The notion that coordinated change in structures is of interest is older
than modern statistics. For example, the philosohpher John Stuart Mill
spoke of the importance of coordinated change in his thinking on
causality \cite{Mill1843}. This is similar to Reichenbach's common cause
principle which states that if two events \(A\) and \(B\) hold the
relation \(P(A \cap B) > P(A)P(B)\) and yet neither causes the other,
then there exists a common cause \(C\) such that \(A\) and \(B\) are
conditionally independent on \(C\) (Hitchcock and RÃ©dei
(2021)).\footnote{This notion of causality is \textit{prima facie} incomplete, but it can be improved by considering the two sources of complexity we mentioned earlier in this chapter: the configuration of the state of the system and the state transition rules. Suppose there exists a universe similar to ours in its rules, but began at $t=0$ with only two electrons positioned billions of light years away from each other. Because electromagnetic and gravitational effects propogate at the speed of light, there would be no physical interaction between these particles for a long time. Now suppose that the particles' velocities are exactly in the same direction, and will remain constant in the absence of external forces according to Newton's laws. This entails that their positions as a function of time are perfectly positively correlated depsite the absence physical interaction. What precisely is causal about such a circumstance? Consider that we do not get hit the face with base balls just because the laws of physics are what they are. Rather we (sometimes) get hit in the face with base balls because the laws of physics are what they are \textbf{and} the (rather unfortunate) initial conditions and boundary conditions are what they are. In terms of the original pair of electrons in an empty universe, the initial conditions that we supposed are also the cause of the resulting correlation.}

\begin{figure}

\begin{minipage}{0.50\linewidth}

\includegraphics{index_files/mediabag/Stuart_Mill_G_F_Watt.jpg}

\subcaption{\label{}John Stuart Mill (Public Domain)}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\begin{quote}
\textbf{Quotation} (Mill (1843))

Whatever phenomenon varies in any manner whenever another phenomenon
varies in some particular manner, is either a cause or an effect of that
phenomenon, or is connected with it through some fact of causation.
\end{quote}

\end{minipage}%

\caption{\label{fig-john-stuart-mill}Portrait of John Stuart Mill and a
quote from his work on causal inference and reasoning.}

\end{figure}%

Perhaps the notion of coordinated change in structures is even older
than recorded history. Humans and other animals are capable of learning
via operant conditioning, which involves an association of a
\textit{stimulus} (i.e.~a change) with a reward/punishment (i.e.~another
change) (Houwer (2018)). While learning this way is very general, it can
lead to false pattern recognition in the form of apophenia including
pareidolia, and agenticity. This establishes a precedent of coordinated
changes in structures being an old, if not ancient, notion.

A metaphor can be based on this possibly-ancient notion which can be
used as a muse for developing more precise mathematical and software
tools. But first we would like to highlight both the challenges and
potential of using metaphors as a starting point for idea generation.

A quote that nicely captures one of the concerns of metaphor-driven idea
generation was stated by Eric Weinstein speaking to Roger Penrose on the
mathematical foundations of modern physics
\cite{Weinstein2020}.\textbackslash{}

\begin{figure}

\begin{tcolorbox}[title={Quotation}]
\textit{"[M]athematics makes you pay for every attempt to [...] intuitively encode something that isn't precise."}

\hspace*{\fill}{- Dr. Eric Weinstein to Sir Roger Penrose, \textit{The Portal} \cite{Weinstein2020}.}
\end{tcolorbox}

\caption{Quote by Dr. Eric Weinstein to Sir Roger Penrose about the difficulties in turning imprecise notions into precise mathematics.}

\end{figure}

One of the ways in which this quote captures the challenge of using
metaphors to inspire mathematical ideas is that metaphors are often
ambiguous. They often have multiple interpretations, thus making any
persuits of a unique instantiation of a metaphor to be na"ive. However,
this property can be used as a
\texttt{feature\textquotesingle{}\ rather\ than\ a}bug' of idea
generation. Based on Figure \ref{fig:exampleinstantiations}, consider
that numerous imprecise notions such as
``probability''\footnote{Probability is precise in the sense of Kolmogorov's axioms of probability, based on measure theory. What remains ambiguous is what such mathematical measures mean in terms of empirical data, which has led to frequentist and Bayesian interpretations of these quantities.},
``network centrality'', ``central tendency'', and ``change'' have
multiple instantiations. In the case of network centrality measures
there are far more of them than would fit in Figure
\ref{fig:exampleinstantiations}. Rather than being concerned about
finding a multiplicity of interpretations, we suggest exploiting them
for different use cases.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.45]{muse_top.pdf} \\
        \includegraphics[scale=0.42]{muse_bottom.pdf}
    \end{center}
    \caption{Examples of notions that are not uniquely encoded into mathematical formalisms: probability, network centrality, central tendency (of a distribution), and change.}
    \label{fig:exampleinstantiations}
\end{figure}

Motivated by the historical existence of a notion of coordinated change
in structures, and expecting some multiplicity of interpretations, here
is a simple metaphor: The Trinity of Covariation (Figure
\ref{fig:trinityofcovariation}).

\begin{figure}[H]
    \begin{center}
        \begin{tikzpicture}[grow cyclic, every node/.style=concept,  concept color=UNBCGold, text width=2.1cm, text=white, align=flush center,
    level 1/.style={level distance=3cm,sibling angle=120, concept color=UNBCGreen},
    level 2/.style={level distance=0.5cm,sibling angle=45}]

            \node{Covariation}
                child {node {Change}}
                child {node {Structure}}
                child {node {Coordination}}
                ;
        \end{tikzpicture}
    \end{center}
    \caption{\textit{The Trinity of Covariation} is a metaphor for creatively exploring our intuitive notions of "\textit{covariation}" as "\textit{coordinated changes in structure}".}
    \label{fig:trinityofcovariation}
\end{figure}

Reflecting on the metaphor, ``covariation'' as ``coordinated changes in
structure'' can be used as a tool for creative thinking. In relation to
this metaphor, the following questions can be considered when doing
open-ended idea generation:

\begin{itemize}
    \item In what sense might something be thought of as 'coordinated'?
    \item In what sense might something be thought of as a 'structure'?
    \item In what sense might something be thought of as 'change'?
\end{itemize}

In this chapter a frame of thinking was introduced in the form of a
metaphor, and from it we wish to define more precise notions that help
us understand complex systems. The coming chapters discuss both new and
existing instances of this metaphor.

\bookmarksetup{startatroot}

\chapter{Previous Instantiations of the Trinity of
Covariation}\label{previous-instantiations-of-the-trinity-of-covariation}

Instantiations of the Trinity of Covariation are already ubiquitous in
mathematics and the sciences. This chapter provides discussion of some
of these instantiations from different areas of mathematics found in the
sciences.

\section{Correlation}\label{correlation}

``\emph{Correlation}'' is an instance of The Trinity of Covariation. It
quantifies `change' in the form of deviations from a mean value of a
random variable. It captures a notion of `structure' in the sense of
quantifying something about the joint distribution of two variables. And
it captures a notion of `coordination' in a direct sense of paired
values (i.e.~points or \emph{coordinates}) whose instances can be
specified by an index set.

Francis Galton, one of the important thinkers in the history of
correlation, made a statement about his calculations of `co-relation' of
`variable organs' (given in Figure~\ref{fig-galton-quote} with his
portrait) that echoes the quote on causality from John Stuart Mill given
in Section \ref{sec:trinityofcovariation}. Here, Galton is literally
referring to biological and physical measurements of organs of the human
body Galton (1888).

\begin{figure}

\begin{minipage}{0.50\linewidth}

\includegraphics{index_files/mediabag/Francis_Galton_1850s.jpg}

\subcaption{\label{}Francis Galton (Public Domain)}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\begin{quote}
Two variable organs are said to be co-related when the variation of the
one is accompanied on the average by more or less of the other, and in
the same direction.
\end{quote}

\end{minipage}%

\caption{\label{fig-galton-quote}Portrait of Francis Galton and a quote
from his work on biometrics.}

\end{figure}%

In this quotation Galton says, ``and in the same direction''. While most
students of statistics today understand the notion of negative
correlation, it may have been the case that the properties that Galton
studied all had positive correlation. Perhaps he did not notice the
possibility of negative correlation. But it is clear from his wording of
`variation of the one is accompanied on the average by more or less of
the other' that he is thinking of coordinated change in empirical
quantities.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\includegraphics{index_files/mediabag/Bravais2.gif}

\subcaption{\label{}Auguste Bravais (Public Domain)}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\includegraphics{index_files/mediabag/Sir_Francis_Galton_b.jpg}

\subcaption{\label{}Francis Galton (Public Domain)}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\includegraphics{index_files/mediabag/Karl_Pearson,_1912.jpg}

\subcaption{\label{}Karl Pearson (Public Domain)}
\end{minipage}%

\caption{\label{fig-correlation-figures}The historical founders of the
intuitive and mathematical notions that led to our modern formulations
of correlation.}

\end{figure}%

The history of correlation is complicated, with many participants that
will not be mentioned here. See Figure~\ref{fig-correlation-figures} for
some of them. For a more detailed discussion, see Piovani (2007).
Correlation began with the study of error theory which endevoured to
form a mathematical description of the error of models. This theory of
errors led to least-squares analysis, which was pioneered by the rivals
Carl Friedrich Gauss and Adrien-Marie Legendre Piovani (2007). While
generations of undergraduate students have associated correlation with
Karl Pearson, he neither developed the first mathematical formulation
nor the interpretation of `co-relation' of variables. Auguste Bravais
purportedly was the first to write down a mathematically-equivalent
expression to Pearson's correlation coefficient Wright (1921), however
he did not provide any interpretation of statistical association to his
calculations. Rather Bravais was focused on developing joint normal
distributions, and what we would now consider to be correlation was only
discussed in his work as an angle between residual vectors Bravais
(1844). It was Francis Galton that found an intuitive interpretation in
his own tabular way of computing correlation as `co-relation'
Stigler1989, and it was Karl Pearson (Pearson (1895)) who later
formalized the product-moment formula that students would recognize
today.

\begin{equation}
\text{Corr}\left[ X, Y\right] \triangleq \frac{\mathbb{E}\left[ \left( X - \mathbb{E}\left[ X \right] \right) \left( Y - \mathbb{E}\left[ Y \right] \right) \right]}{\sqrt{\mathbb{E}\left[\left( X - \mathbb{E}\left[ X \right] \right)^2 \right] \mathbb{E}\left[\left( Y - \mathbb{E}\left[ Y \right] \right)^2 \right]}}
\end{equation}

The \(\mathbb{E}\) operator in the above expression denotes the
expectation operator, which can be defined as
\[\mathbb{E}[X] \triangleq \int_{\Omega} x dF\]

where \(X\) is a random variable, \(\Omega\) is the set of outcomes for
\(X\), and \(F\) is the cumulative distribution function of
\(F\).\footnote{It is possible to define the expectation operator in a more general way, but it will suffice for our purposes.}
In a sense the expectation operator can be thought of as a kind of
weighted average. We will make use of the notion of the expectation
multiple times throughout the thesis.

The Pearson correlation coefficient is one of the most used functions in
applied statistics, and its numerator
\(\operatorname{Cov}[X,Y] \triangleq \mathbb{E}\left[ \left( X - \mathbb{E}\left[ X \right] \right) \left( Y - \mathbb{E}\left[ Y \right] \right) \right]\)
is the \textit{covariance} of two random variables in the conventional
sense. Table~\ref{tbl-pearson-properties} lists some of the basic
properties of Pearson's correlation coefficient, along with the
less-well-known result due Langford, Schwertman, and Owens (2001) that
only very strong correlations can be used in a transitive-like way.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{Properties of Pearson's Product-Moment Correlation
Coefficient}\label{tbl-pearson-properties}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Expression
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Comments
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Expression
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Comments
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Translational invariance &
\(\text{Corr}\left[ X, Y\right] = \text{Corr}\left[ X + c, Y\right]\) &
\(c \in \mathbb{R}\) \\
Absolute Invariance of Scaling &
\(\text{Corr}\left[ X, Y\right] = \text{Corr}\left[ \alpha X, Y\right]\)
& \(\alpha \in \mathbb{R}_{>0}\) \\
Symmetric &
\(\text{Corr}\left[ X, Y\right] = \text{Corr}\left[ Y, X\right]\) & \\
Bounded & \(-1 \leq \text{Corr}\left[ X, Y\right]  \leq 1\) & \\
Dependence Indicator &
\(\perp\!\!\!\!\perp (X, Y) \implies \text{Corr}\left[ X, Y\right] = 0\)
& The converse is not true. \\
Conditional Transitivity &
\(\text{Corr}\left[ X, Y\right]^2 + \text{Corr}\left[ Y,Z\right]^2 > 1 \implies \text{Corr}\left[ X, Z\right]^2 > 0\)
& Langford, Schwertman, and Owens (2001) \\
Even Function &
\(\text{Corr}\left[ X, Y\right] = \text{Corr}\left[ -X, -Y\right]\) & \\
Biased & \(\mathbb{E}_{X|\rho}[\hat{\rho} - \rho] \neq 0\) & Hotelling
(1953), Olkin and Pratt (1958) \\
\end{longtable}

\begin{tcolorbox}[enhanced jigsaw, colbacktitle=quarto-callout-caution-color!10!white, bottomrule=.15mm, left=2mm, arc=.35mm, bottomtitle=1mm, coltitle=black, breakable, rightrule=.15mm, toptitle=1mm, opacityback=0, titlerule=0mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Some Clarifications About Bias}, colframe=quarto-callout-caution-color-frame, toprule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, colback=white]

Above we state that correlation is biased, however this has only been
shown to be the case for certain families of probability distributions.

Further, it is important to distinguish between the estimand as the
population correlation \(\rho\), the estimate \(\hat \rho\), and the
estimator which assigns an estimate to each sample.

\end{tcolorbox}

The notation \(\perp\!\!\!\!\perp (X, Y)\) in
Table~\ref{tbl-pearson-properties} denotes that two random variables are
statistically independent. \emph{Statistical independence} is a property
of a collection of random variables such that their joint cumulative
distribution function is equal to the product of their marginal
cumulative distribution functions. The notion of statistical
independence will become important in interpreting specific mathematical
topics later in this thesis.\textbackslash{}

Another important statistical notion is that of the bias of an
estimator. An estimator is function that can be computed on a sample to
estimate the value of a population parameter. The notation
\(\mathbb{E}_{X|\rho}[\hat{\rho} - \rho] \neq 0\) in
Table~\ref{tbl-pearson-properties} denotes that the average difference
across samples between an estimate \(\hat{\rho}\) of the parameter
population \(\rho\) will not equal zero.\textbackslash{}

There exists a generalization of Pearson's correlation coefficient that
is also an instantiation of The Trinity of Covariation. It is discussed
later in this thesis.

\section{Cokurtosis, Coskewness, and Other Standardized Mixed-Product
Moments}\label{cokurtosis-coskewness-and-other-standardized-mixed-product-moments}

Pearson's product-moment correlation is one of the more famous and
historically precedented examples of standardized mixed-product moments.
Such functions have the form given in Definition
\ref{def:standardmixedproductmoment}, which can be shown to be a
generalization of Definition
\ref{def:pearsoncorrelation}.\textbackslash{}

\begin{Definition}[mydefinition=Standardized Mixed-Product Moment, label=def:standardmixedproductmoment]
Given a collection of real-valued random variables $\{X_1, \cdots, X_n \}$, their \underline{standardized mixed-product moment} is given by:

$$K \left[X_1, \cdots, X_n \right] \triangleq \frac{\mathbb{E} \left[ \prod_{j=1}^{n} \left( X_j - \mathbb{E} \left[ X_j \right] \right) \right]}{\prod_{j=1}^{n} \sqrt{\mathbb{E} \left[ \left (X_j - \mathbb{E} \left[ X_j \right] \right)^2 \right]}}$$
\end{Definition}

When \(n=3\) in Definition \ref{def:standardmixedproductmoment} the
statistic is called the \textit{coskewness} (Miller (2013)) of those
random variables, while if \(n=4\) the statistic is called the
\textit{cokurtosis} (Miller (2013)).

This family of statistics are instances of the Trinity of Covariation in
a very similar way in which the Pearson product-moment correlation is
through computing expectations of products of deviations from their
univariate expectations. One of the new instances presented in this work
is similar to these standardized mixed-product moments but involves a
different normalization.

\section{Interaction Effects}\label{interaction-effects}

Interaction effects are in common usage in the context of linear models.
They were motivated by the non-additivity of treatment effects
\cite{Cox1984}. In the context of linear models an interaction effect is
taken to be the linear regression coefficient of a term called an
\textit{interaction term}. An interaction term is a predictor defined by
the product of a collection of random variables.

Let us consider an example to clarify the concept. Let \(X,Y,Z\) be
random variables defined in a linear model to be
\(Y = \beta_0 + \beta_1 X + \beta_2 Z + \beta_3 X Z\). Here \(\beta_0\)
is an intercept term, and \(\beta_1\) and \(\beta_2\) represent the main
effects of \(X\) and \(Z\) on \(Y\), respectively. The interaction term
is \(\beta_3 XZ\), and the interaction effect is the coefficient
\(\beta_3\). In other words, an interaction effect is a parameter that
specifies how \(X\) and \(Z\) have a coordinated effect on \(Y\).

Interaction effects are not limited to pairs of predictor variables, and
different interaction effects can be included in the same model where
their distinctiveness is obtained from the choice of variables
`interacting'.

\section{Derivatives and Differential
Operators}\label{derivatives-and-differential-operators}

Perhaps no subject has so thoroughly studied change as that of calculus,
and so it is of little surprise that instances of the Trinity of
Covariation can be found within it.

The derivative is a central concept to the subject of calculus, and is
an instance of the Trinity of Covariation. It describes how one quantity
changes whenever a second quantity changes. Not only are these changes
coordinated, but are by definition an instantaneous comparison.

In any introductory course in differential calculus, related rates
problems are presented to the student. It is sometimes a student's first
introduction to applications of both calculus and the chain rule of
derivatives. A classic example is that of the leaning ladder, which
relates certain rates of change through the Pythagorean theorem. In
terms of the metaphor, the structure is the collection of coordinates
describing the bottom and top of the ladder with respect to the ground
and the wall. The notion of change here is simply the derivatives, and
the notion of coordination here comes from the Pythagorean theorem. Via
the chain rule of derivatives, we can consider the total derivatives of
the quantities of interest.

Beyond the derivative of scalar functions are a variety of differential
operators that capture collections of rates of change in a coordinated
way. A function with a scalar image might also have a gradient, which
describes the partial derivatives with respect to each input. The
gradient is a staple of vector calculus, and modern machine learning.
The Jacobian matrix is a construction of how one set of scalars change
with respect to another set of scalars, which can provide a description
of how when a change in coordinate system is performed it can be
transformed into a change in another coordinate system. Another operator
is the Hessian, which considers all of the second-order partial
derivatives of a scalar-imaged function and whose eigenvalue
decomposition can inform us of the convexity of a surface. These and
many other differential operators extend the applicability and scope of
derivatives by considering how comparisons of multiple quantities change
together.

\bookmarksetup{startatroot}

\chapter{New Instantiations of The Trinity of
Covariation}\label{new-instantiations-of-the-trinity-of-covariation}

This chapter introduces new instantiations of the metaphor of \emph{The
Trinity of Covariation}.

\section{Multilinear Correlation}\label{multilinear-correlation}

\subsection{Derivations}\label{derivations}

The covariance of two random variables is conventionally defined as
their centered mixed product moment. Just as the covariance in the
standard sense captures something about simultaneous changes in two
random variables, we explored generalizing this notion to multiple
variables beyond two. We decided that a natural way to accomplish this
was to compute the \emph{centered mixed product moment} of a collection
of random variables. To emphasize in this work that we are focusing on a
generalization of covariance, which is bilinear, we will refer to this
generalization as \emph{multilinear covariance} as given in the
following definition.

\begin{quote}
\textbf{Definition}

Given a finite collection of real-valued random variables
\(\{X_1, \cdots, X_n\}\), their \textbf{multilinear covariance} is
defined to be the following.
\[\text{Cov}\left[ X_1, \cdots, X_n \right] \triangleq \mathbb{E}\left[ \prod_{j=1}^{n} \left(X_j - \mathbb{E}[X_j] \right) \right]\]
\end{quote}

This definition in standard literature bears names such as
\emph{centered mixed product moment} and \emph{centered cross-product
moment}, which are both comparably verbose to \emph{multilinear
covariance}. The qualifier \emph{cross-product} can confuse readers into
considering vector cross-products if clarifications are not given, while
the former is technically accurate but communicates little intuition
about what is being quantified.

Bilinearity is a property of covariance in the standard sense, and we
wished to have multilinearity in the generalized sense. However, just
because we gave a function the adjective ``multilinear'' does not make
it multilinear in the desired sense of a multilinear map. See Definition
\ref{def:multilinearmap} for a definition of \emph{multilinear} as given
by Greub (1978).

\begin{quote}
\textbf{Definition} (Greub (1978))

Let \(E_1, \cdots, E_{p}\) be a collection of vector spaces, and also
let \(G\) be a vector space. A map
\(\phi: E_1 \times \cdots \times E_p \mapsto G\) is called
\textbf{\(p\)-linear} if for every \(i \in \{1, \cdots, p \}\) and
scalars \(\alpha\) and \(\beta\) it holds that \begin{align*}
\phi (x_1, \cdots, x_{i-1}, \alpha x_i + \beta y_i, x_{i+1}, \cdots, x_p) &= \alpha \phi (x_1, \cdots, x_{i-1}, x_i, x_{i+1}, \cdots, x_p) \\
&  + \beta \phi (x_1, \cdots, x_{i-1}, y_i, x_{i+1}, \cdots, x_p).
\end{align*}
\end{quote}

In the following proposition we claim and prove that multilinear
covariance is multilinear in a mathematical sense. We took an approach
of showing that the multilinear covariance of linear combinations of
random variables is itself a linear combination of the multilinear
covariances on those random variables.

\begin{tcolorbox}[enhanced jigsaw, colbacktitle=quarto-callout-caution-color!10!white, bottomrule=.15mm, left=2mm, arc=.35mm, bottomtitle=1mm, coltitle=black, breakable, rightrule=.15mm, toptitle=1mm, opacityback=0, titlerule=0mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Screen Size}, colframe=quarto-callout-caution-color-frame, toprule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, colback=white]

You may need to view the following result on a larger screen to see it
in its entirety.

\end{tcolorbox}

\begin{quote}
\textbf{Proposition}

\[\text{Cov}\left[ \sum_{w_{1}=1}^{K_{1}} a_{w_{1}} U_{w_{1}}, \cdots, \sum_{w_{j}=1}^{K_{j}} a_{w_{j}} U_{w_{j}}, \cdots,  \sum_{w_{n}=1}^{K_{n}} a_{w_{n}} U_{w_{n}} \right]\]

\[=\]

\[\sum_{w_1=1}^{K_1} \cdots \sum_{w_j=1}^{K_j} \cdots \sum_{w_n=1}^{K_n} \left(\prod_{j=1}^{n} a_{w_j} \right) \text{Cov}\left[U_{w_1}, \cdots, U_{w_j}, \cdots, U_{w_n} \right]\]

\textbf{Proof}

Starting with the definition of multilinear covariance, let
\(X_{j} = \sum_{w_{j}}^{K_{j}} a_{w_j}U_{w_j}\) for each
\(j \in \{1, \cdots, n \}\). \begin{align*}
\text{Cov} \left[ \sum_{w_1=1}^{K_1} a_{w_1}U_{w_1}, \cdots, \sum_{w_{j}=1}^{K_{j}} a_{w_{j}} U_{w_{j}}, \cdots,  \sum_{w_n=1}^{K_n} a_{w_n}U_{w_n}\right] &= \mathbb{E}\left[ \prod_{j=1}^{n} \left( \sum_{w_{j}=1}^{K_{j}} a_{w_{j}} U_{w_{j}} - \mathbb{E}\left[ \sum_{w_{j}=1}^{K_{j}} a_{w_{j}} U_{w_{j}} \right]  \right) \right] \\
& = \mathbb{E}\left[ \prod_{j=1}^{n} \left( \sum_{w_{j}=1}^{K_{j}} a_{w_{j}} U_{w_{j}} - \sum_{w_{j}=1}^{K_{j}} \mathbb{E}\left[ a_{w_{j}} U_{w_{j}} \right]  \right) \right] \\
& = \mathbb{E} \left[ \sum_{w_1}^{K_1} \cdots \sum_{w_j}^{K_j} \cdots \sum_{w_n}^{K_n} \prod_{j=1}^{n} \left( a_{w_j}U_{w_j} - \mathbb{E}\left[ a_{w_j}U_{w_j} \right] \right) \right] \\
& = \sum_{w_1}^{K_1} \cdots \sum_{w_j}^{K_j} \cdots \sum_{w_n}^{K_n} \mathbb{E} \left[ \prod_{j=1}^{n} \left( a_{w_j}U_{w_j} - \mathbb{E}\left[ a_{w_j}U_{w_j} \right] \right) \right] \\
& = \sum_{w_1}^{K_1} \cdots \sum_{w_j}^{K_j} \cdots \sum_{w_n}^{K_n} \mathbb{E} \left[ \prod_{j=1}^{n} a_{w_j} \left( U_{w_j} - \mathbb{E}\left[ U_{w_j} \right] \right) \right] \\
& = \sum_{w_1}^{K_1} \cdots \sum_{w_j}^{K_j} \cdots \sum_{w_n}^{K_n} \left( \prod_{j=1}^{n} a_{w_j} \right) \mathbb{E} \left[ \prod_{j=1}^{n} \left( U_{w_j} - \mathbb{E}\left[ U_{w_j} \right] \right) \right] \\
& = \sum_{w_1}^{K_1} \cdots \sum_{w_j}^{K_j} \cdots \sum_{w_n}^{K_n} \left( \prod_{j=1}^{n} a_{w_j} \right) \text{Cov} \left[ U_{w_1}, \cdots,  U_{w_j}, \cdots, U_{w_n} \right]
\end{align*} \(\blacksquare\)
\end{quote}

Multilinear covariance generalizes covariance from a bilinear comparison
of two random variables to a multilinear comparison of finitely-many
random variables. This generalizes the numerator of Pearson's
product-moment correlation coefficient, which leaves finding a
generalization of its denominator. Knowing that the denominator of
Pearson's correlation is the product of the standard deviations of the
respective two variables compared in the numerator, such a
generalization of the denominator should reduce to the product of the
standard deviations in the case of only two variables. Another
particularly useful property of Pearson's correlation is its
normalization onto the interval \([-1,1]\) using the Cauchy-Schwartz
inequality. Combining these properties into a goal, we desired to find
an inequality that generalizes the Cauchy-Schwartz inequality in such a
way that normalizes the multilinear covariance onto \([-1,1]\).

Nantomah (2017) provides such a generalization of the Cauchy-Schwarz
inequality in the form of the \emph{generalized HÃ¶lder's inequality for
sums} (see proposition below). They accomplish this by generalizing the
absolute value of the inner product of two vectors to the sum-aggregated
element-wise product of a collection of vectors, and by generalizing the
product of 2-norms to the product of \(p\)-norms under specific
constraints. The constraints on the orders of the norms are that
individually they are strictly greater than unity, and that the sum of
their reciprocals equals unity.

\begin{quote}
\textbf{Proposition} (Nantomah (2017))

Given \(Q_{i,j} \in \mathbb{R}\ \forall i,j\) where
\(i \in \{ 1, \cdots, m \}\) and \(j \in \{ 1, \cdots, n \}\) then

\[\sum_{i=1}^{m} \left|\prod_{j=1}^{n} Q_{i,j}\right| \leq \prod_{j=1}^{n} \left( \sum_{i=1}^{m} |Q_{i,j}|^{\alpha_j} \right)^{\frac{1}{\alpha_j}}\]

provided that \(\alpha_j > 1 \forall j\) and that
\(\sum_{j=1}^{n} \frac{1}{\alpha_j} = 1\).
\end{quote}

Before returning to the multilinear covariance, let us consider that a
new correlation coefficient can be defined from the generalized
H"older's inequality for sums by defining a quotient of the
left-hand-side of the inequality divided by the right-hand-side. This
gives a function bounded to \([0,1]\). Taking such a ratio requires that
none of the data vectors are the zero vector. Then by dropping the
absolute value in the numerator we obtain a function bounded to
\([-1,1]\). Lastly, under suitable assumptions such as integrability and
the existence of a
moments\footnote{The existence of moments here also entails the existence of a suitable probability function.},
we write this function using expectations as given in Definition
\ref{def:holdercorrelation}.

\begin{quote}
\textbf{Definition}

Given a finite collection of real-valued random variables
\(\{X_1, \cdots, X_n\}\), and \(\alpha_j \in \mathbb{R}_{>1}\), their
\textbf{HÃ¶lder's correlation coefficient} is given by

\[\mathfrak{H} \left[ X_1, \cdots, X_n \right](\vec \alpha) \triangleq \frac{\mathbb{E} \left[ \prod_{j=1}^{n} X_j \right]}{\prod_{j=1}^{n} \sqrt[\alpha_j]{\mathbb{E} \left[ |X_j|^{\alpha_j} \right]}}\]

provided that \(\sum_{j=1}^{n} \frac{1}{\alpha_j} = 1\) and that the
desired moments exist.
\end{quote}

While H"older's correlation coefficient as given in Definition
\ref{def:holdercorrelation} can be estimated from data, its primary use
in this thesis is to provide a definition from which other more familiar
correlation coefficients can be generalized. We begin by generalizing
the reflective correlation coefficient from the bilinear case to the
multilinear case.\textbackslash{}

The reflective correlation
coefficient\footnote{We did not find a citable source for the origin of the reflective correlation coefficient. While some studies refer to it without citing its origin (e.g. \cite{Zhu2014} and \cite{Tllse2021}), the earliest mentioning of it we found was on Wikipedia \textit{circa} 2009.}
is the same in mathematical form as the Pearson product-moment
correlation coefficient except that only uncentered moments are used. By
setting \(\alpha_j = n\) for each \(j \in \{1, \cdots, n \}\) we obtain
a special case of Definition \ref{def:holdercorrelation} which is also a
multilinear generalizaton of the reflective correlation coefficient
(Definition \ref{def:reflectivemultilinearcorrelation}). This can be
easily verified by setting \(n=2\) and comparing to the reflective
correlation coefficient, and it also satisfies the constraints set on
\(\alpha_j\) in Proposition \ref{thm:generalizedholdersinequalitysums}.

\begin{quote}
\textbf{Definition}

Given a finite collection of real-valued random variables
\(\{X_1, \cdots, X_n\}\), and \(\alpha_j \in \mathbb{R}_{>1}\), their
\textbf{multilinear reflective correlation coefficient} is given by

\[R_{\text{reflective}} \left[ X_1, \cdots, X_n \right] \triangleq \frac{\mathbb{E} \left[ \prod_{j=1}^{n} X_j \right]}{\prod_{j=1}^{n} \sqrt[n]{\mathbb{E} \left[ |X_j|^{n} \right]}}\]

provided that the desired moments exist.
\end{quote}

Before generalizing the Pearson correlation coefficient, we introduce a
definition that generalizes the standard deviation. Just as the standard
deviation can be thought of as the 2-norm of the errors from the mean,
we consider the \(p\)-norm of the errors from the mean to be the
\textit{Nightingale deviation of order $p$} as given in Definition
\ref{def:minkowskideviation}.\footnote{We named this generalization of the standard deviation after Florence Nightingale who pioneered modern nursing and certain approaches to data visualization.}
The Nightingale deviation of order \(p\) is a special case of the the
power mean (sometimes called the generalized mean) (Sykora (2009)), but
it first centers the random variables and takes an absolute
value.\textbackslash{}

\begin{quote}
\textbf{Definition}

Given a real-valued random variable \(X\) whose \(p\)th moment is
defined, its \textbf{Nightingale deviation of order \(p\)} is given by
\[\text{Dev}_p[X] \triangleq \sqrt[p]{\mathbb{E}\left[\left|X - \mathbb{E}\left[ X \right]\right|^p\right]}.\]
\end{quote}

With the multilinear reflective correlation coefficient as given in
Definition \ref{def:reflectivemultilinearcorrelation}, a generalization
of the Pearson correlation coefficient is obtained by centering the
random variables by their expectation before computing the multilinear
reflective correlation. The result is Definition
\ref{def:pearsonmultilinearcorrelation} which is abbreviated using the
definitions of multilinear covariance (Definition
\ref{def:multilinearcovariance}) and Nightingale deviations of order
\(p\) (Definition \ref{def:minkowskideviation}).

\begin{quote}
\textbf{Definition}

Given a finite collection of real-valued random variables
\(\{X_1, \cdots, X_n\}\), their \textbf{multilinear Pearson's
correlation coefficient} is defined to be

\[\text{Corr}\left[ X_1, \cdots, X_n \right] \triangleq \frac{\text{Cov} \left[ X_1, \cdots, X_n \right]}{\prod_{j=1}^{n} \text{Dev}_n[X_j]}.\]
\end{quote}

As exemplified above, making substitutions for the random variables in
H"older's correlation coefficient allows for an easy process of deriving
new correlation coefficients. For example, substituting
\(X_j = \sin (U_j)\) produces a generalization of the
\textit{circular correlation coefficient} used in circular statistics.
Similarly, substituting \(X_j = |U_j|\) obtains an
\textit{absolute correlation coefficient}. There is a general reason why
we should take an interest in multiple notions of correlation which we
consider in the next subsection: detecting statistical dependence.

\section{Interpretation}\label{interpretation}

From Athreya and Lahiri (2006) we can consider a proposition which
logically connects the notion of expectations of products of functions
of random variables to the notion of statistical independence. If we
choose a function for which this equality does not hold, then we have
found that those variables exhibit statistical dependence. Statistical
dependence entails that some events are happening more (or less)
together than we would expect compared to the univariate probabilities
of those events. Imprecisely, statistical dependence suggests to us that
\emph{something is going on}.

\begin{tcolorbox}[enhanced jigsaw, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, left=2mm, arc=.35mm, bottomtitle=1mm, coltitle=black, breakable, rightrule=.15mm, toptitle=1mm, opacityback=0, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, toprule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, colback=white]

Formally, Athreya and Lahiri (2006) restrict their result to Borel
measurable functions. This restriction can be easily met in real
applications of statistics so we do not discuss it further here, but in
theory there exists non-Borel measurable functions.

\end{tcolorbox}

\begin{quote}
\textbf{Proposition} (Athreya and Lahiri (2006))

Let \((\Omega, \mathcal{F}, P)\) be a probability space and let
\(\{ X_1, \cdots, X_n \}\), \(2 \leq n < \infty\) be a collection of
random variables on \((\Omega, \mathcal{F}, P)\). Then
\(\{ X_1, \cdots, X_n \}\) are independent if-and-only-if

\[\mathbb{E}\left[ \prod_{j=1}^n f_j(X_j) \right] = \prod_{j=1}^n \mathbb{E} \left[ f_j(X_j)  \right]\]

for all bounded Borel measurable functions
\(f_j : \mathbb{R} \mapsto \mathbb{R}\),
\(\forall j \in \{1, \cdots, n \}\).
\end{quote}

This proposition provides a mathematically precise way of considering
multilinear correlation coefficients, and now we turn to building some
intuitions about the type of dependence that is described by multilinear
correlations. This cannot be done for all choices of functions of random
variables, but we will consider the multilinear Pearson correlation as a
tractable and instructive example.

Since the multilinear Pearson correlation is defined on finitely-many
variables, it can be difficult to understand what it describes when the
number of variables is greater than three. In order to build intuition
about what it describes in higher dimensions, let us first consider what
it describes in two dimensions. Figure \ref{fig:quadrantcorr}
illustrates how points contribute weight to a Pearson correlation
depending on how it is oriented about the centroid of the
data.\textbackslash{}

Figure \ref{fig:quadrantcorr}(a) illustrates how even when two
independent-and-identically distributed standard normal variables are
sampled, there will almost certainly be at least a sleight inbalance
between the positive and negative weights contributed. Panels (b) and
(c) of Figure \ref{fig:quadrantcorr} emphasize the fact that under
perfect negative or positive correlation the points can be reliably
found in specific quadrants. Figure \ref{fig:quadrantcorr}(d) shows how
non-linear relationships between the variables does not mean that there
will be zero correlation. A salient point to take away from all of these
panels is that the weight that a point contributes to a Pearson
correlation coefficient is equal to the signed area of the rectangle
that can be drawn between the centroid and the data point. For trilinear
Pearson correlation coefficients this is replaced with the signed volume
of a 3-dimensional box, and in the multilinear case for more than 3
variables this signed area is replaced with the signed hypervolume of an
\(n\)-orthotope (i.e.~an n-dimensional box). The magnitude of these
signed measures gets scaled down due to the normalization factor
constituted by the product of the \(p\)-norms as discussed earlier in
this section.

\begin{figure}[H]
    \centering
    \begin{tabular}{cc}
    (a) & (b) \\
    \includegraphics[scale=0.45]{./quadrant_cov_visual/zero_corr.pdf} & \includegraphics[scale=0.45]{./quadrant_cov_visual/neg_corr.pdf} \\
    (c) & (d) \\
    \includegraphics[scale=0.45]{./quadrant_cov_visual/pos_corr.pdf} & \includegraphics[scale=0.45]{./quadrant_cov_visual/cubed_corr.pdf} \\
    \end{tabular}
    \caption{Visualization of the weights contributed to bilinear covariance and correlation about the mean. The magenta-coloured \texttt{x} is the position of the sample centroid, however all panels used a randomly-selected mean vector $\vec{\mu} \sim \mathcal{N} (0, 1)$. Rectangles are drawn from the sample centroid to each sample point, coloured red or blue depending on whether they contribute positive or negatively weight to the sample covariance respectively. (a) A sample from a bivariate normal distribution with covariance matrix equal to the identity matrix. (b) A sample from a bivariate normal distribution with perfectly negative correlation. (c) A sample from a bivariate normal distribution with perfectly positive correlation. (d) A sample from $(X,Y)$ where $X \sim \mathcal{N} (0, 1)$ and $Y \leftarrow X^3 + \epsilon$ where $\epsilon \sim \mathcal{N} (0, 1)$.}
    
\end{figure}

The sign of the weight that a point contributes to a multilinear
correlation coefficient can be predicted based on the number of
variables being considered, and which
orthant\footnote{The term \textit{orthant} is a general term that encompasses the left and right about zero on the number line, the quadrants about zero on the plane, and the octants in 3D. In higher dimensions we will simply use the term \textit{orthant}, however some resources use the term \textit{hyperoctant}.}
that point sits within. Table \ref{tab:orthantsign} shows how when
certain combinations of components of a point have a positive or
negative sign, the resulting product will predictably have a positive or
negative sign depending on whether the correlation is among 2, 3, or 4
variables.\textbackslash{}

Figure \ref{fig:trinlinearcorrelationplots} visualizes a trivariate
analog of Figure \ref{fig:quadrantcorr}. Specifically, a matrix
\(\mathbf{\theta}_{1000 \times 3}\) of trainable parameters were
optimized by gradient descent to minimize the absolute difference
between the multilinear Pearson correlation coefficient calculated on
\(\mathbf{\theta}_{1000 \times 3}\), as if the columns represented
random variables, from a target correlation
score.\footnote{Using gradient descent in this case requires computing the partial derivative of the estimator of the multilinear correlation coefficient with respect to an arbitrary element of $\mathbf{\theta}$. We provide a derivation of such a derivative with respect to an arbitrary element of a data matrix in Appendix \ref{chap:derivativecorrelation} because common computer algebra systems including SageMath, Wolfram Alpha, Maple, and SymPy do not handle the choice of entry being arbitrary. However, to produce Figure \ref{fig:trinlinearcorrelationplots} we used automated differentiation \textit{via} the \texttt{tensorflow} Python package \cite{Abadi2015}.}
Such data sets are non-unique due to the translation invariance and
scaling properties of the multilinear Pearson correlation coefficient.
Figure \ref{fig:trinlinearcorrelationplots} conforms to Table
\ref{tab:orthantsign} in the sense that the directions that either
minimized or maximized the multilinear trilinear correlation coefficient
could be found in specific octants that were opposite in combinations of
signum.\textbackslash{}

\begin{figure}[H]
    \begin{tabular}{cc}
     (a) & (b) \\
    \includegraphics[scale=0.47]{./trilinear_correlation_plot/neg_corr_scatter.pdf} & \includegraphics[scale=0.47]{./trilinear_correlation_plot/pos_corr_scatter.pdf} \\
    (c) & (d) \\
    \includegraphics[scale=0.4]{./trilinear_correlation_plot/loss_history_neg.pdf} & \includegraphics[scale=0.4]{./trilinear_correlation_plot/loss_history_pos.pdf} \\
    \end{tabular}
    \caption{Datasets optimized to have trilinear correlation using gradient descent on the absolute difference. The scatter plots are coloured such that blue represents negative contributions to the correlation statistic, red to positive contributions, and white in-between for zero contribution. (a) Dataset with trilinear correlation close to negative one. (b) Dataset with trilinear correlation close to positive one. (c) Training history to produce dataset with negative trilinear correlation. (d)  Training history to produce dataset with positive trilinear correlation.}
    
\end{figure}

A more subtle pattern arises from observing Table \ref{tab:orthantsign},
which is that the \textit{parity} of the number of variables predicts
how an orthant relates to its
reflection.\footnote{If one considers the arity of a function to be the number of variables in its input, then it is the \textit{parity of the arity}. Such a rhyme might serve as a mnemonic to remember this pattern.}
If the number of variables is even, then the weight contributed by a
point and its reflection will have the same sign. But if the number of
variables is odd, then weight contributed by a point and its reflection
will have opposite signs. This has an important consequence for
distributions that are symmetric.\textbackslash{}

If a distribution is symmetric, then its odd moments will always be
zero. A symmetric distribution may or may not have zero even moments.
Thus odd-arity multilinear Pearson correlation coefficients not only
tell us about dependence, but can also tell us about the symmetry of the
distribution via a contrapositive argument. If a distribution being
symmetric implies that the odd-arity multilinear Pearson correlation
coefficients are zero, and the odd-arity multilinear Pearson correlation
coefficients are not zero, then the distribution in question is not
symmetric. The converse argument is not true in general.\textbackslash{}

\begin{table}[H]
    \caption{Relationship in sign of a multilinear correlation between a given orthant and its reflection. Red indicates that the product of certain values with that signature results in a positive number, and blue indicates the product of values when that signature results in a negative number. (left) Two variables. (middle) Three variables. (right) Four variables.}
    \centering

    \begin{tabular}{cc|cc|cc}
    \hline
    Two & Variables & Three & Variables & Four & Variables \\
    \hline
    Orthant & Reflection & Orthant & Reflection & Orthant & Reflection \\
    \hline
    \cellcolor{red!25} (--,--) & \cellcolor{red!25} (+,+) & \cellcolor{red!25} (+,+,+) & \cellcolor{blue!25} (--,--,--)  & \cellcolor{red!25} (--,--,--,--) & \cellcolor{red!25} (+,+,+,+)  \\
    \cellcolor{blue!25} (--,+) & \cellcolor{blue!25} (+,--) &  \cellcolor{red!25} (--,--,+) & \cellcolor{blue!25} (+,+,--)  &     \cellcolor{blue!25} (--,--,--,+) & \cellcolor{blue!25} (+,+,+,--)  \\
     & & \cellcolor{red!25} (--,+,--) & \cellcolor{blue!25} (+,--,+) & \cellcolor{blue!25} (--,--,+,--) & \cellcolor{blue!25} (+,+,--,+) \\
     & & \cellcolor{red!25} (+,--,--) & \cellcolor{blue!25} (--,+,+) &  \cellcolor{red!25} (--,--,+,+) & \cellcolor{red!25} (+,+,--,--)  \\
     & & & & \cellcolor{blue!25} (--,+,--,--) & \cellcolor{blue!25} (+,--,+,+) \\
     & & & & \cellcolor{red!25} (--,+,--,+) & \cellcolor{red!25} (+,--,+,--) \\
     & & & & \cellcolor{red!25} (--,+,+,--) & \cellcolor{red!25} (+,--,--,+) \\
     & & & &  \cellcolor{blue!25} (--,+,+,+) & \cellcolor{blue!25} (+,--,--,--) \\
    \hline
    \end{tabular}
\end{table}

\section{Nightingale Correlation}\label{nightingale-correlation}

We now introduce Nightingale correlation as an instantiation of the
Trinity of Covariation that generalizes the Nightingale deviation of
order \(p\) (Definition \ref{def:minkowskideviation}), and the absolute
multilinear covariance. It provides another way of quantifying
coordinated change by taking a \(p\)-norm of an element-wise product of
vectors.\textbackslash{}

\subsection{Derivations}\label{derivations-1}

We begin with the notion of a seminorm, see Definition
\ref{def:seminorm}, which is a generalization of the notion of a norm
where the assumption of point separation is absent. Point separation of
a function \(p:V \mapsto \mathbb{R}\) is the property that
\(p(x) = 0 \implies x = 0\) which in the context of norms gives the
intuitive property that the size of a vector is only zero when the
vector is the zero vector.\textbackslash{}

\begin{Definition}[mydefinition=Seminorm \cite{weisstein}, label=def:seminorm]
Give a vector space $X$  over  $\mathbb{R}$ or $ \mathbb{C}$, and a scalar field $\mathcal{F}$, a real-valued function $p:X\mapsto \mathbb{R}$ is called a \underline{seminorm} if it satisfies two conditions $ \forall x \in X,$:
\begin{enumerate}
    \item Subadditivity: $ \forall y \in X,$
    $$p(x+y) \leq p(x) + p(y)$$
    \item Absolute homogeneity: $\forall \alpha \in \mathcal{F}$
    $$p(\alpha x) = |\alpha|p(x)$$
\end{enumerate}
\end{Definition}

Here we generalize the notion of a seminorm from a function of a single
vector to a function of multiple vectors as given in Definition
\ref{def:multiseminorm}, which we refer to as \textit{multiseminorms}.
The prefix \textit{multi-} refers to the use of multiple vectors. The
approach to this generalization is to take the subadditivity and
absolute homogeneity properties that hold in a seminorm, and expand the
definition by taking this to be true input-wise for a multiary
function.\textbackslash{}

\begin{Definition}[mydefinition=Multiseminorm, label=def:multiseminorm]
Let $X_1, \cdots, X_n$ be a collection of vector spaces over  $\mathbb{R}$ or $ \mathbb{C}$. Given a scalar field $\mathcal{F}$, a real-valued function $p: X_1 \times \cdots \times X_n \mapsto \mathbb{R}$ is called a \underline{multiseminorm}  if it satisfies two conditions  $\forall i \in \{1, \cdots,  n\}$, and $\forall (x_1, \cdots, x_n) \in X^n$:
\begin{enumerate}
    \item Subadditivity: $\forall y_i \in X_i$\begin{align*}
    p(x_1, \cdots, x_i + y_i, \cdots, x_n) & \leq p(x_1, \cdots, x_i, \cdots, x_n)\\
    & +p( x_1, \cdots, y_i, \cdots, x_n)
\end{align*}
    \item Absolute homogeneity: $\forall \alpha \in \mathcal{F}$

    $$p(x_1, \cdots, \alpha x_i, \cdots, x_n) = |\alpha| p(x_1, \cdots, x_i, \cdots, x_n)$$
\end{enumerate}
\end{Definition}

With the notion of a multiseminorm in mind as an abstract type of
function, we can define a specific function that can be computed on
finite vectors which satisfies these properties. This is given in
Definition \ref{def:minkowskimultiseminorm} to be the
\textit{Minkowski multiseminorm of order $p$}, which is a \(p\)-norm of
an elementwise product of a collection of vectors.\textbackslash{}

\begin{Definition}[mydefinition=Minkowski Multiseminorm of Order P, label=def:minkowskimultiseminorm]
Given a collection of vectors $\{\vec{x_1}, \cdots, \vec{x_n} \}$ satisfying $\vec{x_j} \in \mathbb{R}^m$, their \underline{Minkowski multiseminorm of order $p$}  is defined to be

$$\vert| \vec{x}_1, \cdots, \vec{x}_n \vert|_p \triangleq \sqrt[p]{ \sum_{i=1}^{m} \left| \prod_{j=1}^{n} x_{i,j} \right|^p}$$

where $p \in \mathbb{R}_{>1}$.
\end{Definition}

The Minkowski multiseminorm of order \(p\) is a multiseminorm. The
subadditivity property can be confirmed by considering Minkowski's
inequality (i.e.~the triangle inequality generalized to \(p\)-norms),
and the absolute homogeneity can be checked by scaling an arbitrary
input and applying some algebra to obtain the desired
result.\textbackslash{}

Just as a norm can induce a metric, so can a seminorm induce a
pseudometric. Likewise, one can define a metric-like function from the
notion of a multiseminorm, which we define to be a
\textit{multipseudometric}. In the particular case of the Minkowski
multiseminorm of order \(p\) we define a function in Definition
\ref{def:minkowskimultipseudometric} called a
\textit{Minkowski multipseudometric of order $p$} which inherits the
properties of a pseudometric input-wise.\textbackslash{}

\begin{Definition}[mydefinition=Minkowski Multipseudometric of Order P, label=def:minkowskimultipseudometric]
Given a two collections of vectors $\{\vec{x_1},  \cdots, \vec{x_n} \}$ and $\{ \vec{y_1}, \cdots, \vec{y_n} \}$ satisfying $\vec{x_j}, \vec{y_j} \in \mathbb{R}^m$, their \underline{Minkowski multipseudometric of order $p$}  is defined to be

$$\vert| \vec{x}_1 - \vec{y}_1, \cdots, \vec{x}_n - \vec{y}_n \vert|_p \triangleq \sqrt[p]{ \sum_{i=1}^{m} \left| \prod_{j=1}^{n} \left( x_{i,j} - y_{i,j} \right) \right|^p}$$

where $p \in \mathbb{R}_{>1}$.
\end{Definition}

Taking Definition \ref{def:minkowskimultipseudometric} that applies to
vectors, we can define a statistical function under an assumption of
\(p\)-integrability that considers the deviations from the mean
coordinated by an index set. This function we take to be the
\textit{Nightingale covariance of order $p$} as given in Definition
\ref{def:minkowskimultideviation}.\textbackslash{}

\begin{Definition}[mydefinition=Nightingale Covariance of Order P, label=def:minkowskimultideviation]

Given a collection of random variables $\{ X_1, \cdots, X_n \}$ with sufficiently defined moments, their \underline{Nightingale covariance of order $p$} is given by 
$$\operatorname{NCov}_p \left[X_1, \cdots, X_n \right] \triangleq \sqrt[p]{ \mathbb{E} \left[ \left| \prod_{j=1}^{n} \left( X_{j} - \mathbb{E}[X_{j}] \right) \right|^p \right]}$$

where $p \in \mathbb{R}_{>1}$.
\end{Definition}

Definition \ref{def:minkowskimultideviation} represents a unification of
multiple notions in addition to being a generalization. We saw with
Definition \ref{def:minkowskideviation} giving the Nightingale deviation
of order \(p\) that the standard deviation can be generalized through
the use of different orders of norms. Since the Nightingale covariance
of order \(p\) reduces to the Nightingale deviation of order \(p\) when
there is only one variable, the Nightingale covariance of order \(p\) is
a generalization of the Nightingale deviation of order \(p\) and the
standard deviation. The Nightingale covariance of order \(p\) is also a
generalization of the multilinear absolute covariance, which is obtained
when \(p=1\).\textbackslash{}

Similar to multilinear correlation, it is desirable to find a
normalization of Nightingale covariance using a suitable inequality. The
inequality in Proposition \ref{prop:nightinequality} serves this
purpose, which generalizes the generalized H"older's inequality for sums
given in Proposition
\ref{thm:generalizedholdersinequalitysums}.\textbackslash{}

\begin{Proposition}[myproposition=, label=prop:nightinequality]
Given $Q_{i,j} \in \mathbb{R}\ \forall i,j$ where $i \in \{ 1, \cdots, m \}$ and $j \in \{ 1, \cdots, n \}$ then

$$\sqrt[p]{\sum_{i=1}^{m} \left|\prod_{j=1}^{n} Q_{i,j}\right|^p} \leq \prod_{j=1}^{n} \left( \sum_{i=1}^{m} |Q_{i,j}|^{\alpha_j} \right)^{\frac{1}{\alpha_j}}$$

where $p \geq 1$, $\alpha_j > 1 \forall j$, and $\sum_{j=1}^n \frac{1}{\alpha_j} = 1$.

\tcbline
\textbf{Proof}\\
\scriptsize
Beginning with the generalized H\"older's inequality for sums given in Proposition \ref{thm:generalizedholdersinequalitysums}, we have

$$\sum_{i=1}^{m} \left|\prod_{j=1}^{n} Q_{i,j}\right| \leq \prod_{j=1}^{n} \left( \sum_{i=1}^{m} |Q_{i,j}|^{\alpha_j} \right)^{\frac{1}{\alpha_j}}$$

where $\alpha_j > 1 \forall j$ and $\sum_{j=1}^n \frac{1}{\alpha_j} = 1$. Writing 

$$\biggl \| \bigodot_{j=1}^{n} \vec{P}_j \biggr  \|_1 = \sum_{i=1}^{m} \left|\prod_{j=1}^{n} Q_{i,j}\right|$$

where $\bigodot$ is the Hadamard product (also called the elementwise product) and 

$$\vec{P}_j = \begin{bmatrix}
Q_{1,j} \\
Q_{2,j} \\
\vdots \\
Q_{m-1,j} \\
Q_{m,j} \\
\end{bmatrix}.$$

Using a property of $p$-norms that $\vert| \vec{x} \vert|_p \leq \vert| \vec{x} \vert|_q$ where $p \geq q$, we have that 

$$\biggl \| \bigodot_{j=1}^{n} \vec{P}_j \biggr  \|_p \leq \biggl \| \bigodot_{j=1}^{n} \vec{P}_j \biggr  \|_1$$

for any $p \geq 1$. Finally, note that

$$\biggl \| \bigodot_{j=1}^{n} \vec{P}_j \biggr  \|_p = \sqrt[p]{\sum_{i=1}^{m} \left|\prod_{j=1}^{n} Q_{i,j}\right|^p}.$$

\hfill $\blacksquare$
\end{Proposition}

With Proposition \ref{prop:nightinequality} we have a normalization for
the Nightingale covariance which we us to define the
\textit{Nightingale correlation} as in Definition
\ref{def:nightingalecorrelation}.\textbackslash{}

\begin{Definition}[mydefinition=Nightingale's Correlation Coefficient, label=def:nightingalecorrelation]
Given a finite collection of real-valued random variables $\{X_1, \cdots, X_n\}$,  and $p, \alpha_j \in \mathbb{R}_{>1}$, their \underline{Nightingale's correlation coefficient} is given by

$$\mathfrak{N} \left[ X_1, \cdots, X_n \right] \triangleq \frac{\operatorname{NCov}_p\left[ X_1, \cdots, X_n \right]}{\prod_{j=1}^{n} \operatorname{Dev}_{\alpha_j}[X_j]}$$

where $\operatorname{Dev}_{\alpha_j}[X_j]$ is the Nightingale deviation of order $\alpha_j$ as given in Definition \ref{def:minkowskideviation}, and provided that $\sum_{j=1}^{n} \frac{1}{\alpha_j} = 1$ and that the desired moments exist.
\end{Definition}

Having defined Nightingale correlation, we will next discuss how to
interpret it.

\subsection{Interpretation}\label{interpretation-1}

The Nightingale correlation coefficient is a mathematically precise
formula that intuitively encodes simultaneous change because in each
instance it is quantifying how much a collection of random variables
deviate together from their centroid.

Similar to the multilinear Pearson correlation coefficient, the
Nightingale correlation coefficient can be interpreted as being weighted
measures of \(n\)-orthotopes drawn out between a centroid and a point. A
key difference between these two statistics is that the contributing
weights in the expectation of product deviations will always be
positive. Consider again Figure \ref{fig:quadrantcorr}, but as if all
the rectangles were positive (i.e.~red) for a visual understanding of
this statistic. This distinction also means that the Nightingale
correlation does not distinguish between orthants in the same way, and
that a symmetric distribution will not necessarily have a zero
Nightingale covariance among an odd number of variables.

Lastly, the Nightingale correlation coefficient uses an expectation of a
product of functions of random variables, so it is interpretable in
terms of Proposition \ref{thm:prodmomentindependence}.

Next we will introduce inner correlations.

\section{Inner Correlation}\label{inner-correlation}

The notion of an inner product space (Definition
\ref{def:innerproductspace}) is a central concept of many disciplines of
modern mathematics, and has found applications in virtually every branch
of science. A familiar example to a student of linear algebra is the dot
product between two vectors, while in statistics it can be found in the
covariance between two random variables. They also occur frequently in
functional analysis where infinite-dimensional vector spaces such as
Hilbert spaces and Banach spaces are considered.\textbackslash{}

\begin{Definition}[mydefinition=Inner Product Space, label=def:innerproductspace]
An \underline{inner product space} is a vector space $V$ over a field $F$ equipped a map $\langle \cdot, \cdot \rangle : V \times V \mapsto F$ called an \underline{inner product} satisfying the following properties for all $x,y,z \in V$ and $a,b \in F$

\begin{itemize}
    \item Conjugate symmetry:  $\langle x, y \rangle = \overline{\langle y, x \rangle}$
    \item Linearity: $\langle ax + by, z \rangle = a\langle x, z \rangle + b\langle y, z \rangle$
    \item Positive-definiteness: $x \neq 0 \implies \langle x, x \rangle > 0$
\end{itemize} 
\end{Definition}

We will turn to two generalizations of inner product spaces to define
new correlation functions on collections of random variables.

\subsection{Misiak Correlation}\label{misiak-correlation}

In 1989 Misiak introduced \(n\)-inner product spaces as a natural
generalization of \(2\)-inner product spaces \cite{Misiak1989}. The
earlier motivations for Misiak's work were built on a history of
studying abstract \(n\)-dimensional metric spaces, which we do not focus
on in this work. Rather we will focus on using the definitions and
propositions developed by Misiak to define a new statistc. We begin with
considering a generalization of inner products given in Definition
\ref{def:ninnerproduct}.\textbackslash{}

\begin{Definition}[mydefinition=$n$-Inner Product \cite{Misiak1989}, label=def:ninnerproduct]
Let $n$ be a positive integer and $V$ a real vector space such that $\dim V \geq n$ and $( \bullet, \bullet | \bullet, \cdots, \bullet )$ is a real function defined on $\underbrace{V \times V \times  \cdots \times V}_{n+1}$ such that\\ $ \forall\ \vec{a}, \vec{b}, \vec{x}_1, \cdots, \vec{x}_{n} \in V$

\begin{enumerate}
    \item $(\vec{x}_1, \vec{x}_1 | \vec{x}_2, \cdots, \vec{x}_n) \geq 0$;
    \item $(\vec{x}_1, \vec{x}_1 | \vec{x}_2, \cdots, \vec{x}_n) = 0 \iff \vec{x}_1, \vec{x}_2, \cdots, \vec{x}_n$ are linearly independent;
    \item $(\vec{a}, \vec{b} | \vec{x}_1, \cdots, \vec{x}_{n-1}) = (\phi(\vec{a}), \phi (\vec{b}) | \pi(\vec{x}_1), \cdots, \pi(\vec{x}_{n-1}))$ and for any bijections
    \begin{align*}
        \pi : \{ \vec{x}_1, \cdots, \vec{x}_{n-1}  \} &\mapsto \{ \vec{x}_1, \cdots, \vec{x}_{n-1}  \}\\
        \phi:  \{ \vec{x}_1, \cdots, \vec{x}_{n-1}  \} &\mapsto \{ \vec{x}_1, \cdots, \vec{x}_{n-1} \}; \\
    \end{align*}
    \item $n > 1 \implies (\vec{x}_1, \vec{x}_1 | \vec{x}_2, \cdots, \vec{x}_n) = (\vec{x}_2, \vec{x}_2 | \vec{x}_3, \cdots, \vec{x}_n)$;
    \item $(\alpha \vec{a}, \vec{b} | \vec{x}_1, \cdots, \vec{x}_{n-1})=  \alpha (\vec{a}, \vec{b} | \vec{x}_1, \cdots, \vec{x}_{n-1})\ \forall\  \alpha \in \mathbb{R}$;
    \item $ (\vec{a} + \vec{y}, \vec{b} | \vec{x}_1, \cdots, \vec{x}_{n-1}) =  (\vec{a}, \vec{b} | \vec{x}_1, \cdots, \vec{x}_{n-1}) + (\vec{y}, \vec{b} | \vec{x}_1, \cdots, \vec{x}_{n-1})\ \forall\ \vec{y} \in V$;
\end{enumerate}

Then $( \bullet, \bullet | \bullet, \cdots, \bullet )$ is called an \underline{$n$-inner product} and $(V, ( \bullet, \bullet | \bullet, \cdots, \bullet ))$ is called an \underline{$n$-inner product space}.
\end{Definition}

Definition \ref{def:ninnerproduct} is quite general, and does not by
itself give us specific a function that we could compute on a function
or random variable. Rather \(n\)-inner products represent an abstract
class of functions that satisfy a list of axioms, similar to inner
products themselves. Misiak provided a way of systematically defining
\(n\)-inner products from inner products using the notion of a
determinant of a matrix of inner products as given in Proposition
\ref{prop:misiakcorollary15}.\textbackslash{}

\begin{Proposition}[myproposition=\cite{Misiak1989}, label=prop:misiakcorollary15]

For every inner product $(\bullet, \bullet)$ on $V$, the function $( \bullet, \bullet | \bullet, \cdots, \bullet )$ defined on $V^{n+1}$ by

$$( \vec{a}, \vec{b} | \vec{x}_1, \cdots, \vec{x}_{n-1} ) \triangleq \begin{vmatrix}
(\vec{a}, \vec{b}) & (\vec{a}, \vec{x}_1) & \cdots & (\vec{a}, \vec{x}_{n-1}) \\ 
(\vec{x}_1, \vec{b}) & (\vec{x}_1, \vec{x}_1) & \cdots & (\vec{x}_1, \vec{x}_{n-1}) \\ 
\vdots & \vdots & \ddots & \vdots \\
(\vec{x}_{n-1}, \vec{b}) & (\vec{x}_{n-1}, \vec{x}_1) & \cdots & (\vec{x}_{n-1}, \vec{x}_{n-1}) \\
\end{vmatrix} $$

is an $n$-inner product on $V$.
\tcbline
\textbf{Proof}\\
See Corollary 15 in \cite{Misiak1989}.
\end{Proposition}

Proposition \ref{prop:misiakcorollary15} allows us to make various
choices of inner products on suitable operands. In the context of a
sample we might choose for each vector to be an indexed set of
deviations of the mean of instantiations of a random
variable.\textbackslash{}

It would be desirable to have normalized statistic onto an interval of
\([-1,1]\) as we achieved with multilinear correlations earlier in this
chapter. This is especially the case when the scale of the statistic is
not readily interpreted. Misiak provided an inequality that sets bounds
on the value that an \(n\)-inner product can take for a given set of
vectors, which is given in Proposition
\ref{prop:cauchyBunyakowskiinequalityarbitraryn} as the
\textit{Cauchy-Bunyakowski inequality for arbitrary $n$}.\footnote{\cite{Misiak1989} did not give any indication as to why Bunyakowski's name is included in the name of this result.}\textbackslash{}

\begin{Proposition}[myproposition=Cauchy-Bunyakowski Inequality for Arbitrary $n$ \cite{Misiak1989}, label=prop:cauchyBunyakowskiinequalityarbitraryn]
For $\vec{a}, \vec{b}, \vec{x}_1, \cdots, \vec{x}_{n-1} \in V$,

$$|(\vec{a}, \vec{b}| \vec{x}_1, \cdots, \vec{x}_{n-1})| \leq \sqrt{ \left(\vec{a}, \vec{a}| \vec{x}_1, \cdots, \vec{x}_{n-1} \right)} \sqrt{ \left(\vec{b}, \vec{b}| \vec{x}_1, \cdots, \vec{x}_{n-1} \right)} $$

\tcbline
\textbf{Proof}\\
See Theorem 3 in \cite{Misiak1989}.
\end{Proposition}

With the above definitions and propositions, we can construct a new
statistic called \textit{Misiak's correlation coefficient} (Definition
\ref{def:misiakscorrelation}).\textbackslash{}

\begin{Definition}[mydefinition=Misiak's Correlation Coefficient, label=def:misiakcorrelation]
Given random variables $X,Y$ and $\{ Z_j \}_{j=1}^{n-1}$ their \underline{Misiak correlation} is given by

$$R_{\text{Misiak}}[X, Y ; Z_1, \cdots, Z_{n-1}] \triangleq \frac{(X, Y | Z_1, \cdots, Z_{n-1})}{\sqrt{(X, X | Z_1, \cdots, Z_{n-1})} \sqrt{(Y, Y | Z_1, \cdots, Z_{n-1})} }$$

where $( \bullet, \bullet | \bullet, \cdots, \bullet )$ is defined according to Proposition \ref{prop:misiakcorollary15} using $\mathbb{E}[U_i, U_k]$ as an inner product between any two random variables $U_i, U_k$ with finite second moments.
\label{def:misiakscorrelation}
\end{Definition}

\subsection{\texorpdfstring{Tren\v cevski-Mal\v ceski
Correlation}{Trenevski-Maleski Correlation}}\label{trenevski-maleski-correlation}

While \(n\)-inner products compare a pair of vectors or functions to a
collection of other vectors or functions, a further generalization of
this is possible that compares one collection of vectors or functions to
another collection of vectors or functions. A
\textit{generalized $n$-inner product space} was defined by
\cite{trencevski2006}, which is given in Definition
\ref{def:trencevskimalceski}.\textbackslash{}

\begin{Definition}[mydefinition=Generalized $n$-Inner Product Space \cite{trencevski2006}, label=def:trencevskimalceski]
Assume that $n$ is a positive integer, $V$ is a real vector space such that $\dim V \geq n$ and $\langle \bullet, \cdots, \bullet | \bullet, \cdots, \bullet \rangle$ is a real function on $V^{2n}$ such that

\begin{itemize}
    \item $\langle \vec{x}_1, \cdots, \vec{x}_n | \vec{x}_1, \cdots, \vec{x}_n \rangle > 0$ if $\vec{x}_1, \cdots, \vec{x}_n$ are linearly independent vectors,
    \item $\langle \vec{x}_1, \cdots, \vec{x}_n | \vec{y}_1, \cdots, \vec{y}_n \rangle = \langle \vec{y}_1, \cdots, \vec{y}_n | \vec{x}_1, \cdots, \vec{x}_n \rangle$,
    \item $\langle \lambda \vec{x}_1, \cdots, \vec{x}_n | \vec{y}_1, \cdots, \vec{y}_n \rangle = \lambda \langle \vec{x}_1, \cdots, \vec{x}_n | \vec{y}_1, \cdots, \vec{y}_n \rangle$,
    \item $\langle \vec{x}_1, \cdots, \vec{x}_n | \vec{y}_1, \cdots, \vec{y}_n \rangle = -\langle  \vec{x}_{\sigma (1)}, \cdots, \vec{x}_{\sigma (n)} | \vec{y}_1, \cdots, \vec{y}_n \rangle$ for any odd permutation $\sigma$ on the set $\{1, \cdots, n \}$,
    \item $\langle \vec{x}_1 + \vec{z}, \cdots, \vec{x}_n | \vec{y}_1, \cdots, \vec{y}_n \rangle = \langle \vec{x}_1, \cdots, \vec{x}_n | \vec{y}_1, \cdots, \vec{y}_n \rangle + \langle \vec{z}, \cdots, \vec{x}_n | \vec{y}_1, \cdots, \vec{y}_n \rangle$
\end{itemize}

for any $\vec{x}_1, \cdots, \vec{x}_n, \vec{y}_1, \cdots, \vec{y}_n \in V$ and $\lambda \in \mathbb{R}$.\\

Then the function $\langle \bullet, \cdots, \bullet | \bullet, \cdots, \bullet \rangle$ is called a \underline{(generalized) $n$-inner product} and the pair $(V, \langle \bullet, \cdots, \bullet | \bullet, \cdot, \bullet \rangle)$ is called an \underline{$n$-prehilbert space}.
\end{Definition}

According to \cite{trencevski2006}, the \(n\)-inner product defined by
\cite{Misiak1989} is a special case of the generalized \(n\)-inner
product by the relation
\[(\vec{x}, \vec{y} | \vec{z}_1, \cdots, \vec{z}_{n-1}) = \langle \vec{x}, \vec{z}_1, \cdots, \vec{z}_{n-1} | \vec{y}, \vec{z}_1, \cdots, \vec{z}_{n-1} \rangle.\]

With a generalization of \(n\)-inner products as given in Definition
\ref{def:trencevskimalceski} we desire a specific approach to defining
examples of generalized \(n\)-inner products. \cite{trencevski2006}
provide an analogous result to Proposition \ref{prop:misiakcorollary15}
in the form of Proposition \ref{prop:trencevskiexample}.\textbackslash{}

\begin{Proposition}[myproposition=\cite{trencevski2006}, label=prop:trencevskiexample]

For every inner product $(\bullet, \bullet)$ on $V$, the function $\langle \bullet, \cdots, \bullet| \bullet, \cdots, \bullet \rangle$ defined on $V^{2n}$ by

$$\langle \vec{x}_1, \cdots, \vec{x}_n | \vec{y}_1, \cdots, \vec{y}_n  \rangle \triangleq \begin{vmatrix}
( \vec{x}_1, \vec{y}_1 ) & ( \vec{x}_1, \vec{y}_2 ) & \cdots & ( \vec{x}_1, \vec{y}_{n} ) \\ 
( \vec{x}_2, \vec{y}_1 ) & ( \vec{x}_2, \vec{y}_2 ) & \cdots & ( \vec{x}_2, \vec{y}_{n} ) \\ 
\vdots & \vdots & \ddots & \vdots \\
( \vec{x}_{n}, \vec{y}_1 ) & ( \vec{x}_{n}, \vec{y}_2 ) & \cdots & ( \vec{x}_{n}, \vec{y}_{n} ) \\
\end{vmatrix} $$

is a generalized $n$-inner product on $V$.
\tcbline
\textbf{Proof}\\
See Example 2.1 in \cite{trencevski2006}.
\end{Proposition}

It is also desirable to obtain a normalization of any function that is
an example of Proposition \ref{prop:trencevskiexample}. Similar to
Proposition \ref{prop:cauchyBunyakowskiinequalityarbitraryn} proven in
\cite{Misiak1989}, \cite{trencevski2006} provide the analogous result as
given in Proposition \ref{prop:trencevskiinequality} to obtain a
normalization.\textbackslash{}

\begin{Proposition}[myproposition=\cite{trencevski2006}, label=prop:trencevskiinequality]

If $\langle \bullet, \cdots, \bullet | \bullet, \cdots, \bullet \rangle$ is a (generalized) $n$-inner product on $V$, then

$$| \langle \vec{x}_1, \cdots, \vec{x}_n | \vec{y}_1, \cdots, \vec{y}_n \rangle |  \leq \sqrt{\langle \vec{x}_1, \cdots, \vec{x}_n | \vec{x}_1, \cdots, \vec{x}_n \rangle} \sqrt{\langle \vec{y}_1, \cdots, \vec{y}_n | \vec{y}_1, \cdots, \vec{y}_n \rangle}$$

for all $\vec{x}_1, \cdots, \vec{x}_n,  \vec{y}_1, \cdots, \vec{y}_n \in V$.

\tcbline
\textbf{Proof}\\
See the first part of Theorem 2.1 in \cite{trencevski2006}.
\end{Proposition}

Taking our choice of inner product to be the mixed product moment
between two random variables, and utilizing it to construct a normalized
statistic we obtain Definition
\ref{def:trencevskimalceskicorrelation}.\textbackslash{}

\begin{Definition}[mydefinition=Tren\v cevski-Mal\v ceski Correlation Coefficient, label=def:trencevskimalceskicorrelation]
Given two collections of random variables $\{ X_1, \cdots, X_n \}$ and $\{ X_1, \cdots, X_n \}$ their \underline{Tren\v cevski-Mal\v ceski correlation} is given by

$$R_{\text{TM}}[X_1, \cdots, X_n; Y_1, \cdots, Y_n] \triangleq \frac{\langle X_1, \cdots, X_n | Y_1, \cdots, Y_n \rangle}{\sqrt{\langle X_1, \cdots, X_n | X_1, \cdots, X_n \rangle} \sqrt{\langle Y_1, \cdots, Y_n | Y_1, \cdots, Y_n \rangle} }$$

where $\langle \bullet, \bullet | \bullet, \cdots, \bullet \rangle$ is defined according to Proposition \ref{prop:trencevskiexample} using $\mathbb{E}[U_i, U_k]$ as an inner product between any two random variables $U_i, U_k$ with finite second moments.
\end{Definition}

\subsection{Interpretation}\label{interpretation-2}

In terms of the Trinity of Covariation, making a substitution such as
\(U := U - \mathbb{E}[U]\) entails that the resulting determinants of
matrices computed via either Proposition \ref{prop:misiakcorollary15} or
Proposition \ref{prop:trencevskiexample} will be equivalent to computing
the determinant of some covariance matrix. The covariance matrix itself
is a quantification of coordinated change between pairs of variables,
and its determinant does so in an aggregated way. Due to our usage of
expectations of random variables, we can also leverage Proposition
\ref{thm:prodmomentindependence} in knowing that statistical
independence implies that such expectations should distribute across
products of measureable functions of random variables. In the particular
case of computing covariances, we know that
\(X \ind Y \implies \operatorname{Cov}[X,Y] = 0\).\textbackslash{}

Definition \ref{def:misiakcorrelation} and Definition
\ref{def:trencevskimalceskicorrelation} also tell us either about linear
dependence or about spanning the same subspace through Proposition
\ref{prop:trencevskilineardependence}.\textbackslash{}

\begin{Proposition}[myproposition=\cite{trencevski2006}, label=prop:trencevskilineardependence]

Equality holds in Proposition \ref{prop:trencevskiinequality} if-and-only-if at least one of the following conditions is satisfied

\begin{itemize}
    \item $\vec{x}_1, \cdots, \vec{x}_n$ are linearly dependent,
    \item $\vec{y}_1, \cdots, \vec{y}_n$ are linearly dependent,
    \item $\vec{x}_1, \cdots, \vec{x}_n$ and $\vec{y}_1, \cdots, \vec{y}_n$ span the same vector subspace of dimension $n$.
\end{itemize}

\tcbline
\textbf{Proof}\\
See the second part of Theorem 2.1 in \cite{trencevski2006}.
\end{Proposition}

If either of the first two conditions hold in Proposition
\ref{prop:trencevskilineardependence} then the derived correlation
coefficients will be indeterminant because both the left and the right
hand sides of Proposition \ref{prop:trencevskiinequality} will be zero.
Thus calculating the numerator and factors of the denominator of the
Misiak and Tren\v cevski-Mal\v ceski correlation coefficients to check
if linear dependence holds is recommended. In the remaining third case
equality holds if the two collections of vectors span the same subspace,
yielding a correlation score of \(\pm 1\) depending on whether the
orientation of the vectors is needed. Section 3 of \cite{trencevski2006}
provides a discussion of how such a coefficient as given in Definition
\ref{def:trencevskimalceskicorrelation} can be defined as a cosine of
the angle between two subspaces which can be used to obtain a metric on
subspaces of a vector space \(V\) simply by taking the \(\arccos\) to
obtain the angle. They show that such an angle is invariant to choice of
basis. Therefore inner correlation coefficients as we have defined in
this section are a way of describing the ``closeness'' of two sets of
random variables in a way that does not depend on scaling or
translation.\textbackslash{}

\section{Agnesian Operators}\label{agnesian-operators}

We introduce Agnesian operators as an instantiation of the Trinity of
Covariation that utilizes elementary calculus. We have named this family
of operators after the Italian mathematician Maria Gaetana Agnesi
(1718-1799) (Figure \ref{fig:mariaagnesi}(a))\cite{Dumbaugh2019}. She
was the first woman to be appointed as a mathematics professor (Figure
\ref{fig:mariaagnesi}(c)), and she was known for writing the textbook
\textit{Instituzioni Analitiche ad uso della gioven\`{u} italiana}\footnote{English translation: \textit{Analytical Institutions for the use of Italian youth}.}
(Figure \ref{fig:mariaagnesi}(b)) that covered both differential and
integral calculus \cite{Dumbaugh2019}. Because the Agnesian operators
unite the notions of derivatives and integrals into a single operator,
it seems symbolically fitting that Maria Agnesi be their namesake
because her work brought differential and integral calculus together in
the education of young mathematicians.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\includegraphics{index_files/mediabag/Maria_Gaetana_Agnesi.jpg}

\subcaption{\label{}Maria Gaetana Agnesia (Public Domain)}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}
\includegraphics{index_files/mediabag/Il_frontispizio_dell.png}\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\includegraphics{index_files/mediabag/Il_diploma_di_nomina.png}

\subcaption{\label{}Agnesi's diploma from UniversitÃ  di Bologna (Public
Domain)}
\end{minipage}%

\caption{\label{fig-maria-gaetana-agnesi}Maria Gaetana Agnesi}

\end{figure}%

\begin{figure}[H]
    \centering
    \begin{tabular}{cc}
    (a) & (b) \\
    \includegraphics[scale=0.5]{Maria_Gaetana_Agnesi.jpg} & \includegraphics[scale=1.1]{Agnesi_book.png} \\
    \tiny \ccPublicDomain\ Public Domain & \tiny \ccPublicDomain\ Public Domain\\
    \end{tabular}
    
    \begin{tabular}{c}
    (c) \\
    \includegraphics[scale=1]{Agnesi_diploma.png} \\
    \tiny \ccPublicDomain\ Public Domain \\
    \end{tabular}
    \caption{(a) Portrait of Maria Gaetana Agnesi. (b) The first page of \textit{Instituzioni Analitiche ad uso della gioven\`{u} italiana}. (c) Maria Agnesi's diploma in mathematics from the University of Bologna.}
    
\end{figure}

Derivatives are linear approximations of the slope of the line tanget to
a function at a given point, providing a
representation\footnote{To clarify our intended meaning of the word \textit{representation} for mathematicians, we are \textbf{not} using the word \textit{representation} here to mean a representation of a group. In this thesis we take it to mean a commonplace notion of showing certain aspect of one thing in terms of another.}
of the change part of the Trinity of Covariation. In the case of
intergrals, the definite integral over \([a,b]\),
\(\int_a^b \frac{df}{dt}dt = f(b) - f(a)\), models the net change
\textit{via} the net change theorem. This idea is adopted to represent
the change part of the Trinity of Covariation. We can choose collections
of differentiable and/or integrable scalar functions to represent a kind
of `structure'. The notion of `coordination' can be captured in the form
of scalar multiplication in which one might think of a collection of
changes mutually scaling each other. Putting these notions together,
Definition \ref{def:totalagnesian} gives a precise definition of the
(total) Agnesian operator of a given
order.\footnote{A *partial* Agnesian operator is easily defined by replacing the total derivatives in Definition \ref{def:totalagnesian} by partial derivatives, which we denote $\mathcal{A}_{\partial t}^{k} S$.}

\begin{quote}
\textbf{Definition}

Let
\(S = \{x_1(t),  \cdots, x_n(t) | x_j(t) \in \mathbb{R}, t \in \mathbb{R} \}\)
be a collection of suitably smooth or integrable functions of a real
parameter \(t\), then their \textbf{Agnesian of order \(k\)} is given by
\[\mathcal{A}_{t}^{k} S= \begin{cases} \prod_{j=1}^{n} \frac{d^k}{dt^k}x_j(t) & k > 0 \\ \prod_{j=1}^{n} x_j (t) & k = 0 \\ \prod_{j=1}^{n} \underbrace{\int \cdots \int}_k x_j(t)\ \underbrace{dt \cdots dt}_k & k < 0. \end{cases}\]
\end{quote}

Because the Agnesian is defined using set-builder notation, it is
possible to construct a great variety functions with it. For a given
collection of \(n\) suitably smooth or integrable functions, there
exists \(|\mathcal{P}(S)/\emptyset| = 2^n-1\) (i.e.~the powerset
excluding the empty
set)\footnote{If empty sets are considered, we find it natural to suggest that $\mathcal{A}_{t}^{k} \emptyset := 0$.}
possible choices of Agnesian functionals that could be defined at some
given order. For convenience of notation, we can consider Agnesian
operators of scalar functions and vector
functions.\footnote{We can further consider Agnesian operators of matrices and multidimensional arrays of functions by using compositions of pairing functions. Such pairing functions $\phi : \mathbb{N}^n \mapsto \mathbb{N}$ achieve a one-to-one mapping between a multi-index function and an index function.}
For a scalar function \(f(t)\) we will assume that notation
\(\mathcal{A}_t^k f(t)\) is equivalent to
\(\mathcal{A}_t^k \{ f(t) \}\). In the vector case we will take
\(\mathcal{A}_t^k \vec{x}(t)\) where the components of \(\vec{x}(t)\)
are indexed by \(j \in \{1, \cdots, n \}\) to be equivalent to
\(\mathcal{A}_t^k \{x_1(t), \cdots, x_n(t) \}\). These two notational
conventions immediately gives us the property that
\(\mathcal{A}_t^k \vec{x}(t) = \prod_{j=1}^n \mathcal{A}_t^k x_j(t)\),
which is used in the proof of Proposition
\ref{prop:agnesiansqueezeinvariant}.

Let us consider the example of a helix embedded in \(\mathbb{R}^3\)
according to the vector equation

\[\vec{s}(t) = \begin{bmatrix}
\cos t \\
\sin t \\
t \\
\end{bmatrix}.\]

Figure \ref{fig:Agnesianhelixpanel} shows the Agnesians of a helix for
orders \(k \in \{-3, -2, -1, 0, 1, 2\}\), and illustrates how the order
specifies different curves. The zero-order Agnesian in Figure
\ref{fig:Agnesianhelixpanel}(a) has reflective symmetry about \(t=0\)
and will oscillate between larger amplitudes as
\(|t| \rightarrow \infty\). The first-order Agnesian in Figure
\ref{fig:Agnesianhelixpanel} is equivalent to \(\frac{1}{2} \sin 2t\),
which is a reflection equivariant function (i.e.~an odd function) in
\(t\). As shown in Figure \ref{fig:Agnesianhelixpanel} (c), the
second-order Agnesian is zero for all time because the
\(\frac{d^2 t}{dt^2} = 0\) factor. Indeed, any Agnesian of this helix in
the given coordinates will be zero if \(k \geq 2\). Panels (d), (e), and
(f) of Figure \ref{fig:Agnesianhelixpanel} show that with constants of
integration taken to be zero we have an alternating pattern of odd or
even Agnesians depending on the parity of the degree of the polynomial
factor \(\frac{t^{k+1}}{(k+1)!}\). For \(k < 0\), if \(k+1\) is even
then the result Agnesian is an odd function of \(t\), and if \(k+1\) is
even then the resulting Agnesian will be odd. Similar to what we
observed with the zero-order Agnesian, we find that the negative order
Agnesians of this helix will oscilate between larger amplitudes as
\(|t| \rightarrow \infty\).\textbackslash{}

Obtaining a scalar function of time affords us the opportunity to ask
elementary questions about the function. Such questions include its
domain, image, and extrema. While all of the Agnesian curves in the
previous example were defined over all real numbers, this does not need
to be the case in general. An example is the curve \([t, \log t]^T\)
which will not be defined for \(t=0\). The example of the helix also
showed us that some Agnesians have local and global optima, while others
have no finite global optima.\textbackslash{}

\begin{figure}[H]
\centering
\begin{tabular}{cc}
(a) & (b) \\
\includegraphics[scale=0.4]{./agenesian_helix/zeroeth.pdf} & \includegraphics[scale=0.4]{./agenesian_helix/first.pdf} \\
(c) & (d) \\
 \includegraphics[scale=0.4]{./agenesian_helix/second.pdf} & \includegraphics[scale=0.4]{./agenesian_helix/negative_first.pdf} \\
 (e) & (f) \\
 \includegraphics[scale=0.4]{./agenesian_helix/negative_second.pdf} &  \includegraphics[scale=0.4]{./agenesian_helix/negative_three.pdf}\\
\end{tabular}
\caption{Agnesians of different orders of a helix plotted on the interval of $t \in [-2 \pi, 2 \pi ]$. (a) Order zero ($k=0$). (b) Order one ($k=1$). (c) Order two ($k=2$). (d) Order negative one ($k=-1$), with constants of integration set to zero. (e) Order negative two ($k=-2$), with constants of integration set to zero. (f) Order negative three ($k=-3$), with constants of integration set to zero.}

\end{figure}

In the example of the helix we assumed a given coordinate system. It is
important to consider that the Agnesian of a parametric curve is not
invariant to linear changes in basis. For a parametric curve
\(\vec{x}(t) \in \mathbb{R}^n\) and linear transformation
\(T \in \mathbb{R}^{n \times n}\), it does not hold in general that
\(\mathcal{A}_t^k \left[ T \vec{x}(t) \right] = \mathcal{A}_t^k \left[ \vec{x}(t) \right]\).
There exist invariants for the Agnesian of parametric curves in
\(\mathbb{R}^{2}\) including rotations of \(\pi\) radians, squeezing,
and permutation. The identity matrix is a permutation matrix, and
clearly leaves the curve unchanged. In Proposition
\ref{prop:agnesiansqueezeinvariant} we show that multidimensional
squeezing is an invariant of the Agnesian operator.

\begin{quote}
\textbf{Proposition}

Let \(\vec{x}(t) \in \mathbb{R}^n\) be a parametric curve and
\(D \in \mathbb{R}^{n \times n}\) be a diagonal matrix with constant
entries and \(\det D = 1\), then

\[\mathcal{A}_t^k \left[ D \vec{x}(t) \right] = \mathcal{A}_t^k \left[ \vec{x}(t) \right].\]

\textbf{Proof}

Taking \[
 D =
 \begin{bmatrix}
   d_{1} & & \\
   & \ddots & \\
   & & d_{n}
 \end{bmatrix}\]

and \[\vec{x}(t) = \begin{bmatrix}
x_1(t) \\
\vdots \\
x_n(t)
\end{bmatrix}\]

then \[D \vec{x}(t) = \begin{bmatrix}
d_1 x_1(t) \\
\vdots \\
d_n x_n(t)
\end{bmatrix}.\] This entails that

\begin{align*}
\mathcal{A}_t^k \left[ D \vec{x}(t) \right] =& \prod_{j=1}^n d_j \mathcal{A}_t^k x_j(t) \\
=& \left( \prod_{j=1}^n d_j \right) \left( \prod_{j=1}^n \mathcal{A}_t^k x_j(t) \right) \\
=& \det D \mathcal{A}_t^k \left[ \vec{x}(t) \right] \\
=& \mathcal{A}_t^k \left[ \vec{x}(t) \right]
\end{align*}

\(\blacksquare\)
\end{quote}

What these invariant linear transformations have in common is that they
have determinant of unity. This suggests a hypothesis that the property
of being volume-preserving might be a necessary-but-insufficient
condition for an operation to be Agnesian-preserving, but we do not
explore this hypothesis further in this thesis.

The Agnesian provides a scalar function of a single parameter, which
could be time, length, or some other variable. It is sometimes desirable
to incorporate multiple parameters, like when there is one parameter of
time and three directions of physical space. An extension to the
Agnesian operator comes from considering coordinated change in multiple
parameters in the form of the \emph{multiagnesian} as given in
Definition \ref{def:multiagnesian}.

\begin{quote}
\textbf{Definition}

Let
\(S = \{x_1(t_1, \cdots, t_p),  \cdots, x_n(t) | x_j(t_1, \cdots, t_p) \in \mathbb{R} \land t_1, \cdots, t_p \in \mathbb{R} \}\)
be a collection of suitably smooth or integrable functions of a real
parameters \(t_1, \cdots, t_p\), then their
\underline{multiagnesian of order $k$} is given by \begin{equation*}
\mathcal{A}_{(t_1, \cdots, t_p)}^{k} S = \begin{cases} \prod_{j=1}^{n} \prod_{w=1}^{p} \frac{d^k}{dt_{w}^k}x_j(t_1, \cdots, t_p) & k > 0 \\ \prod_{j=1}^{n} \prod_{w=1}^{p} x_j (t_1, \cdots, t_p) & k = 0 \\ \prod_{j=1}^{n} \prod_{w=1}^{p} \underbrace{\int \cdots \int}_k x_j(t_1, \cdots, t_p)\ \underbrace{dt_{w}  \cdots dt_{w}}_k & k < 0 \end{cases}
\end{equation*}
\end{quote}

The multiagnesian operator is a natural generalization of the Agnesian
operator to include multiple parameters. While the Agnesian operator can
be used to study parametric curves in finite-dimensional vector spaces,
the multiagnesian operator can be used to study surfaces and
hypersurfaces in similar spaces. Using the commutativity and
associativity of scalar multiplication, we can readily find that a
multiagnesian is the product of Agnesian operators with respect to each
parameter:\footnote{We adopt similar conventions for multiagnesians of scalar functions and vectors functions as we have for Agnesians.}

\[\mathcal{A}_{(t_1, \cdots, t_p)}^{k} \vec{x}(t_1, \cdots, t_p) = \prod_{w=1}^p \mathcal{A}_{t_w}^k \vec{x}(t_1, \cdots, t_p).\]

We have defined the Agnesian operator, and given some exemplification
and discussion of it in mathematical language. In the next subsection we
will offer some further interpretations of this operator.

\subsection{Interpretation}\label{interpretation-3}

The Agnesian operators can be understood in terms of mutually scaling
change. Due to the product expansion in the definition of an Agnesian,
it is clear that a collection of quantities are mutually scaling each
other, and the order tells us what sort of quantities are performing
this scaling. For positive order \$ k \textgreater{} 0\$ we are
considering how a collection of derivatives of a corresponding order are
mutually scaling. A zero-order Agnesian quantifies how large the
functions are together at some value of the parameter. And for the case
of negative orders we are considering the coordinated net changes in a
collection of functions with respect to the parameter. Decreasing the
order below \(k=-1\) leads to net changes in net changes, etc, depending
on the value of \(k\).\textbackslash{}

A picture that comes from taking the product of a collection of scalars
is to consider an \(n\)-dimensional box whose lengths are equal to the
\(n\) scalars being multiplied. The product of these scalars would then
be the signed volume of the box. For \(n \leq 3\) we can draw pictures
to motivate what this looks like. Let us suppose an example where
\(\{ f(t), g(t) \}\) is our set of functions of time. Figure
\ref{fig:agnesianbox} illustrates a hypothetical planar curve whose
coordinates are defined by
\(\left( \mathcal{A}_t^k f(t), \mathcal{A}_t^k g(t) \right)\). For a
given point on this curve there exists a rectangular region between it
and the origin. The signed area of this rectangle gives the value of
\(\mathcal{A}_t^k\{ f(t), g(t) \}\). The sign of the Agnesian operator
follows a similiar pattern to that illustrated in Figure
\ref{fig:quadrantcorr} and Table \ref{tab:orthantsign} describing the
behaviour of multilinear correlation in the sense that multiplying
scalars with particular combinations of positive and negative sign
result in a scalar with a parciular sign. Unlike the multilinear
correlation function, the Agnesian operator as we have defined above
does not consider any random variables.\textbackslash{}

\begin{figure}[H]
    \centering
    \begin{tabular}{cc}
    (a) & (b) \\
    \includegraphics[scale=0.48]{./agnesian_box_interpretation/agnesian_box.pdf} & \includegraphics[scale=0.48]{./agnesian_box_interpretation/agnesian_beside_box.pdf} \\
    \end{tabular}
    \caption{Visual interpretation of the Agnesian operator of two functions as signed volumes of rectangles. The red corresponds to positive area, and the blue corresponds to negative area. (a) Illustration of rectangles representing the signed volumes of the Agnesian of two parametric functions. There exists four boxes corresponding the rectangular regions between the origin at $t=0$ out to $t=\pm \frac{1}{2}$ or $t=\pm \frac{3}{4}$. (b) Agnesian of both functions.}
    
\end{figure}

We noted earlier that Agnesian operators are not invariant to linear
changes in basis in general. This mathematical fact can be related back
to the Trinity of Covariation: the amount of coordinated change as
quantified by the Agnesian operator depends on from what perspective we
are `looking' at the structure.\textbackslash{}

We have introduced the Agnesian operator as an instance of the Trinity
of Covariation and as an integro-differential operator. In the next
section we will introduce grade entropies as an instance of the Trinity
of Covariation that connects the subjects of information theory and
order theory.

\section{Grade Entropies}\label{grade-entropies}

In this section we will introduce a family of functions called
\textit{grade entropies}. Because entropy is an often-misunderstood
concept, we provide some historical context and review of mathematical
definitions and properties to clarify it sufficiently for understanding
the concept of grade entropy. Then we will review grade functions in the
context of partially ordered sets and relate them to entropy. Finally we
discuss the interpretation of grade entropies and how they relate to the
Trinity of Covariation.

\subsection{Entropy}\label{entropy}

The first notions that would become a formal model of entropy were
developed by Rudolf Clausius (Figure \ref{fig:foundersentropy}(a)),
which he described as the \textit{transformational content} of a body in
terms of the reciprocal temperature integrated over a heat differential
\cite{clausius1867}.

\begin{figure}[H]
    \centering
    \begin{tabular}{ccc}
    (a) & (b) &  \\
    \includegraphics[scale=0.4]{rudolf_clausius.jpg} & \includegraphics[scale=0.905]{ludwig_boltzmann.jpg} & \\
    {\tiny \href{https://commons.wikimedia.org/wiki/File:Clausius.jpg}{\ccPublicDomain\ Public Domain}} & {\tiny \href{https://commons.wikimedia.org/wiki/File:Boltzmann2.jpg}{\ccPublicDomain\ Public Domain}} &  \\
    (c) & (d) & (e) \\
    \includegraphics[scale=0.5]{josiah_gibbs.jpg} & \includegraphics[scale=0.355]{claude_shannon.jpg} & \includegraphics[scale=0.222]{JohnvonNeumann.png} \\
    {\tiny \href{https://commons.wikimedia.org/wiki/File:Josiah_Willard_Gibbs_-from_MMS-.jpg}{\ccPublicDomain\ Public Domain}} & {\tiny \href{https://commons.wikimedia.org/wiki/File:ClaudeShannon_MFO3807.jpg}{CC BY-SA 2.0 de}} & {\tiny Â©   \href{https://www.lanl.gov/resources/web-policies/copyright-legal.php}{Los Alamos National Laboratory}$^{\dagger}$}\\
    & {\tiny Credit: Konrad Jacobs} &  {\tiny Credit: Los Alamos National Laboratory}\\
    \end{tabular}
    \caption{The founders of the modern notions of entropy. (a) Rudolf Clausius. (b) Ludwig Boltzmann. (c) Josiah Gibbs. (d) Claude Shannon. (e) John von Neumann (Hungarian name: Neumann JÃ¡nos Lajos).}
    

{\tiny \begin{singlespace}$^{\dagger}$Unless otherwise indicated, this information has been authored by an employee or employees of the Los Alamos National Security, LLC (LANS), operator of the Los Alamos National Laboratory under Contract No. DE-AC52-06NA25396 with the U.S. Department of Energy. The U.S. Government has rights to use, reproduce, and distribute this information. The public may copy and use this information without charge, provided that this Notice and any statement of authorship are reproduced on all copies. Neither the Government nor LANS makes any warranty, express or implied, or assumes any liability or responsibility for the use of this information. \end{singlespace}}
\end{figure}

Entropy was formalized as a function of the number of possible
microscopic states (i.e.~microstates) in the context of statistical
thermodynamics by Ludwig Boltzmann (Figure \ref{fig:foundersentropy}(b))
in his efforts to relate the average kinetic energy of gas particles to
the thermodynamic temperature of the gas \cite{Uffink2022}. Each
microstate represents the physical configuration of a part of a physical
system, such as the orbital states of electrons in atoms, that are
associated with an energy level.

\begin{quote}
\textbf{Definition}

\textbf{Boltzmann's entropy} equation is given by \begin{equation*}
S \triangleq k_B \ln \Omega
\end{equation*} where \(S\) is the (thermodynamic) entropy, \(k_B\) is
the Boltzmann constant taking the exact value of
\(1.380649 \times 10^{-23}\ \frac{\text{J}}{\text{K}}\) in units of
Joules-per-Kelvin, and \(\Omega\) is the number of equilibrium
microstates of a system.
\end{quote}

Definition \ref{def:boltzmannentropy} assumes that each microstate is
equally probable, which is the case when a system is already in
thermodynamic equilibrium. To describe the entropy of systems in which
not all states are equally probable, a generalization of Definition
\ref{def:boltzmannentropy} given in Definition \ref{def:gibbsentropy}
was developed by Josiah Gibbs (Figure \ref{fig:foundersentropy}(c))
(Jaynes (1965)).

\begin{quote}
\textbf{Definition}

Gibbs's entropy equation is given by \begin{equation*}
S \triangleq -  k_B \sum_i p_i \ln p_i
\end{equation*} where \(S\) is the (thermodynamic) entropy, \(k_B\) is
the Boltzmann constant as found in Boltzmann's entropy, and \(p_i\) is
the probability of the \(i\)th state from a finite set of physical
states.
\end{quote}

In 1948 entropy was generalized beyond the context of Physics by Claude
Shannon (Figure \ref{fig:foundersentropy} (d)) to a purely mathematical
construct that has applications in many domains. This generalization,
sometimes called Shannon-Wiener entropy, is given in Definition
\ref{def:shannonentropy}. Jon von Neumann (Figure
\ref{fig:foundersentropy} (e)) adapted this definition to the context of
quantum mechanics, which we do not explore further here.

\begin{quote}
\textbf{Definition} Shannon's entropy equation is given by
\begin{equation*}
S \triangleq -  K \sum_i p_i \ln p_i
\end{equation*} where \(S\) is the Shannon entropy, \(K\) is a
proportionality constant, and \(p_i\) is the probability of the \(i\)th
event from a discrete event space.
\end{quote}

The differences between Definition \ref{def:gibbsentropy} and Definition
\ref{def:shannonentropy} are simple, subtle, but also of great
importance. The first difference is explicit: we exchange the Boltzmann
constant \(k_B\) for an arbitrary proportionality constant \(K\). This
proportionality constant is often taken to be unity in applications of
Definition \ref{def:shannonentropy}, however in principle it could be
used to scale the log-linear factor onto a desired measurement scale.
The second difference is less explicit, but of great practical
significance: the probability measure can be over \emph{any} discrete
random variable.

Since entropy in a general sense is not about thermodynamic states
\emph{per se}, it is worth considering what it actually tells us about a
discrete random variable. Often entropy is heuristically described as a
``measure of disorder'', but this way of describing entropy has
limitations that are outlined in Lambert (2002). Among them are the
observations that maximal entropy distributions can generate ordered
structures, and that subjective disagreement between observers can occur
over what consistutes `disorder'.

Since entropy (specifically Shannon's entropy) has a specific
mathematical definition, it is worth considering one of its important
properties in the form Proposition \ref{prop:maxentropy}.

\begin{quote}
\textbf{Proposition}

Given a discrete random variable \(X\) with probability space
\((\Omega, \mathcal{F}, P)\), its Shannon entropy \(H(X)\) satisfies

\[H(X) \leq \log |\Omega| \]

where \$H(X) \leq \log \textbar{}\Omega\textbar{} \$ if-and-only-if
\(P\) is uniform.

\textbf{Proof}

See Theorem 2.6.4 in Cover and Thomas (2006).
\end{quote}

Proposition \ref{prop:maxentropy} gives us a specific way to think about
what Shannon entropy quantifies: uniformity of the distribution of a
discrete random variable. Abstracting entropy beyond the context of
thermodynamics and taking this precise notion that quantifies uniformity
will be important for understanding grade entropy.

Let us briefly consider an example of applying the mathematical concept
of entropy outside of thermodynamics that comes from ecology. In
ecology, a community is a collection of populations. In the analysis of
species abundance tables it is desirable to quantify the `diversity' of
a community of organisms.

\begin{tcolorbox}[enhanced jigsaw, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, left=2mm, arc=.35mm, bottomtitle=1mm, coltitle=black, breakable, rightrule=.15mm, toptitle=1mm, opacityback=0, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{What is a species abundance table?}, colframe=quarto-callout-note-color-frame, toprule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, colback=white]

A species abundance table is a matrix whose entries are the count of how
many times each species was observed in each sample.

\end{tcolorbox}

Entropy is one method of quantifying diversity if we consider diversity
in the sense of uniformity of species abundances. The species abundance
table gives us an empirical frequency distribution with discrete
support. Computing the Shannon entropy from this distribution tells us
how uniform the species abundances are in the community.

In summary we have noted that entropy was originally motivated and
developed within the context of physics, and was later generalized to
any discrete probability distribution. Rather than quantifying
`disorder' as a synonym for `macroscopic messiness', Shannon entropy
tells us about the uniformity of a discrete random variable.

\subsection{Grades and Partially-Ordered
Sets}\label{grades-and-partially-ordered-sets}

Before introducing grade entropies, we will briefly review the
properties of relations. All partial orders are relations. A relation is
a subset of a Cartesian product of sets. Given a collection of sets
\(S_1,\cdots, S_n\), a relation
\(R \subseteq S_1 \times S_2 \times \cdots \times S_{n-1} \times S_n\)
is said to be \emph{homogenous} if all pairs of sets are equal.
Otherwise such a relation is \emph{inhomogenous}. Relations are also
classified by the number of sets being used in the set multiplication. A
\emph{binary relation} is a subset of the Cartesian product of two sets,
and more generally an \emph{\(n\)-ary relation} is obtained from taking
a subset of the Cartesian product of \(n\) sets. In this section we
focus on binary relations. Beyond these distinctions, many relations are
classified by certain rules about which elements of a set are allowed to
be members of the relation.

The rows of Table \ref{tab:relationproperties} show some common types of
relations or relation properties including reflexivity, symmetry,
antisymmetry, asymmetry, transitivity, and connectedness. These
properties in certain combinations give partial orders, strict partial
orders, total orders, and strict total orders.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1538}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.3497}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1049}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1538}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1469}}@{}}
\caption{Summary of common types of order relations. The listed
properties are assumed to hold for all \(x, y, z\) as needed from a set
\(S\) whose binary relation \(R\) is a subset of
\(S \times S\).}\label{tbl-relation-properties}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Property
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Expression
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Partial Order
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strict Partial Order
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Total Order
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strict Total Order
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Property
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Expression
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Partial Order
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strict Partial Order
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Total Order
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strict Total Order
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Reflexive & \(xRx\) & â & & â & \\
Irreflexive & \(\neg x R x\) & & â & & â \\
Symmetric & \(x R y \implies y R x\) & & & & \\
Antisymmetric & \(x R y \land y R x \implies x = y\) & â & & â & \\
Asymmetric & \(x R y \implies \neg y R x\) & & â & & \\
Transitive & \(x R y \land y R z \implies x R z\) & â & â & â & â \\
Connected & \(x \neq y \implies x R y \lor y R x\) & & & & â \\
Strongly Connected & \(x R y \lor y R x\) & & & â & \\
\end{longtable}

The familiar comparisons \(a < b\) and \(a \leq b\) with real numbers
make clear that orders exist in one dimension. But in this work we are
interested in orders over sets of multidimensional points. One approach
is to find a map \(f:\mathbb{R}^n \mapsto \mathbb{R}\) and compare
\(f(\vec{x}) < f(\vec{y})\) for any
\(\vec{x}, \vec{y} \in \mathbb{R}^n\). A common choice for \(f\) is a
norm, such as the Euclidean norm. Beyond such function-induced orders,
three possibilities include product orders, Pareto orders, and
lexicographical orders.

A \textit{product order} is satisfied by the componentwise comparisons
all being satisfied. For vectors \(\vec{x}, \vec{y} \in \mathbb{R}^n\),
they satisfy a strict product order if \(x_i < y_i\) for all
\(i \in \{1, \cdots, n \}\). Similarly, they satisfy a non-strict
product order if \(x_i \leq y_i\) for all \(i \in \{1, \cdots, n \}\).

A \textit{Pareto order} does not have a strict or non-strict form
because it already requires both strict and non-strict componentwise
comparisons. For vectors \(\vec{x}, \vec{y} \in \mathbb{R}^n\), they
satisfy a Pareto order if \(x_i \leq y_i\) for all
\(i \in \{1, \cdots, n \}\) \textbf{and} there exists
\(j \in \{1, \cdots, n \}\) such that \(x_j < y_j\). A Pareto order is
more restrictive than a non-strict product order, but less restrictive
than a strict product order, in the sense of what fraction of an
arbitrary finite set would satisfy the relation. Pareto orders are
applied in finance (Amershi (1985)), game theory (Aubin (2014)), and
multiobjective optimization (Emmerich and Deutz (2018)).

A \emph{lexicographical order} is a kind of ordering that prioritizes
comparing certain components before others. An example of a
lexicographical order is the use of an alphabet to order words. The
alphabet itself is an order on the characters, while the lexicographical
order entails looking to compare the first two letters, then the second
two, etc. As an example, suppose we had the points \(a = (2,1)\) and
\(b=(1,3)\). Under a lexicographical ordering it holds that \(b < a\)
because the first component is compared first, and further components
are irrelevant beyond the ealiest component that breaks the tie between
the points. Lexicographical orders can be strict or non-strict, and are
frequently used to define the conditions under which a collection of
strings are sorted.

Next we will give some background to the notion of grades (i.e.~ranks)
on points based on a given partial order.

\subsection{Graded Posets}\label{graded-posets}

For the construction of the notion of grade entropies we make use of
grades of points. Given a finite collection of multidimensional points,
which may possibly be a statistical sample from a population, we can use
a partially ordered set (i.e.~a poset) to assign a rank to each point. A
point dominanted by no other point would have a rank of 1, a point
dominated by one point would have a rank of two, and a point dominated
by \(k\) number points would have a rank of \(k+1\).

\begin{tcolorbox}[enhanced jigsaw, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, left=2mm, arc=.35mm, bottomtitle=1mm, coltitle=black, breakable, rightrule=.15mm, toptitle=1mm, opacityback=0, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, toprule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, colback=white]

This description of assigning ranks according to point dominance is
sufficient for what we later term a \emph{pseudograde} function, and
that a covering relation provides the constraint needed to obtain a
\emph{grade} function.

\end{tcolorbox}

Before defining a graded partially ordered set (i.e.~a graded poset), it
is convenient to define the notion of a covering relation as given in
Definition \ref{def:coveringrelation}.

\begin{quote}
\textbf{Definition} (Stanley (2011))

Suppose \(S\) is a set with a partial order \(\leq\), and strict partial
order \(<\) that holds whenever \(x \leq y \land x \neq y\).

The \textbf{covering relation}, denoted \(x \lessdot y\) for
\(x,y \in S\), holds if \(x < y\) and there does not exist \(z \in S\)
such that \(x < z < y\).
\end{quote}

A covering relation captures an intuitive notion that certain elements
are ``beside each other'', which is closely related to the notion of a
successor function. A successor function sends a natural number to the
next natural number by incrementing by unity, while the predecessor
function sends a natural number to the previous natural number by
subtracting unity. For a graded poset (Definition \ref{def:gradedposet})
it is assumed that if two elements are beside each other in the covering
relation, then their grades should correspondingly be predecessors or
successors of each other.

\begin{quote}
\textbf{Definition} (Stanley (2011))

Let \(S\) be a partially ordered set equipped with a rank function
\(\rho: S \mapsto \mathbb{N}\), where \(\rho\) satisfies the following:

\begin{itemize}
\item $x,y \in S \land x < y \implies \rho(x) < \rho(y)$
\item $x,y \in S \land x \lessdot y \implies \rho(x) + 1 = \rho(y)$
\end{itemize}

where \(\lessdot\) is a covering relation on \(S\). Such a partially
ordered set is called a \underline{graded poset} or
\textit{graded partially ordered set}.
\end{quote}

What Definition \ref{def:gradedposet} provides is a clear notion of
grading a collection of points under the assumption of a partial order,
that these grades preserve the order, and preserve closeness in the
sense of a covering relation.

With these notions in place, we will now construct the notion of a grade
entropy.

\subsection{Grade Entropies}\label{grade-entropies-1}

Grade entropies are about quantifying the totality of a partial order or
a strict partial order. Traditionally a given order relation is total,
or it isn't total, with no consideration of how close a relation is to
being total. But we suggest that it is desirable to quantify how far
short a partial order falls from being a total order, which we
informally refer to here as ``totality''

\begin{tcolorbox}[enhanced jigsaw, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, left=2mm, arc=.35mm, bottomtitle=1mm, coltitle=black, breakable, rightrule=.15mm, toptitle=1mm, opacityback=0, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, toprule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, colback=white]

In this work we take an information theory approach to quantifying
totality, but it may also be possible to approach this notion with
metric spaces.

\end{tcolorbox}

\begin{quote}
\textbf{Definition}

Suppose that \(S \subset   \mathbb{R}^n\) is a finite set of points of
size \(|S|=m\) that is also a graded poset. With a grade function \(g\),
and probability mass function \(p\) over \(g\) we define the
\underline{grade entropy} to be

\[H_g \triangleq - \sum_{i=1}^{k} p(g_i)  \log_b  p(g_i) \]

where \(b\) is a chosen base, \(g_i\) is a distinct grade assigned to
any element of a partition \(S_i\) of \(S\), and \(k\) is the number of
distinct grades assigned to elements of \(S\).
\end{quote}

It may be desirable to compare grade entropies computed on different
systems, but the non-negative real number that results from computing
grade entropy in Definition \ref{def:gradeentropy} depends on \(m\). We
can utilitize Proposition \ref{prop:maxentropy} to define a normalized
grade entropy as given in Definition \ref{def:normalizedgradeentropy}.

\begin{quote}
\textbf{Definition} Given a grade entropy function \(H_g\), the
\textbf{normalized grade entropy} is given by

\[H_{\gamma} \triangleq \frac{H_g}{\log_b |m|}\]

where \(b\) is the same base used in \(H_g\) and \(m\) is the number of
points
\end{quote}

The normalization in Definition \ref{def:normalizedgradeentropy}
provides a functional with similar properties to the grade entropy in
Definition \ref{def:gradeentropy}, but with the additional property of
being bounded to the interval \([0,1]\). This allows comparisons of
grade entropies for variables with different sizes of outcome space, and
also makes it clear that the entropy has reached its maximum when the
normalized grade entropy is equal to unity.

Note that Definition \ref{def:gradeentropy} does not suppose what
partial order is used, nor does it suppose which probability
distribution is used. One might wish to use product orders,
lexicographical orders, Pareto orders, or others. And one might wish to
consider either empirical or theoretical discrete probability
distributions. Grade entropies allow the researcher to choose the order
relation and probability distribution that is relevant to their chosen
domain.

One limitation of grade entropies is they do not distinguish between
partially ordered sets with slightly different non-dominating pairs. Let
us consider panel (b) of Figure \ref{fig:examplegradeentropy} as an
example. Nodes \texttt{e} and \texttt{d} would be assigned equal grades,
and nodes \texttt{c} and \texttt{b} would also be assigned equal grades.
If the edge \texttt{e}\(\rightarrow\)\texttt{b} was also part of the
relation this would not change the aforementioned grades, but sometimes
it may be desirable to distinguish between these two versions of the
lattice. This motivates a generalization of grade entropies that we term
\textit{pseudograde entropies}, which is achieved by first defining a
\emph{pseudograded poset} by taking Definition \ref{def:gradedposet} and
simply dropping the requirement that
\(x,y \in S \land x \lessdot y \implies \rho(x) + 1 = \rho(y)\). The
definition of a pseudograde entropy is then identical to Definition
\ref{def:gradeentropy} except that a pseudograde function is used
instead of a grade function.

\begin{tcolorbox}[enhanced jigsaw, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, left=2mm, arc=.35mm, bottomtitle=1mm, coltitle=black, breakable, rightrule=.15mm, toptitle=1mm, opacityback=0, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, toprule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, colback=white]

A normalized pseudograde entropy is similarly obtained *mutatis
mutandis.

\end{tcolorbox}

Next we consider how to interpret what a grade entropy tells us about a
finite set of points.

\subsection{Interpretation}\label{interpretation-4}

In this subsection we offer a general interpretation of grade entropies
that should apply regardless of the choice of partial order or
distribution of grades.\textbackslash{}

Proposition \ref{prop:maxentropy} entails that the grade entropy is
maximized when the distribution of grades is uniform. If the grade
entropy is maximal for a collection of \(m\) points, then the
probability measure for each grade is \(\frac{1}{m}\) for each of the
\(m\) points. If we take our probability to be a normalized counting
measure over the set of points, then we must conclude that there is
exactly one point per grade. If every point has a distinct grade, then
we can equivalently state that no two points share a
grade.\textbackslash{}

If no two points share a grade, then the partial order is also a total
order. With the grade entropy being maximized by this uniformity, it can
be considered a quantification of the totality of the partial order.
Example \ref{fig:examplegradeentropy} illustrates three cases including
one in which the partial order is also total, a case where it is not
completely total but also has some degree of totality, and a third case
in which there is no totality (i.e.~no dominance of one point over
another).

\begin{figure}[H]
\begin{tabular}{c|c|c}
(a) & (b) & (c) \\
\includegraphics[scale=0.3]{./partial_total_order/totalorder.gv.pdf} & \includegraphics[scale=0.3]{./partial_total_order/partialorder.gv.pdf} &  \includegraphics[scale=0.3]{./partial_total_order/noorder.gv.pdf} \\
\includegraphics[scale=0.3]{./partial_total_order/hist_uniform.pdf} & \includegraphics[scale=0.3]{./partial_total_order/hist_partialorder.pdf} & \includegraphics[scale=0.3]{./partial_total_order/hist_nonorder.pdf} \\
\includegraphics[scale=0.3]{./partial_total_order/pseudo_hist_uniform.pdf} & \includegraphics[scale=0.3]{./partial_total_order/pseudo_hist_partialorder.pdf} & \includegraphics[scale=0.3]{./partial_total_order/pseudo_hist_nonorder.pdf} \\
\end{tabular}
\caption{Informal drawings of lattices representing partially ordered sets, a corresponding distribution of grades below in blue, and a corresponding distribution of pseudogrades at the bottom in red. (a) Total order in which the normalized grade entropy and normalized pseudograde entropy is equal to one. (b) Non-total partial order with a normalized grade entropy  and normalized pseudograde entropy not equal to either zero or one. (c) Non-total partial order with a grade entropy and pseudograde entropy equal to zero.}

\end{figure}

We can further interpret grade entropies in a way that depends on the
choice of order. Let us consider a product order over a finite
collection of points in \(\mathbb{R}^n\) as an example. If the grade
entropy is zero, then for every pair of points there must exist at least
one component that violates the comparison. Thus a grade entropy of zero
for such a product order tells us that the collection of variables is
not comonotonic. A collection of variables are comonotonic if they go up
or down together whenever one them goes up or down in
value.\textbackslash footnote\{When there exists two variables in which
one always decreases as the other increases, and \emph{vice versa}, this
is called \emph{antimonotonicity}. We are not aware of a term for the
case in which some variables within a collection of variables are
comonotonic while others are antimonotonic, but we suggest the term
\emph{anticomonotonic} suitably suggests this mixture. Likewise, if the
grade entropy is at its maximum then the product order holds for all
pairs of points, which entails perfect comonotonicity among those
variables. Intermediate values of grade entropy between zero and its
maximum would suggest to us a quantification of comonotonicity of a set
of variables.

Comonotonicity is related to the Trinity of Covariation when we consider
a partial order on points whose components are variables. If the order
is total, then the variables must go up and down together in such a way
that always satisfies the order relation. Likewise, when the order is
entirely non-total the variables never go up and down together in a way
that satisfies the order. The \emph{change} part is implicit in
considering the point-to-point comparisons for all points. The
\textit{coordination} part comes from such points be specified by
coordinates represented by the components of each
tuple.\footnote{The notions of grade entropy and pseudograde entropy do not actually *require* that the partial order in consideration be on a set of multidimensional points. Our discussion in terms of points with components is primarily with a view toward analyzing a data matrix, but it is sufficient to construct an order lattice by other means.}
The \emph{structure} part comes from the notion of a partial order
existing on the collection of points.

Interpreting pseudograde entropies is similar to interpreting grade
entropies. It holds that a total order maximizes its value, and if no
point dominates another then it will take its minimum value of zero. But
pseudograde entropies may be larger than their grade entropy counterpart
for a given partially ordered set.

Having introduced grade entropies, pseudograde entropies, and their
interpretation, we will next summarize the chapter.

\section{Conclusion}\label{conclusion}

In this chapter we introduced multilinear correlation coefficients,
Nightingale covariance, inner correlations, Agnesian operators, and
grade entropies.

Each of these families of functions are an instantiation of the Trinity
of Covariation in its own way. Mulilinear correlation relies as
averaging over multiplications of random variables, and make use of a
recently-proved normalization that generalized the Cauchy Schwarz
inequality. Nightingale correlation is similar to multilinear
correlations, but imposes non-negativity and have similar properties to
seminorms and semimetrics. Inner correlations utilize a generalization
of the notion of inner products to multiple variables by constructing
normalized Gram determinants. Agnesian operators make direct use the
notions of change as either derivatives or net changes and has
applications to the notions of parametric curves and surfaces in finite
dimension. Lastly, grade entropies combine elementary notions of
information theory and order theory.

With these mathematical notions in mind, in the next chapter we
introduce a software package that implements their calculation.

\bookmarksetup{startatroot}

\chapter{ConAction}\label{conaction}

We believe that the mathematical functions developed in this thesis are
far more likely to be used if there exists a software package that
implements them. This is the purpose of
ConAction\footnote{The name of this package was motivated by the prefix \textit{con-} being taken to mean "with" or "together", and *action* being a verb that entails change. Thus *ConAction* is an allusion to the Trinity of Covariation.},
a Python package we developed as part of this thesis. We chose the
Python programming language for its rapid development times, package
management system, mathematical libraries, portability, and support for
performance and feature enhancements. In this package the user will find
tools for computing instances of the Trinity of Covariation on data
matrices, numerically on Python functions, or symbolically on computer
algebra expressions. This diversity of implementations facilitates
ConAction being useful to both theoreticians and practitioners.

In this chapter we cover the installation of ConAction, its
dependencies, the main features, our testing approach, the algorithmic
complexity of the implemented algorithms, some run time performance
analysis, our approach to documentation of ConAction, and our plans for
future support.

\section{Installation}\label{installation}

In order to reduce the technical skills required to set up ConAction, it
must be easy to install. This is really important because many users
give up on using a package if it requires considerable configuration. We
also wanted the installation process to be the same on any platform. For
these reasons, we use the Python package management system known as Pip.
Pip makes it easy to install packages from the Python Package Index
(PyPI (2003)). Figure \ref{fig:installation} shows how ConAction can be
installed with a single command.

\begin{figure}[H]
\SU{user=user,host=ubuntu,color=lime}
\begin{ubuntu}
pip install conaction `\StartConsole`
Collecting conaction
  Using cached conaction-<Version Number>-py3-none-any.whl (<Package Size>)
Installing collected packages: conaction
Successfully installed conaction-<Version Number> `\SU{root}`
\end{ubuntu}
\caption{Example installation of ConAction on a Ubuntu system using PIP, a popular package manager tool for Python.}
\label{fig:installation}
\end{figure}

Because Pip is available on MacOS, Windows, and various distributions of
Linux, the command given in Figure \ref{fig:installation} can be run on
practically any desktop. Thus this approach to installation is simple to
perform and cross-platform.

\section{Dependencies}\label{dependencies}

ConAction has dependencies in order to avoid reinventing solutions to
solved problems, and to reduce development time. The main submodules of
ConAction and their dependencies are given in Figure \ref{fig:dep_tree}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{./dependency_tree/Digraph.gv.pdf}
\caption{Dependencies of the ConAction package. Ellipses are submodules of the ConAction package, and rectangles are third party libraries. Two nodes share an arc if the tail node is required by the head node.}
\label{fig:dep_tree}
\end{figure}

The supported submodules \texttt{numparam}, \texttt{estimators},
\texttt{sampling}, and \texttt{symparam} all depend upon \texttt{numpy}
for either numerical and/or array operations. The \texttt{scipy} package
provides the singular values of a correlation matrix and integration
techniques in \texttt{numparam} as well as data ranking functions
\texttt{estimators}. The \texttt{tqdm} package provides progress bars
for some of the functions found in \texttt{estimators} and
\texttt{sampling} that potentially take a long time. The \texttt{pathos}
package is used for in \texttt{numparam} and \texttt{estimators} to
parallelize certain loops (McKerns and Aivazis (2010)). We preferred the
\texttt{pathos} package over the \texttt{multiprocessing} package in the
standard Python library because it uses \texttt{dill} instead of
\texttt{pickle} to serialize objects, and \texttt{dill} covers a wider
range of cases including nested functions (McKerns et al. (2011)). The
\texttt{networkx} package is used in computing grade entropies by
providing graph algorithms and data structures related to the lattice
representation of a partial order.

\section{Features by Module}\label{features-by-module}

The ConAction package is organized into multiple sub-modules to support
different features (Figure \ref{fig:conactionmodules}). The same
function names are used for a given mathematical function in different
modules to make it easier for users to remember which command they want.
But this also means that users of ConAction must also adhere quite
strictly to the instructions in the Python Enhancement Proposal version
8 (PEP8) to avoid wildcard imports of the form
\texttt{from\ \textless{}module\textgreater{}\ import\ *} because names
can be overwritten in the namespace (Rossum, Warsaw, and Coghlan
(2001)). Rather we intend for users to import a submodule of ConAction,
and use dot notation to specify the function they wish to use.

\begin{figure}[H]
    \smartdiagramset{
    set color list={UNBCGreen, UNBCGreen, UNBCGreen, UNBCGreen},
    description title text width={1cm},
    description title width={1cm},
    descriptive items y sep={1.5cm},
    description text width={9cm},
}
    \centering
    \smartdiagram[descriptive diagram]{
  {\tiny estimators, {Statistical estimators on data matrices.}},
  {\tiny numparam, {Numerical integration and differentiation methods.}},
  {\tiny symparam, {Symbolic integration and differentiation methods.}},
  {\tiny sampling, {Statistical sampling and search functions.}},
  }
  \caption{The sub-modules of the ConAction package that partition different features.}
  \label{fig:conactionmodules}
\end{figure}

The \texttt{estimators} module contains many of the functions that can
be computed on data matrices (i.e.~tabular data), and thus can be
thought of as statistical estimators for most intents and purposes.
Table \ref{tab:conactionfunctions} is a summary of the available
functions in this submodule.

\begin{table}[H]
    \centering
    \caption{Functions featured in the three main modules of the ConAction package.}
    \scriptsize
    \begin{tabular}{llccc}
    \hline
    Function & Command & \texttt{estimators} & \texttt{numparam} & \texttt{symparam} \\
    \hline
    Mean & \texttt{mean} & \cmark & \cmark & \cmark \\
    Nightingale deviation & \texttt{nightingale\_deviation} & \cmark & \cmark & \cmark \\
    Nightingale covariance & \texttt{nightingale\_covariance} & \cmark & \cmark & \cmark \\
    Nightingale correlation & \texttt{nightingale\_correlation} & \cmark & \cmark & \cmark \\
    Standard deviation & \texttt{standard\_deviation} & \cmark & \cmark & \cmark  \\
    Multilinear covariance & \texttt{covariance} & \cmark & \cmark & \cmark  \\
    Multilinear Pearson correlation & \texttt{pearson\_correlation} & \cmark & \cmark & \cmark  \\
    Multilinear reflective correlation & \texttt{reflective\_correlation} & \cmark & \cmark & \cmark  \\
    Multilinear circular correlation & \texttt{circular\_correlation} & \cmark & \cmark & \cmark  \\
    Multilinear signum correlation & \texttt{signum\_correlation} & \cmark & \cmark & \cmark  \\
    Misiak correlation & \texttt{misiak\_correlation} & \cmark & \cmark & \cmark  \\
    Tren\v{c}evski-Mal\v{c}eski correlation & \texttt{trencevski\_malceski\_correlation} & \cmark & \cmark & \cmark  \\
    Partial Agnesian & \texttt{partial\_agnesian} & \cmark$^\dagger$ & \cmark & \cmark  \\
    Partial Multiagnesian & \texttt{partial\_multiagnesian} & \xmark & \cmark & \cmark  \\
    Grade Entropy & \texttt{grade\_entropy} & \cmark$^\ddag$ & \xmark & \xmark \\
    Pseudograde Entropy & \texttt{pseudograde\_entropy} & \cmark$^\ddag$ & \xmark & \xmark \\
    \hline
    $\dagger$ Non-negative order only. \\
    $\ddag$ Strict product order only. \\
    \end{tabular}
    
\end{table}

The example in Figure \ref{code:estimatorexample} shows how the
multilinear Pearson correlation coefficient can be estimated from data.
Any of the functions in Table \ref{tab:conactionfunctions} can be called
in a similar fashion. \textbackslash{}

\begin{figure}[H]
\tiny
\centering
\includegraphics[scale=0.8]{./code_example_boxes/estimators/estimator_example.pdf}
\caption{Code example of how to estimate multilinear Pearson correlation using the \texttt{estimators} submodule.}
\label{code:estimatorexample}
\end{figure}

The \texttt{numparam} submodule contains similar functions to
\texttt{estimators}, but the purpose and scope differs. While
\texttt{estimators} contains estimators to be computed on data matrices,
whereas \texttt{numparam} submodule assumes that a mathematical
\emph{function} is known and that numerical integration of the function
is desired.

\begin{figure}[H]
\tiny
\centering
\includegraphics[scale=0.8]{./code_example_boxes/numparam/numparam_example.pdf}
\caption{Code example of how to estimate multilinear Pearson correlation using the \texttt{numparam} submodule.}
\label{code:numparamexample}
\end{figure}

The \texttt{symparam} submodule is most similar to the \texttt{numparam}
submodule in its scope. But rather than numerically integrating the
statistic as is done in \texttt{numparam}, \texttt{symparam} computes
the integrals symbolically.

\begin{figure}[H]
\tiny
\centering
\includegraphics[scale=0.8]{./code_example_boxes/symparam/symparam_example.pdf}
\caption{Code example of how to estimate multilinear Pearson correlation using \texttt{symparam} submodule.}
\label{code:numparamexample}
\end{figure}

At this time both \texttt{numparam} and \texttt{symparam} assumes a
uniform probability measure, however future development will look at
allowing user-specified probability measures.\textbackslash{}

The \texttt{sampling} module allows contains resampling methods used in
Chapter \ref{chap:mtnpinebeetle}.\textbackslash{}

Next we will consider how ConAction has been tested.

\section{Testing}\label{testing}

Software testing is an important part of ensuring software quality. Due
to the mathematically precise nature of the features of ConAction, it is
readily testable by setting up function inputs and checking whether the
correct (usually numerical) output is obtained.

The Python standard library includes the \texttt{doctest} module which
served as the primary mode of testing this first release of ConAction.
Every function in the Conaction code base has a docstring under the
function name, including an example input and output. The
\texttt{doctest} module can be imported, and running
\texttt{doctest.testmod()} function will go through every such example
for every function to check if the correct output is given.

As unforeseen cases are found by users, this will require further
testing outside the scope of the examples used in the documentation
strings. Future testing will make use of the \texttt{unittest} module
which is built-in to the standard Python library to perform unit tests.

Thus ConAction has undergone basic testing to ensure a minimal viable
product, but can be further tested when unforeseen cases arise.

\section{Algorithmic Complexity}\label{algorithmic-complexity}

In this section we will overview the theoretical scalability of the
algorithms implemented in ConAction.

\subsection{Estimators}\label{estimators}

The \texttt{estimators} submodule has numerous functions that compute
expectation operators. When computed on a sample, we took any
expectation \(\mathbb{E}[U]\) of a random variable \(U\) to be estimated
by

\[\bar{u} = \frac{1}{m} \sum_{i=1}^m u_i\]

where \(\bar{u}\) is the sample mean of a sample
\(\{ u_1, \cdots, u_m \}\). Computing such an expectation requires
\(\mathcal{O}(m)\) time and space, but since this is often computed for
each variable in a collection of \(n\) random variables the complexity
often comes to \(\mathcal{O}(mn)\) time. While multilinear correlations
and Nightingale's correlation involve more operations than just
computing a sample average for each variable, none of them change the
resulting complexity when computing using
BEDMAS\footnote{Recall that BEDMAS is an abbreviation for "\textbf{b}rackets-then-\textbf{e}xponents-then-\textbf{d}ivision-then-\textbf{m}ultiplication-then-\textbf{a}ddition-then-\textbf{s}ubtraction".}
order of operations.\textbackslash{}

In computing the Misiak correlation and Tren\v cevski-Mal\v ceski
correlation we always took the inner product to be the dot product when
computed in the \texttt{estimators} submodule. Most of the steps of
computing these coefficients are also in \(\mathcal{O}(mn)\) of time
which is due to computing the inner products of all pairs of variables
to obtain a Gram matrix. But the overall time complexity will be that of
the algorithm used to compute the determinant of the Gram matrix. We
used the \texttt{numpy.det} function which computes the determinant
using a \(\mathcal{O}(n^3)\) algorithm based on performing a LU
decomposition.\footnote{A LU decomposition is a matrix factorization of matrix $A$ satisfying $A=LU$ where $L$ is a lower triangular matrix and $U$ is an upper triangular matrix.}\textbackslash{}

Computing the partial Agnesian of non-negative order \(k \geq 0\) has a
complexity in terms of the sample size \(m\), the number of variables
\(n\) (not inlcuding the parameter \(t\)), and the order \(k\). Instead
of computing the partial derivatives \textit{per se}, they are
approximated using

\[\frac{\partial x_j(t)}{\partial t} \approx \frac{x_{i,j} - x_{i-1,j}}{t_i - t_{i-1}}\]

where \(j \in \{ 1, \cdots, n \}\) and \(i \in \{ 1, \cdots, m \}\). It
requires \(\mathcal{O}(m)\) time to compute this for a single variable,
and consequently requires \(\mathcal{O}(mn)\) time for all the
variables. Repeating this calculation \(k\) times suggests that the
overall time complexity is \(\mathcal{O}(mnk)\).\textbackslash{}

Calculating grade entropy involves comparing each point to each other
point, suggesting at least \(\mathcal{O}(m^2)\) time complexity is
required, to construct a directed acyclic graph (DAG) of which points
dominate other points. But since each point has \(n\) components to be
compared in a componentwise fashion, the time complexity for
constructing the DAG is actually \(\mathcal{O}(m^2n)\). We then perform
a transitive reduction on the constructed DAG to convert it to a lattice
of the partial order. \cite{Aho1972} showed that the time complexity of
performing a transitive reduction of a DAG requires the same complexity
as a matrix multiplication, which we can assume to be somewhat better
than \(\mathcal{O}(m^3)\) depending on the implmentation. Our
implementation of grade entropy uses the
\texttt{networkx.transitive\_reduction} function. At first glance the
source code for \texttt{networkx.transitive\_reduction} has only two
nested for loops, but within the inner loop there exists a set
comprehension in place of a third for loop. Next a topological sort is
performed, which requires \(\mathcal{O}(|V| + |E|)\) where \(V\) and
\(E\) are the vertex and edge sets of the lattice, respectively
\cite{Kahn1962}. We know that \(|V| = m\) and that
\(|E| \leq |V \times V| = m^2\), suggesting that we are adding no more
than \(\mathcal{O}(m^2)\) which is already a factor of
\(\mathcal{O}(m^2n)\) and is dominated by the \(\mathcal{O}(m^3)\) time
complexity of computing a transitive reduction. Thus the topological
sort in our implementation does not add to the complexity. The remaining
steps of looping over the nodes in topological order, counting the
distinct grades, and computing the entropy are all linear complexity or
dominated by linear complexity, and thus do not change the time
complexity in terms of either \(m\) or \(n\).\textbackslash{}

The algorithm we implemented for computing a pseudograde entropy based
on a strict product order has a better time complexity than that of our
grade entropy implementation because it does not require performing a
transitive reduction. Similarly to grade entropy, every point is
compared to each other point in a componentwise fashion giving a
\(\mathcal{O}(m^2n)\) time complexity in order to occupy an
\(m \times m\) adjacency matrix. Summing this matrix along one of the
index sets provides an array of the pseudogrades of the points. Counting
the distinct pseudogrades from this array, normalizing them into
probabilities, and computing the entropy all take linear-or-better time
and therefore do not further change the complexity.

\subsection{NumParam and SymParam}\label{numparam-and-symparam}

The complexity of the functions computed in \texttt{numparam} and
\texttt{symparam} are more difficult to consider than those in the
\texttt{estimator} module because (possibly non-constant) functions are
considered rather than a data matrix.

Each function scales linearly in the number of functions it takes as
input just as its counterpart in the \texttt{estimators} submodule
scales linearly in the number of variables. But since we are no longer
considering a sample of size \(m\) but rather a collection of functions
either at a point or over an interval, the scalability in terms of \(m\)
does not apply. In the case of the Agnesian operators, both
differentiation and integration are considered whereas the various
correlation coefficients all require integration.

We refer the interested reader to \texttt{scipy.integrate.quad} in the
SciPy documentation for further reading of how the integration in
\texttt{numparam} works, and to the the SymPy documentation for
information about how integrals are symbolically computed in
\texttt{symparam}.

\section{Performance}\label{performance}

Run time performance tests were used to ensure that (1) the code ran in
a reasonable amount of time, and (2) compare to a pure Python
implementation that we called ``naÃ¯ve'' in that it did not utilize
certain optimization strategies. Putting aside the choice of algorithm,
there were two main approaches we took to implementing functions in
ConAction to improve the run time performance.

The first was to avoid dynamic typing when it is safe to assume static
types. This is exemplified by Figure \ref{fig:intextperformance}(a)
where the multilinear correlation coefficient was computed on increasing
sample sizes with either dynamic typing (\texttt{naive.py}) or static
typing (\texttt{estimators.py}). Using static typing can provide an
order of magnitude or more improvement in rune times, which is used
heavily as a strategy for improving the performance of functions in
\texttt{estimators}.

The second strategy to improve performance was to parellelize any nested
loops. We chose not to parallelize every function because many of them
would run slower due to the process (i.e.~thread) management overhead.
In the case of the \texttt{pseudograde\_entropy} there exists a nested
for loop, and parallelizing the inner loop had a marked improvement in
performance for large input sizes. Figure \ref{fig:intextperformance}(b)
illustrates two effects of changing the number of threads working on
computing the pseudograde entropy using the
\texttt{pseudograde\_entropy} function. The first effect is that there
is increasing overhead as the number of threads increases, which is most
evident for the smaller sample sizes where using fewer threads was
actually faster. The second effect is the improved scalability of using
more threads, which can be inferred from Figure
\ref{fig:intextperformance}(b) by noticing that using 64 threads gave a
nearly flat response to increasing the sample size up to \(10^4\) from
\(2\) whereas using 1 or 2 threads had a noticable increase in runtime
by an order of magnitude.

\begin{figure}[H]
    \centering
    \begin{tabular}{cc}
    (a) & (b) \\
 \includegraphics[scale=0.45]{./performance_tests/est_vs_naive_runtime_pearson_correlation_8_10000.pdf} & \includegraphics[scale=0.45]{./performance_tests/est_vs_naive_runtime_grade_entropy_8_10000.pdf} \\
    \end{tabular}
    \caption{Example run time performance tests showing time requirements of computing statistics at increasing sample size. 'Na\"ive' implementation run times are plotted to compare performance. (a) Multilinear Pearson correlation coefficient computed on 8 Gaussian variables. (b) Pseudograde entropy under a strict product order on 8 Gaussian variables. Non-na\"ive computations were tested using 1, 2, and 64 threads.}
    
\end{figure}

Other minor improvements were made in the implementation. For example,
multilinear correlation coefficients and Nightingale's correlation
coefficient involve computing \(n\)th roots in their denominator, which
were computed only once by taking advantage of the distributive property
of multiplication that entails
\(\sqrt[n]{\prod_{j=1}^n x_j} = \prod_{j=1}^n \sqrt[n]{x_j}\). This does
not affect the algorithmic complexity classes these functions have as
the sample size or number of variables increases, but it is expected to
slightly improve run time and numerical precision.

There are also optional ways to improve the performance of ConAction
either by using auxillary packages or by changing the data type of the
input data.

Because the \texttt{estimators} makes heavy use of the NumPy package, we
can use the Numba package which is designed to interroperate with NumPy
to increase its performance. Numba provides options for parallelism,
just-in-time compiling, and ahead-of-time compiling. Numba is not a
dependency for ConAction because at the time of writing the latest
version of Numba was not compatible with the latest version of NumPy
under Python 3.10, but this may change in the future.

We can also make some adjustments for when the input data matrix is very
large and/or sparse. The \texttt{numpy.memoryview} function can be used
to create a virtual memory representation of an array that can be used
like any ordinary NumPy array, including as a data matrix in ConAction.
Using virtual memory involves using hard drive space, so the increase in
apparent memory comes at the cost of run time because of the added
input-output (IO) between memory and the hard drive. Likewise, the
\texttt{scipy.sparse} submodule of SciPy contains classes for sparse
matrices that can also be used like ordinary NumPy arrays in ConAction,
which can dramatically improve the effective short-term storage space.

In this section we discussed how ConAction has some built-in performance
enhancements, and the availability of tools for increasing performance
when coding large or sparse data matrices.

\section{Documentation}\label{documentation}

While being easy to install and use are essential requirements for any
scientific computing package, it is likewise important that the software
package is well-communicated. One of the ways that a software package
can be communicated is through effective documentation.

In this thesis we used two forms of documentation in a integrated way
thanks to modern package development tools.

The first for was to include documentation strings within every function
and class definition so that programmers inspecting the code can
understand the gist of what is entailed in the code. This form of
documentation can be found throughout the original source files if
inspected, however calling the built-in Python \texttt{help} function
will also display these documentation string.

The second form of documentation was generated from the former by a
combination of tools including Sphinx and Autodoc, which allowed us to
automatically generate HTML documentation based on a combination of the
documentation strings we wrote and some additional restructured text
files we provided. This improves the maintainability of the software by
ensuring that changes need to only be made in one or two places when a
change to the source code is made. The documentation is hosted at
\texttt{readthedocs.io}, but it can also be built locally by going to
the \texttt{/conaction/docs} path and calling the \texttt{make html}
command.

\section{Future Support}\label{future-support}

We will continue to support the ConAction Python package through its
GitHub repository after the completion of this thesis. This includes
addressing issues raised by users about the correctness and performance
of the code, considering feature requests, and keeping the package
compatible with supported versions of dependencies.

\section{Conclusion}\label{conclusion-1}

ConAction is a Python package which provides researchers and data
analysts with functions for computing various notions of coordinated
change in structures. While ConAction is not yet a mature
software\footnote{A mature software is a software that has been used for a long enough period of time that most of its initial faults have been removed or reduced by further development.}
due to only recently being released to the public, it has undergone
testing for performance and correctness, it is documented, and has a
framework for further development and support.

The starting set of features in ConAction it suitable for supporting
simulation studies, empirical research, and theoretical research.

\bookmarksetup{startatroot}

\chapter{Correlational Sufficiency of an Isolation by Distance Model of
Mountain Pine Beetle in Western North
America}\label{correlational-sufficiency-of-an-isolation-by-distance-model-of-mountain-pine-beetle-in-western-north-america}

In this chapter we describe an application of the multilinear Pearson
correlation coefficient in the science of molecular ecology.

\section{Introduction}\label{introduction-1}

For three random variables, \(X\), \(Y\), and \(Z\), we can consider the
proposition that \(X\) and \(Y\) are independent conditioned on \(Z\),
denoted by \(X \perp\!\!\!\!\perp Y | Z\). If \(Z\) is controlled for,
then \(X\) and \(Y\) should be found to be independent. Testing
\(X \perp\!\!\!\!\perp Y\) while stratifying by \(Z\) is an approach to
evaluating this proposition. Stratification can be done discretely by
grouping the values of \(X\) and \(Y\) into different levels of \(Z\),
and then estimating the measure dependence between \(X\) and \(Y\)
within those groupings. While binning data into a discrete partition can
introduce binning error, it can be an effective way of handling apparent
discontinuities in a data set. A second approach to stratification
called \textit{partialling} is to find continuous functions
\(X \approx f(Z)\) and \(Y \approx g(Z)\), and then examine whether
\([X-f(Z)] \perp\!\!\!\!\perp [Y-g(Z)]\). When \(f\) and \(g\) are
linear, and we compute the Pearson correlation coefficient on their
residuals, which has been called \emph{partial correlation} (Fisher
(1924)). Partialling with linear models will lead to the partial
correlation coinciding with the conditional correlation when the random
variables follow certain joint distributions such as the multivariate
normal distribution (Baba, Shibata, and Sibuya (2004)), but in general a
careful consideration of how the conditional expectation is being
modelled is needed. These two approaches to stratifying are compatible
in the sense that they can be used within the same analysis.

Multilinear Pearson correlation can be used to indicate multilinear
dependence. Since multilinear dependence is mutually exclusive with
independence, we can use multilinear Pearson correlation as evidence of
dependence. A multilinear Pearson correlation score of zero does not
imply independence however, so the inference is one-sided. Because the
multilinear Pearson correlation is defined partly in terms of
expectation operators, we can condition the correlation score on other
variables. In this way we can compute conditional correlations at
discrete levels of a random variable, or partial and
semi-partial\footnote{The distinction between a partial correlation and a semi-partial correlation pertains to whether the same set of variables are used to model $X$ as are used to model $Y$. In the semi-partial case one might consider  $X \ind Y | Z,U$ and find that $[X - f(Z)] \ind [Y -  g(U)]$ is true for $Z \neq U$. Even more complicated conditional independence exist such as $[X - f(Z)] \ind [Y -  g(Z,U)]$.}
correlation coefficients when we can model the conditional dependence
with functions. We can therefore use multilinear Pearson correlation
coefficients to estimate the existence of multilinear forms of
conditional independence. When a collection of propositions about
(in)dependence or conditional (in)dependence are tested with correlation
statistics, and those correlations seem to agree with the hypothetical
statements, we will call the propositions *correlationally sufficient.

Mountain pine beetle (\emph{Dendroctonus ponderosae}) (Figure
\ref{fig:pinebeetle}(a)) is a species of bark beetle native to western
Canada, the United States of America, and Mexico \cite{Furniss1977}. It
predates on a variety of tree species, and especially pine. According to
a manual published by the United States Department of Agriculture Forest
Service in 1977, mountain pine beetle had been observed attacking
various species of pine trees including \emph{Pinus contorta},
\emph{Pinus lambertiana}, \emph{Pinus monticola}, \emph{Pinus
ponderosa}, \emph{Pinus albicaulis}, \emph{Pinus aristata}, \emph{Pinus
balfouriana}, \emph{Pinus coulteri}, \emph{Pinus edulis}, \emph{Pinus
flexilis}, and \emph{Pinus monophylla} (Furniss and and (1977)). Within
British Columbia the most common species of host for mountain pine
beetle is lodgepole pine (\emph{Pinus contorta})(Burleigh (2014)). A
couple of indicators of mountain pine beetle attacks on trees include
pitch tubes (Figure \ref{fig:pinebeetle}(b)) which are a result of the
host tree response (Burleigh (2014)), and areas of dead (especially
pine) trees on a landscape (Figure \ref{fig:pinebeetle}(c)).
Approximately 18.3 million hectares of British Columbia were affected by
a mountain pine beetle epidemic that began in the early 1990's (Corbett
et al. (2015)). This left a large negative impact on the forest industry
of British Columbia, damaging over half of the merchantable pine in the
province, which has been forecasted to result in a 57.37 billion dollar
loss in GDP of the next few decades (Corbett et al. (2015)). The
epidemic disrupted the habitat of numerous species of birds and mammals
(Saab et al. (2014)). It is worthwhile to continue studying the mountain
pine beetle post-epidemic to understand how to prevent and mitigate
further economic and environmental disturbances from mountain pine
beetle and similar forest pests.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\includegraphics{index_files/mediabag/Pine_beetle_pitch_tu.jpg}

\subcaption{\label{}Mountain pine beetle specimen (Public Domain)}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\includegraphics{https:/inaturalist-open-data.s3.amazonaws.com/photos/8048768/original.jpg}

\subcaption{\label{}Pitch tubes caused by beetles boring into the bark
(CC BY-SA 3.0)}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\includegraphics{index_files/mediabag/Mountain_pine_beetle.jpg}

\subcaption{\label{}Mountain pine beetle damage in Rocky Mountain
National Park in 2012 (CC BY-SA 3.0)}
\end{minipage}%

\caption{\label{fig-pine-beetle}Mountain pine beetle have a substantial
impact on individual trees and landscapes.}

\end{figure}%

Isolation by distance (IBD) is a hypothesis that can be made about a
collection of populations (i.e.~a metapopulation) that will potentially
interbreed \cite{Wright1943}. Specifically, a metapopulation exhibits
IBD if its members are more genetically dissimilar the further apart
they are from each other, which is motivated by the notion that mere
distance between populations can serve as a gradual contribution to
reproductive isolation. Mountain pine beetle has been analyzed for
distinct populations of mountain pine beetle in
\cite{booneunpublished, samarasekera2012} using software such as
STRUCTURE \cite{Pritchard2000} and BAPS \cite{Corander2003}, and was
found to have a North-South clustering pattern \cite{samarasekera2012}.
Subclusters within the northern and southern clusters are also supported
\cite{samarasekera2012}. In \cite{booneunpublished} evidence of four
clusters (see Figure \ref{fig:geo_cluster_dist}(a)) was also found,
however \cite{samarasekera2012} considered the geographic scope of
western Canada while \cite{booneunpublished} considered the geographical
scope of western North America. After identifying populations of
mountain pine beetle, \cite{samarasekera2012} found evidence of IBD
using linear regression.\textbackslash{}

\begin{figure}[H]
    \centering
    \begin{tabular}{cc}
    (a) & (b)\\
        \includegraphics[scale=0.5]{./IBD_figures/lat_long_STRUCTURE_clusters.pdf} & \includegraphics[scale=0.5]{./IBD_figures/lat_long_200km_radius.pdf} \\
    \end{tabular}
    \caption{Geographic distribution of mountain pine beetle samples in western North America. (a) Geographic scatter plot with Gaussian kernel density estimated distributions  \cite{Rosenblatt1956, Parzen1962} stratified by genetic clusters identified by STRUCTURE in \cite{booneunpublished}. (b) Geographic scatter plot with bivariate Gaussian kernel density estimated distributions \cite{Rosenblatt1956, Parzen1962} and marginal histograms. Each sample point has a disk with a 200 km radius drawn around it.}
    
\end{figure}

A question that remains under-addressed is whether host selection is
independent from the genetic variation among mountain pine beetle
populations. Previous studies found that allozyme
\footnote{Two or more enzymes are \textit{allozymes} or \textit{alloenzymes} if they differ structurally, but not in terms of their biochemical function. This might be considered in a qualitative fashion as enzymes may not have the same enzyme activity level under equivalent chemical conditions.}
variation is confounded by geography \cite{Sturgeon1986, Langor1991}.
Namely that the isolation by distance effect observed in mountain pine
beetle may be confounding our understanding of dependence between host
selection and genetic variability of mountain pine beetle. One approach
is to perform an analysis of molecular variance conditioned on the
geographical distances being small. This approach was used in
\cite{booneunpublished} where they conditioned the maximum distance to
be 200 km (visualized in Figure \ref{fig:geo_cluster_dist}(b)), and did
not find statistically significant (\(\alpha=0.05\)) results for host
selection effects with this approach. Their choice of 200 km, and
whether the corrections they performed (see \cite{Weir1984}) were
sufficient to account for the unbalanced group sizes, may or may not
have affected the analysis outcome. Since no statistical procedure is
without assumptions, it is desirable to perform multiple analyses under
different assumptions to see if there is consilience (i.e.~convergence
of evidence).

In this application we considered whether there is correlational
sufficiency of certain IBD models stratified by clusters identified in
\cite{booneunpublished} using STRUCTURE. The first hypothesis was
whether the IBD models were correlationally sufficient in their
exogeneity. This term, \textit{exogenous}, comes from Economics. It is a
hypothesis that the errors of a model are independent of the explanatory
variables (Wooldridge (2009)). When this does not occur, it is an
indication that the model has been improperly specified. When a model is
not exogenous, it is called \textit{endogenous}. The second question was
whether the residuals of the IBD models appear to be independent of host
selection along with potential covariates related to temperature, phase
changes of water, and elevation. We also examine these hypotheses via
assessing correlational sufficiency.

In order to do this we first hypothesized a functional relationship
between the genetic dissimilarity and the geographical distance based on
existing literature. \cite{Peterson1998} summarizes slopes and
intercepts of IBD models for numerous insect species in which a linear
model between the log of gene flow and the log of geographic distances
were computed. This motivates the use of a linear model to begin with.
Similarly, \cite{Rousset1997} suggests the modelling
\(\frac{F_{ST}}{1- F_{ST}}\) against the log of the geographic
distances, which is also used in \cite{booneunpublished}. We adopt the
variable
\texttt{fst\_score}\footnote{We call it a 'score' rather than a distance because it does not appear to satisfy all of the axioms of a metric, namely the triangle inequality.}
in our analysis to be the pairwise \(F_{ST}\) as calculated in the
GenAlEx plugin for Excelâ¢ (Peakall and Smouse (2006)) to represent the
degree of genetic differentiation between populations.

\section{Dataset}\label{dataset}

Because we used the same dataset as Boone et al. (2022), the underlying
materials and methods are the same. However, since
\cite{booneunpublished} is currently unpublished we will give some
description the dataset's properties as they were provided to us.

The dataset as provided constituted a GenAlEx6.5 software analysis, a
summary of the sampled sites, and an extensive collection of
climate-related variables paired to the site locations. Two variables of
particular importance are the pairwise Fst scores as calculated by
GenAlEx6.5, and the geolocations in terms of latitude and longitude. The
geolocations were used in our analysis to calculate intersite distances
as the arc lengths on a sphere. The climate-related data contained
monthly and annual measurements related to temperature, pressure, and
precipitation.\textbackslash{}

We are grateful to Dr.~Brent Murray for provided this dataset to us
along with the \cite{booneunpublished} manuscript. We are also grateful
to the teams of people that participated in obtaining and curating such
a large and rich dataset as acknowledged in Boone et al. (2022).

\section{Data Analysis}\label{data-analysis}

Originally we had intended to perform equation-by-equation ordinary
least squares regression (EBE-OLS), but upon examination of the
histograms (as found in Figure \ref{fig:cluster_regression}) of
\texttt{fst\_score} and \texttt{ln(geo\_dist)} it became apparent that
normality and homoschedasticity assumptions were not met. Strong
violations of these assumptions imply that EBE-OLS is no longer
guaranteed to be unbiased or the most efficient estimator of the linear
regression parameters ((\textbf{ohnson2007?})). Instead we used EBE
iteratively-reweighted least squares (IRLS) (Lawson (1961)) with a Huber
loss (Huber (1964)) using the \texttt{statsmodels 0.13.1} Python package
to provide outlier-robust estimates of the regression parameters and
residuals (see Appendix sections \ref{sec:ibdmodelsummaries} and
\ref{sec:ibdevalplots} for model summaries and diagnostic plots). The
fitted linear models are plotted in each panel of Figure
\ref{fig:cluster_regression} along with the Pearson correlation,
coefficient of determination and a p-value computed with the
\texttt{scipy.stats.pearsonr} command (Virtanen et al. (2020)). This
command assumes a null hypothesis that the two variables follow a
bivariate normal distribution, implying that the correlation coefficient
follows a sampling density of
\(f(r) = \frac{(1 - r^2)^{\frac{n}{2} - 2}}{B(\frac{1}{2}, \frac{n}{2} - 1)}\)
where \(r\) is the Pearson correlation, \(n\) is the sample size, and
\(B\) is the beta function \cite{Student1908}. Therefore p-values on
Figure \ref{fig:cluster_regression} should be interpreted as the
two-tailed likelihood of the data under the assumption of being sampled
from this null distribution.

Beyond mere non-normality, the data distributions in Figure
\ref{fig:cluster_regression} show some unusual features. One is
exemplified by the NL vs SW plot, but possibly present to some degree in
other pairs such as NU vs SW and within SW. There appears to be more
than one grouping in the data even after clustering. This suggests that
the clustering is not fully accounting for the genetic distribution of
microsatellites. The second unusual feature is that some of the plots,
within NL, within NU, and within SE have skewed left tails along the
axis of log geographic distance. Lastly, the shape of the connected
components under visual inspection seem to be hinting at a bend in the
data such as in NL vs NU or within SE, which may be an indication that
the relationship between \texttt{fst\_score} and \texttt{ln(geo\_dist)}
is non-linear.

\begin{figure}[H]
    \centering
    \begin{tabular}{ccc}
        \tiny Within NL & \tiny NL vs NU &  \tiny  NL vs SW \\
        \includegraphics[scale=0.3]{./IBD_figures/IBD_jointplot_NL_vs_NL.pdf} & \includegraphics[scale=0.3]{./IBD_figures/IBD_jointplot_NL_vs_NU.pdf} & \includegraphics[scale=0.3]{./IBD_figures/IBD_jointplot_NL_vs_SW.pdf} \\
        \tiny Within NU & \tiny  NU vs SW & \tiny SE vs NL \\
         \includegraphics[scale=0.3]{./IBD_figures/IBD_jointplot_NU_vs_NU.pdf} & \includegraphics[scale=0.3]{./IBD_figures/IBD_jointplot_NU_vs_SW.pdf} & \includegraphics[scale=0.3]{./IBD_figures/IBD_jointplot_SE_vs_NL.pdf} \\
         \tiny SE vs NU &\tiny  Within SE & \tiny  SE vs SW \\
        \includegraphics[scale=0.3]{./IBD_figures/IBD_jointplot_SE_vs_NU.pdf} & \includegraphics[scale=0.3]{./IBD_figures/IBD_jointplot_SE_vs_SE.pdf} &
        \includegraphics[scale=0.3]{./IBD_figures/IBD_jointplot_SE_vs_SW.pdf}\\
        \tiny Within SW &  & \\
        \includegraphics[scale=0.3]{./IBD_figures/IBD_jointplot_SW_vs_SW.pdf} \\
    \end{tabular}
    \caption{Isolation-by-distance plots of pairwise Fst scores (\texttt{fst\_score}) vs the natural log of the geographic (shortest spherical arc) distance in kilometres (\texttt{ln(geo\_dist)}). The subplots are stratified by pairs of clusters identified by STRUCTURE. (NL) North Lower. (NU) North Upper. (SW) South Western. (SE) South Eastern.}
    
\end{figure}

We wanted to include some potential covariates from the climate data
that we were provided. We also wanted to evaluate whether we could
reduce the dataset down to a small number of dimensions without losing
much information about the original dataset. The multilinear correlation
methodology we wished to use can be computed on different combinations
of variables to learn about the dependence between particular
combination of variables, but with over 200 variables that entails over
\(2^{200} - 201\) possible combinations. Thus we were motivated to look
for ways to reduce the set of hypotheses to examine.\textbackslash{}

We performed a principal components analysis on the standardized scores
of the site-level data as part of an exploratory data analysis. Figure
\ref{fig:climatepca}(a) indicates that approximately 20-30 components
would be needed to be kept to preserve a large majority of the variation
in the data, suggested that the principal components are not efficient
at compressing the data. Figures \ref{fig:climatepca}(b) and
\ref{fig:climatepca}(c) confirms for us that the initial basis vectors
point in various directions on the plane spanned by the first two
principal components. Visual inspection of Figure \ref{fig:climatepca}
suggests the NU cluster and the SW are linearly separable on the plane,
and perhaps SE from SW, but most clusters do not appear to be easily
separated for other pairings.

\begin{figure}[H]
    \centering
    \begin{tabular}{cc}
    (a) & (b) \\
    \includegraphics[scale=0.45]{./climate_pca/elbow_plot_traiphil_pca.pdf} & \includegraphics[scale=0.38]{./climate_pca/triaphil_pca_plot.pdf}  \\
    \end{tabular}
    
    \begin{tabular}{c}
    (c) \\
    \includegraphics[scale=0.53]{./climate_pca/triaphil_pca_labels_plot.pdf}
    \end{tabular}
    \caption{PCA of site location data paired with climate data. (a) Scree plot of singular values with first component explaining approximately 17 percent of the total variance. (b) Biplot of variables with all sites visible, colour-coded by the clusters provided by \cite{booneunpublished}. Marginal Gaussian kernel density estimates are included \cite{Rosenblatt1956, Parzen1962}. (c) Close up of biplot with the variable labels added to the arrow tips.}
    
\end{figure}

To ameliorate the issue of too many candidate hypotheses, we took
another approach \textit{via} principal components analysis. Most of the
variables were explicitly about various annual, seasonal, and monthly
scores related to (1) temperature and (2) phase changes in water. We
annotated each of the climate variables as either having more to do
temperature, phase changes of water, or neither. We labelled the group
of variables related to temperature as \texttt{temperature} and the
group of variables to phase changes of water as \texttt{precipitation}.
By grouping the variables semantically we expected that variation in
their first principal component would be ostensibly related to their
semantic label, and that subsequent calculations of correlations with
these variables would also be ostensibly related to the semantic label.
Because we would not be computing the multilinear correlation on the
original variables tied to a single site, but rather on scores tied to
pairs of sites, we chose the absolute difference in these climate
variables as new features to satisfy this
requirement.\footnote{If we had continued with differences in climate variable scores instead, we would have had an issue with the scores being antisymmetric. Thus we would have had to choose between the upper vs lower triangle of the pairwise difference matrix, which seemed needlessly arbitrary.}

We also stratified the climate-related variables by pairs of clusters
before performing principal components analysis on them so that when we
computed the correlations between climate-related principal components
and the IBD model residuals, they would have the same discrete
conditioning as the pairwise \texttt{fst\_score} values. This approach
may also have reduced the amount of variation in the principal
components that was due to geographic distance, however we account for
the possibility of the principal components being partly explained by
geographic distance in our interpretation of the multilinear correlation
scores. The resulting first principal components's are labelled as
\texttt{Temperature\_PC} and \texttt{Precipitation\_PC}, and their
explained variance ratios within each pair of clusters is summarized in
Table \ref{tab:expvarpcaclimate}.

\begin{table}[H]
\centering
\caption{Explained variance ratios of first principal components of temperature and precipitation variable groups stratified by cluster pair.}
\begin{tabular}{lll}
\hline
Cluster Pair & Explained & Explained \\
 & Variance & Variance \\
  & Ratio & Ratio \\
 & \texttt{Temperature\_PC} & \texttt{Precipitation\_PC} \\
 \hline
Within NL & 0.58943674 &  0.22945464 \\
NL vs NU & 0.50543862 &  0.24115797 \\
NL vs SW & 0.53261144 &  0.23811749 \\
Within NU & 0.50923444 &  0.33917943 \\
NU vs SW & 0.53237635 &  0.27474814 \\
SE vs NL & 0.55363292 &  0.26139871 \\
SE vs NU & 0.54086253 &  0.28579717 \\
Within SE & 0.65255971 &  0.40494985 \\
SE vs SW & 0.4742398 &  0.25735202 \\
Within  SW & 0.6142342 &  0.34369012 \\
\hline
\end{tabular}

\end{table}

Table \ref{tab:expvarpcaclimate} shows that most cluster pairs had
around 50-60 percent total variance of their temperature differences
explained by \texttt{Temperature\_PC}, and about 20-30 percent total
variance of their water-related phase change differences explained by
\texttt{Precipitation\_PC}. Especially for the temperature variables,
this is a marked improvement over the approximately 17 percent total
variation explained by the first principal component in Figure
\ref{fig:climatepca}. While we have lost a lot of information with this
compression, it adds two potential covariates related to climate to the
analysis instead of over 200 potential covariates.\textbackslash{}

To asses the correlational sufficiency of the exogeneity of the IBD
models, we computed the pairwise correlation between \texttt{residuals}
and \texttt{ln(geo\_dist)} stratified by cluster pair. See Table
\ref{tab:endogencorrs} for a summary of these statistics. We did not
find any statistically significant (\(\alpha=0.05\)) correlations after
correction for multiple tests, however we noted that the pre-corrected
correlation between NL and SW was significant at this level. However,
the effect size of this correlation was only 0.14. Therefore the models
appear to be correlationally sufficient for exogeneity according to a
pairwise correlation analysis.\textbackslash{}

For the multilinear correlation analysis to estimate correlational
sufficiency we used IBD model residuals (\texttt{residuals}), host
selection distance
(\texttt{Host})\footnote{We operationalized host selection distance to be an indicator function. If the hosts were the same, we assigned a score of zero for \texttt{Host}. If they were different host species, we assigned 1 to the value of \texttt{Host}.},
geographic log-distance \linebreak (\texttt{ln(geo\_dist)}), the first
temperature-difference-related principal
component\linebreak (\texttt{Temperature\_PC}), and the first
phase-change-of-water-related principal
component\linebreak (\texttt{Precipitation\_PC}). With these six
variables there are 57 correlation hypotheses that could be tested on
each cluster pair, amounting to 570, however many of them are not
directly related to our research questions. And it would be desirable to
minimize the number of hypotheses tested to conserve statistical power.
Since we were interested in the correlational sufficiency of the IBD
models, we looked at a subset of correlations that always included
always included \texttt{redisuals} and \texttt{Host} among the
considered variables. This allowed us to compute only 16 correlation
coefficients per cluster pair, entailing the calculation of 160
correlation coefficients among the 10 pairs of clusters.\textbackslash{}

Within each cluster pair the multilinear Pearson correlations were
calculated with an estimated p-value using a permutation test using 10
000 resamplings, which tests a null hypothesis of exchangeability. As
cited by Serafino (2016), all independent and identically distributed
random variables are exchangeable. Thus a strongly inexchangeable
collection of random variables entails that the random variables are
either dependent and/or non-identical. This is a weak condition, but a
substantially-non-zero multilinear Pearson correlation score suggests
dependence anyway. Some of the p-values were zero because they were
observed zero times, however we know that there exists at least one
permutation of the columns (i.e.~the one we observed in the original
data) that could have been sampled to obtain a large enough score.
Therefore the p-values are somewhere inbetween zero and \(10^{-4}\). We
assigned such zero-scores to be \(10^{-4}\), taking on more type II
error while reducing type I error. Of the ten pairs of clusters, we
found statistically-significant (\(\alpha=0.05\)) correlations in six of
them. Using \texttt{hypernetx 1.2.3} we prepared Figure
\ref{fig:setcoveribscorrelation} to visualize which sets of variables
had statistically significant multilinear correlation, coloured by the
strength of the correlation score \cite{Praggastis2019}.

\begin{tcolorbox}[enhanced jigsaw, colbacktitle=quarto-callout-tip-color!10!white, bottomrule=.15mm, left=2mm, arc=.35mm, bottomtitle=1mm, coltitle=black, breakable, rightrule=.15mm, toptitle=1mm, opacityback=0, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Relationship Between IID, Exchangeable and Identical in Marginals}, colframe=quarto-callout-tip-color-frame, toprule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, colback=white]

As cited above, is a collection of random variables are IID then they
are exchangeble. Note that the converse is not true in general. A
collection of random variables might be exchangeable but not IID.

Furthermore, if a collection of random variables are exchangeable then
they will have the same marginal distribution up to labelling of the
variables. The converse is not true in general. It is possible to have a
collection of random variables which are identical in their marginal
distribution but are not exchangeable.

\end{tcolorbox}

\begin{figure}[H]
    \centering
    \begin{tabular}{cc}
    \tiny within NL & \tiny NL vs NU \\
    \includegraphics[scale=0.25]{./IBD_figures/hypergraph_NL_vs_NL.pdf} & \includegraphics[scale=0.25]{./IBD_figures/hypergraph_NL_vs_NU.pdf} \\
    \tiny NL vs SW & \tiny NU vs SW \\  
     \includegraphics[scale=0.25]{./IBD_figures/hypergraph_NL_vs_SW.pdf} & \includegraphics[scale=0.25]{./IBD_figures/hypergraph_NU_vs_SW.pdf} \\
    \tiny SE vs NL & \tiny SE vs NU \\   
      \includegraphics[scale=0.25]{./IBD_figures/hypergraph_SE_vs_NL.pdf} & \includegraphics[scale=0.25]{./IBD_figures/hypergraph_SE_vs_NU.pdf} \\
    \end{tabular}
    \caption{Set cover visualizations of multilinear Pearson correlation among sets of variables including \texttt{Host} and the \texttt{residuals} of the IBD linear regression models.}
    
\end{figure}

It is apparent from Figure \ref{fig:setcoveribscorrelation} that
\texttt{residuals} and \texttt{ln(geo\_dist)} appear to be mutually
dependent along with \texttt{Host} within each pair of clusters. Thus
the correlationally sufficient exogeneity inferred with the pairwise
Pearson correlations did not fully assess independence between the IBD
model residuals with the geographical distances when covariates such as
differences in host were considered. Rather these multilinear Pearson
correlation coefficients suggest that a linear model between
\texttt{fst\_scores} and \texttt{ln(geo\_dist)} does not render the
genetic distances as independent from \texttt{Host}. This leaves the
question of dependence between genetic distance and host selection
confounded by geography. This exemplifies how multilinear correlation
can be used selectively on different combinations of variables depending
on the question of interest.\textbackslash{}

We also noted from Figure \ref{fig:setcoveribscorrelation} that all six
variables occur in only four of the six pairs of clusters included all
of the variables. Namely that absolute differences in elevation
(\texttt{Elevation}) did not appear to be correlated be correlated with
\texttt{residuals} and \texttt{Host} within the within NL and NL vs NU
clusters. We speculate based on Figure \ref{fig:elevationdist} that the
spatial distribution of elevations might be more similar within and
between the northern clusters.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{./IBD_figures/elevation_scatter.pdf}
    \caption{Scatterplot of latitude vs longitude of samples of mountain pine beetle. The points are coloured by bins of elevation.}
    \label{fig:elevationdist}
\end{figure}

Table \ref{tab:shortcorrresults} summarizes the multilinear correlations
with absolute scores greater-than-or-equal-to 0.2. The rest of the
multilinear Pearson correlations are summarized in tables found in
appendix section \ref{sec:summarycorrhost}. Notably only a short list of
multilinear correlations make this list, and they are not particularly
strong effects on the scale of \([-1,1]\).

\begin{table}[H]
\caption{Condensed summary of permutation tests of multilinear Pearson correlations calculated on collections of variables that included host species and IBD residuals. The results are stratified by cluster pair. The absolute effects sizes are selected to be greater-than-or-equal-to 0.2.}
\tiny
\centering
\begin{tabular}{lllll}
\hline
Cluster Pair &                                                                    Variables &  P-Value$^*$ &     $R$ &  Holm-Sidak \\
 & & & & P-Value\\
\hline
    within NL &                                                            \texttt{Host}, \texttt{residuals} &  0.0001 &  0.295213 &    0.015873 \\
   NL vs SW &                                                            \texttt{Host}, \texttt{residuals} &  0.0001 &  0.245797 &    0.015873 \\
   NL vs SW &                                          \texttt{Precipitation\_PC}, \texttt{Host}, \texttt{residuals} &  0.0001 &  0.253056 &    0.015873 \\
   NU vs SW &                                                            \texttt{Host}, \texttt{residuals} &  0.0001 &  0.304575 &    0.015873 \\
   NU vs SW &                                            \texttt{Temperature\_PC}, \texttt{Host}, \texttt{residuals} &  0.0001 &  0.298889 &    0.015873 \\
   NU vs SW &                                          \texttt{Precipitation\_PC}, \texttt{Host}, \texttt{residuals} &  0.0001 &  0.300972 &    0.015873 \\
   NU vs SW &                                              \texttt{ln(geo\_dist)}, \texttt{Host}, \texttt{residuals} &  0.0001 &  0.206326 &    0.015873 \\
   NU vs SW &                         \texttt{Temperature\_PC}, \texttt{Precipitation\_PC}, \texttt{Host}, \texttt{residuals} &  0.0001 &  0.219195 &    0.015873 \\
   NU vs SW &                               \texttt{Precipitation\_PC},  \texttt{Elevation}, \texttt{Host}, \texttt{residuals} &  0.0001 & -0.213909 &    0.015873 \\
   SE vs NL &                          \texttt{Temperature\_PC}, \texttt{Precipitation\_PC}, \texttt{Host}, \texttt{residuals} &  0.0001 &  0.221663 &    0.015873 \\
   SE vs NU &                                                           \texttt{Host}, \texttt{residuals} &  0.0002 &  0.205100 &    0.023521 \\
   within SW & \texttt{Host}, \texttt{residuals} & 0.0778 & 0.383154 & 0.995944 \\
\hline
 * $\max (P, 10^{-4})$. & & & & \\
\end{tabular}

\end{table}

\section{Future Research}\label{future-research}

The main challenge with testing the conditional independence of host
selection and genetic diversity given the dependence on geography is
that the dependence between genetic variation and geographical
distribution is complicated. In order to account for this dependence, we
believe a combination of discrete stratification (i.e.~clustering or
similar methods) combined with continuous stratification
(i.e.~regression models) will be required. We offer some discussion of
how future research can improve on the modelling we presented in this
work.

\subsection{Variable Extraction and
Selection}\label{variable-extraction-and-selection}

The first aspect we will consider is which variables to use in an IBD
model.

For variables representing abstract ideas such as ``genetic
variability'', ``genetic differentiation'', ``genetic distance'',
``genetic structure'' we must extract variables the from original data
that operationalize our often-less-precise notions. A couple of the most
common genetic disimilarity scores include pairwise
Fst\footnote{We caution the reader that different functions are referred to as pairwise Fst. In this analysis we used Fst as calculated in the GenAlEx6.5 documentation.}
and Nei's standard genetic distance score. Neither of these functions
are distance functions (i.e.~metrics) in the mathematicians sense, but
they do have useful properties such as identity of indiscernibles and
non-negativity that intuitively capture the notion of a
\texttt{disimilarity\ score\textquotesingle{}.\ The\ property\ that\ they\ do\ not\ have\ is\ the\ triangle\ inequality,\ which\ can\ be\ intuitively\ stated\ as\ a}taking
a detour requires more distance' property. For example, the distance
between Prince George and Vancouver must be less-than-or-equal-to the
distance of going from Prince George to some other city (e.g.~Edmonton)
and then going to Vancouver. If we considered three genetic populations
\(A\),\(B\),\(C\), and a distance function between them, \(d\), then we
would find that \(d(A,B) \leq d(A,C) + d(C,B)\). The triangle inequality
is a property that can aid in interpretation of the results, and thus we
recommend performing a literature search for alternative functions to
Fst and Nei's score that satisfy this property.\textbackslash{}

In our model of IBD we assumed that the best way to quantify the
geographical distance between two points on the Earth is by the arc
length on the surface of a sphere. This is certainly an excellent
approximation given that the Earth's surface is even more accurately
modelled as an oblate spheroid with a small eccentricity, but we wish to
suggest a different motivation for selecting a measurement of distance
based on an abstract reason for why IBD should work. The underlying
rationale for IBD is that populations that are further away will be more
genetically dissimilar because the resources required to travel the
distance is a gradual reproductive barrier. This leads us to question
what other spatially-relevant factors serve as gradual barriers, and we
offer some speculations. While mountain pine beetle can potentially fly
for hours at a time \cite{Evenden2014}, and even be carried by the winds
at around 800 m above the tree canopy for 100 kilometers
\cite{Jackson2008}, it is not unreasonable to hypothesize that
topographical features can serve as gradual barriers. The first
hypothesis that we offer is to use surface distances that account for
the actual topographically details such as mountains and valleys. One
way to achieve this computationally would be to create a weighted graph
whose vertices are spatial locations and whose weights are the 3D
Euclidean distance. Figure \ref{fig:elevationsurface} provides a low
resolution example of such a surface using the provided georeferenced
mountain pine beetle data. For such a mesh model with high enough
resolution, one could consider the existence of paths through the graph
between two points. The shortest path problem is a classic problem from
computer science that is often solved by Dijkstra's algorithm
\cite{Dijkstra1959} or extensions of it such as A* (Doran and Michie
(1966), Dechter and Pearl (1985)). These shortest paths on along the
surface of Earth might better reflect the graduation of reproductive
isolation that exists between populations.

\begin{figure}
    \centering
    \includegraphics[scale=1]{./elevation_plots/view_1.pdf}
    \caption{Three-dimensional embedding of a two-dimensional Delaunay triangulation of geospatial coordinates where mountain pine beetle were sampled. The vertical component for each vertex is given by the elevation (in meters), and the elevation of the faces are linearly interpolated between vertices.}
    \label{fig:elevationsurface}
\end{figure}

Once this local impediment notion is realized for Euclidean distance,
one might imagine that more abstract network weights could be devised
based on other variables including local climate variables including
wind direction, local density of host species, the local density of
predators, and the geographical distribution of suitable habitat.
Building such surface models requires more work as variables are added
and as the resolution of the spatially-embedded graph increases, but it
may be an effective way of modelling the non-linear and
possibly-non-smooth effects of geography on IBD as mediated through
other variables.

\subsection{Discrete Stratification}\label{discrete-stratification}

Let us consider the discrete stratification first. It is evident from
the NL vs SW subplot in Figure \ref{fig:cluster_regression} that there
are separate connected components that were not identified by the
STRUCTURE software package, which in that particular subplot appear to
be separated primarily by \texttt{fst\_score} axis. However, visual
examination of the NU vs SW and SE vs NU subplots are more ambiguous on
the existence of additional connected components and by which
discriminant function (linear or non-linear) they would be separated by.
This motivates finding a new partition of the data. It is not
practically possible to exhaustively compute every possible partition of
a modestly-sized dataset because the number of partitions on a set is
equal to the Bell number for cardinality of that set. The Bell number of
a set of cardinality \(n\) is equivalent to adding up all the Stirling
numbers of the second kind up to and including \(n\) (Weisstein (1999)),
which can be written as

\[B_n = \sum_{k=0}^{n} \left[ \frac{1}{k!} \sum_{i=0}^{k} (-1)^i {k \choose i} (k-i)^n\right].\]

Clustering algorithms guide the data analyst toward particular choices
of partitions of the data, based on a choice of algorithm, parameters,
and (usually) a function quantifying similarity or dissimilarity. There
are a large number of hard clustering
algorithms\footnote{By a \textit{hard} clustering algorithm, we mean one in which a point is placed either in a cluster, or it is not in that cluster, but not both and not neither. In other words, a clustering algorithm find finds a partition of the data set. There are \textit{soft} clustering methods such as fuzzy C means that assign a weight to each pair of points and clusters suggesting 'how much membership' a point has to a given set (@Dunn1973}).
including K-means (Steinhaus (1957)), affinity propagation (Frey and
Dueck (2007)), mean-shift (Comaniciu and Meer (2002)), spectral
clustering (Shi and Malik (2000)), Ward hierarchical clustering (Ward
(1963)), density-based spatial clustering of applications with noise
(DBSCAN) (Ester et al. (1996)), ordering points to identify the
clustering structure (OPTICS) (Ankerst et al. (1999)), Gaussian mixture
models (GMM's; see \cite{pedregosa2011} for examples), and balanced
iterative reducing and clustering using hierarchies (BIRCH)
\cite{Zhang1996}. The Scikit-Learn Python package provides a useful
comparison and contrast of the tradeoffs among these methods in terms of
(hyper)parameters, scalability, and assumptions made by each algorithm
\cite{pedregosa2011}. Including aforementioned tools such as STRUCTURE
\cite{Pritchard2000} and BAPS \cite{Corander2003}, there are other tools
in the literature as cited by \cite{Corander2008} which are aimed at
modelling genetic structure. We suspect that many of these approaches
will also require choices of hyperparameters. In our opinion, there does
not exist a clustering algorithm that is suitable for all use cases and
data sets. Therefore we recommend that a quantitative and qualitative
exploration of which clustering algorithms best suite this
microsatellite data be undergone in future research.

\subsection{Continuous Stratification}\label{continuous-stratification}

Now let us consider the continuous stratification approach. After
stratifying the data based on the cluster assignment, we can further
substratify by a continuous function. In this thesis we tested a na"ive
hypothesis that the IBD effects between the Fst score and geographic
log-distance would be linear based on a prevalence of such models in the
literature. As shown in Figure \ref{fig:cluster_regression}, the
assumption of a linear relationship between the pairwise Fst scores and
the natural logarithm of the geographic (i.e.~spherical arc-length)
distance was poor-to-moderate in fit. The lack of excellent fit could be
due to multiple factors. As mentioned above, some of the lack of model
success may be attributable to not specifying the right strata. However,
in subplots within NU, within SE, and within SW there appears to be
substantial influence of (visually-estimated) ouliers even with the use
of a Huber loss function to robustify the training. We suspect that the
relationship between pairwise Fst scores and the geographic log-distance
are non-linear based on visual inspection of the plots in our analysis,
but we also believe there are first-principles reasons to expect
non-linearity when we consider populations that are extremely close or
extremely far away. When populations are relatively close compared to
their mobility, we might expect that such distances explain very little
genetic variation because they do not serve as a barrier to
reproduction. The other extreme is when the populations are already so
far away that they are very rarely interbreeding, in which case moderate
variations in their geographical distance may have a smaller effect than
if they were only moderately far away compared to their mobility. The
notion that IBD effects have smaller sensitivity at very small distances
and very large distances, but with a more sensitive range of distances
in-between, suggests non-linearities such as can be found with S-shaped
curves. Such curves are common in nature, from population dynamics to
levels of gene transcription. Some of the S-shaped curves that might be
used in future research include the Verhulst model (BacaÃ«r (2011)),
monod function (Monod (1949)), Gomperz function (Gomperz (1825)), Von
Bertalanffy's equation (Pauly and Morgan (1987)), Richard's curve
(i.e.~the generalized logistic growth function) (Richards (1959)), and
the hyperbolastic functions of types I, II, and III (Tabatabai,
Williams, and Bursac (2005)).

\section{Conclusions}\label{conclusions}

In this application we showed how multilinear Pearson correlation can be
used in model evaluation. The existence of measure dependence between
the residuals of the IBD models with geographical distance after
partialling suggests that the models tested here are not correlationally
sufficient for the exogeneity assumption. Our results suggest that there
may be measure dependence between differences in host choice and
pairwise Fst scores, however these correlations are still confounded by
geographical distance. We provided multiple avenues for improving the
IBD models in future research, and exemplified how correlational
sufficiency can be used in future model evaluations.

\bookmarksetup{startatroot}

\chapter{Discussion and Conclusions}\label{discussion-and-conclusions}

In this final chapter we discuss what was accomplished in this thesis,
the limitations of this work, and potential areas for further research.

The Trinity of Covariation is a guiding metaphor for the mathematical
notions developed in this work, with the core motivation that studying
coordinated changes in structure is a way of aggregating information
about systems. We sought to construct more precise mathematical notions
that in some sense exemplify or instantiate this metaphor. The
instantiations of this metaphor developed in this thesis include
multilinear correlations, Nightingale's correlation, inner correlations,
Agnesian operators, and grade entropies.

Multilinear correlations, Nightingales's correlation, and inner
correlation were all attempts to generalize correlation from its usage
in introductory statistics by accommodating multiple variables.
Nightingale's correlation focused on a generalization of norms, and
inner correlation was based on a generalization of inner products. These
functions are all statistics, and because statistics are functions of
random variables it is also the case that statistics are themselves
random variables. Any random variable has a distribution function, even
if it is degenerate. In most practical cases we suspect that the
distribution will not be degenerate. There is not an easy way to derive
what the distribution of a statistic should be, even when a joint
probability model for the input variables is given. While the
distribution of summations of random variables can be approached with
convolutions or Fourier transforms, the distribution of products of
random variables are less easily obtained. Existing methods often
require an assumption of measure independence, which is precisely one of
properties that we would like to infer rather than assume. It is a
further difficulty to analytically determine whether an estimator is
biased or
consistent\footnote{An estimator is consistent if-and-only-if its estimate equals the target parameter in the population as the sample size tends to infinity.}
without having its sampling distribution. Future research should examine
approaches for considering sums of products of random variables in order
to obtain distributions that do not assume independence.

Another limitation of some of the statistics defined in this work is the
requirement that higher moments exist. Fortunately many commonly-used
distributions have finite moments of all positive order. In some cases
it may be possible to substitute expectations with the principal Cauchy
values of the
expectations\footnote{For example, the principal Cauchy value of the expectation of a zero-centered univariate Cauchy random variable is zero even though the first moment is not defined.},
or substitue expectations with medians which are always defined for a
random variable.

The work in this thesis focuses on considering hypotheses about
particular combinations of variables. This expressiveness leads to an
exponentially large class of statistical hypotheses on any finite
collection of random variables. If the particular combinations of
variables can be used to test specific hypotheses, then immediately we
have an exponential complexity (or worse) problem. When it comes to
hypothesis testing there is the problem of inflated family-wise error
rate that comes from performing many Fisher-Neyman hypothesis
tests.\footnote{Fisher-Neyman hypothesis testing is what is often taught in statistics courses as \textit{hypothesis testing}.}
While this can be countered with multiple correction methods such as
Benjamini-Hochberg corrections, such corrections also result in lower
statistical power. Future research should investigate how to usefully
search the space of possible hypotheses.

The Agnesian operators bring the Trinity of Covariation into the realm
of calculus by quantifing coordinated changes that are either
instantaneous, implicit, or accumulated over an interval over some
parameter. Future research is warranted to explore how the Agnesian
operator can be applied in areas of mathematics and science that are
already using calculus, or models amenable to calculus. Some of these
subjects potentially include multivariable calculus, differential
geometry, linear algebra, multilinear algebra, and differential
equations. The Agnesian operators do not make use of mathematical
statistics, however they may find application in the subject of
stochastic differential equations.

Grade entropies and pseudograde entropies allow us to quantify the
totality of partial orders. In the case of grade entropies for product
orders, we learn about the comonotonicity of a collection of variables
as they change value from observation-to-observation. Partial orders are
ubiquitous in mathematics and the natural sciences. Some examples of
partial orders include organizational trees of corporations,
evolutionary trees, causal graphs \cite{Pearl2009}, assembly histories
\cite{Marshall2021}, social hierachies, and the dependency of
subroutines in a concurrent computer program.

A product of this thesis is the ConAction Python package. It provides an
easy-to-install and easy-to-use way of applying the mathematical notions
developed in this thesis. This initial release has been tested for
correctness and performance, documented online and in code, and there
exists opportunities for improvements \textit{via} user feedback on
GitHub.

We were able to evaluate the correlational sufficiency of a classic
isolation-by-distance model on mountain pine beetle population data in
western North America, and discovered that there exists a dependence
between genetic disimilarity and geographic distance left to be
explained in the presence of climate-related covariates. We gave
suggestions in the previous chapter on how to further explore this,
including

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  trying different clustering algorithms,
\item
  using metrics that satisfy the triangle inequality, and
\item
  using non-linear regression models that have vanishing
  isolation-by-distance effects for both large and small distances.
\end{enumerate}

We also suspect that using geographical distance as arc lengths on a
sphere may be less informative than distances over land that consider
topography. Rather, the biological paths (i.e.~the paths that an
organism actually takes or tends to take) should be more useful in
estimating isolation-by-distance effects but are also multivariate and
more difficult to estimate.

While we were able to apply the multilinear Pearson correlation
coefficient to a real research problem, there is a large number of
opportunities to explore applications of the ideas developed in this
thesis. It can be used to detect multilinear dependence in complex
systems, but it can also be used to check the correlational sufficiency
of models throughout the sciences.

In summary, this thesis provides fundamental motivations, mathematical
tools, software tools, and a demonstration of applying these tools. In
order to tackle complexity in the formal and empirical sciences, we must
have a diverse armamentarium. This thesis is a contribution to that
armamentarium.

\bookmarksetup{startatroot}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-Amershi1985}
Amershi, Amin H. 1985. {``A Complete Analysis of Full Pareto Efficiency
in Financial Markets for Arbitrary Preferences.''} \emph{The Journal of
Finance} 40 (4): 1235--43.
\url{https://doi.org/10.1111/j.1540-6261.1985.tb02374.x}.

\bibitem[\citeproctext]{ref-Ankerst1999}
Ankerst, Mihael, Markus M. Breunig, Hans-peter Kriegel, and JÃ¶rg Sander.
1999. {``OPTICS: Ordering Points to Identify the Clustering
Structure.''} In, 49--60. ACM Press.

\bibitem[\citeproctext]{ref-Athreya2006}
Athreya, Krishna B, and Soumen N Lahiri. 2006. \emph{Measure Theory and
Probability Theory}. 2006th ed. Springer Texts in Statistics. New York,
NY: Springer.

\bibitem[\citeproctext]{ref-Aubin2014}
Aubin, Jean-Pierre. 2014. \emph{Mathematical Methods of Game and
Economic Theory}. 2nd ed. Studies in Mathematics and Its Applications.
North-Holland.

\bibitem[\citeproctext]{ref-Azevedo2009}
Azevedo, Frederico A. C., Ludmila R. B. Carvalho, Lea T. Grinberg, JosÃ©
Marcelo Farfel, Renata E. L. Ferretti, Renata E. P. Leite, Wilson Jacob
Filho, Roberto Lent, and Suzana Herculano-Houzel. 2009. {``Equal Numbers
of Neuronal and Nonneuronal Cells Make the Human Brain an Isometrically
Scaled-up Primate Brain.''} \emph{The Journal of Comparative Neurology}
513 (5): 532--41. \url{https://doi.org/10.1002/cne.21974}.

\bibitem[\citeproctext]{ref-Baba2004}
Baba, Kunihiro, Ritei Shibata, and Masaaki Sibuya. 2004. {``Partial
Correlation and Conditional Correlation as Measures of Conditional
Independence.''} \emph{Blackwell Publishing Asia Pty Ltd.} 46 (4):
657--64. \url{https://doi.org/10.1111/j.1467-842x.2004.00360.x}.

\bibitem[\citeproctext]{ref-Bacar2011}
BacaÃ«r, Nicolas. 2011. {``Verhulst and the Logistic Equation (1838).''}
In \emph{A Short History of Mathematical Population Dynamics}, 35--39.
Springer London. \url{https://doi.org/10.1007/978-0-85729-115-8_6}.

\bibitem[\citeproctext]{ref-booneunpublished}
Boone, Celia K., Kirsten M. Thompson, Philippe Henry, and Brent W.
Murray. 2022. {``Host Use Does Not Drive Genetic Structure of Mountain
Pine Beetles in Western North America,''} June.
\url{https://doi.org/10.1101/2022.06.28.498011}.

\bibitem[\citeproctext]{ref-bravais1844}
Bravais, Auguste. 1844. \emph{Analyse Mathematique Sur Les Probabilites
Des Erreurs de Situation d'un Point}. Paris Royal Printing.
\url{https://books.google.ca/books?id=PJl-mgEACAAJ}.

\bibitem[\citeproctext]{ref-burleigh2014}
Burleigh, Jennifer. 2014. \emph{Field Guide to Forest Damage in British
Columbia}. Victoria, B.C: Ministry of Forests, Lands; Natural Resource
Operations.

\bibitem[\citeproctext]{ref-Comaniciu2002}
Comaniciu, D., and P. Meer. 2002. {``Mean Shift: A Robust Approach
Toward Feature Space Analysis.''} \emph{{IEEE} Transactions on Pattern
Analysis and Machine Intelligence} 24 (5): 603--19.
\url{https://doi.org/10.1109/34.1000236}.

\bibitem[\citeproctext]{ref-cook2004}
Cook, Mathew. 2004. {``Universality in Elementary Cellular Automata.''}
\emph{Complex Systems}, 1--40.

\bibitem[\citeproctext]{ref-Corbett2015}
Corbett, L. J., P. Withey, V. A. Lantz, and T. O. Ochuodho. 2015. {``The
Economic Impact of the Mountain Pine Beetle Infestation in British
Columbia: Provincial Estimates from a {CGE} Analysis.''} \emph{Forestry}
89 (1): 100--105. \url{https://doi.org/10.1093/forestry/cpv042}.

\bibitem[\citeproctext]{ref-Thomas2006}
Cover, Thomas M., and Joy A Thomas. 2006. \emph{Elements of Information
Theory}. 2nd ed. Nashville, TN: John Wiley \& Sons.

\bibitem[\citeproctext]{ref-Dechter1985}
Dechter, Rina, and Judea Pearl. 1985. {``Generalized Best-First Search
Strategies and the Optimality of a*.''} \emph{Journal of the {ACM}} 32
(3): 505--36. \url{https://doi.org/10.1145/3828.3830}.

\bibitem[\citeproctext]{ref-DeWeerdt2019}
DeWeerdt, Sarah. 2019. {``How to Map the Brain.''} \emph{Nature Outlook}
571 (7766): S6--8. \url{https://doi.org/10.1038/d41586-019-02208-0}.

\bibitem[\citeproctext]{ref-Doran1966}
Doran, J. E., and D. Michie. 1966. {``Experiments with the Graph
Traverser Program.''} \emph{Proceedings of the Royal Society of London.
Series A. Mathematical and Physical Sciences} 294 (1437): 235--59.
\url{https://doi.org/10.1098/rspa.1966.0205}.

\bibitem[\citeproctext]{ref-Emmerich2018}
Emmerich, Michael T. M., and AndrÃ© H. Deutz. 2018. {``A Tutorial on
Multiobjective Optimization: Fundamentals and Evolutionary Methods.''}
\emph{Natural Computing} 17 (3): 585--609.
\url{https://doi.org/10.1007/s11047-018-9685-y}.

\bibitem[\citeproctext]{ref-Ester1996}
Ester, Martin, Hans-Peter Kriegel, JÃ¶rg Sander, and Xiaowei Xu. 1996.
{``A Density-Based Algorithm for Discovering Clusters in Large Spatial
Databases with Noise.''} In \emph{KDD}.

\bibitem[\citeproctext]{ref-fisher1924}
Fisher, Ronald Aylmer. 1924. {``The Distribution of the Partial
Correlation Coefficient.''} \emph{Metron} 3: 329--32.
\url{https://digital.library.adelaide.edu.au/dspace/handle/2440/15182}.

\bibitem[\citeproctext]{ref-Frey2007}
Frey, Brendan J., and Delbert Dueck. 2007. {``Clustering by Passing
Messages Between Data Points.''} \emph{Science} 315 (5814): 972--76.
\url{https://doi.org/10.1126/science.1136800}.

\bibitem[\citeproctext]{ref-Furniss1977}
Furniss, R. L., and V. M. Carolin and. 1977. \emph{Western Forest
Insects /}. Dept. of Agriculture, Forest Service :
\url{https://doi.org/10.5962/bhl.title.131875}.

\bibitem[\citeproctext]{ref-galton1888}
Galton, Francis. 1888. {``Co-Relations and Their Measurement, Chiefly
from Anthropometric Data.''} \emph{Proceedings of the Royal Society of
London} 45: 135--45. \url{http://www.jstor.org/stable/114860}.

\bibitem[\citeproctext]{ref-Gomperz1825}
Gomperz, Benjamin. 1825. {``{XXIV}. On the Nature of the Function
Expressive of the Law of Human Mortality, and on a New Mode of
Determining the Value of Life Contingencies. In a Letter to Francis
Baily, Esq. F. R. S. {\&}c.''} \emph{Philosophical Transactions of the
Royal Society of London} 115 (December): 513--83.
\url{https://doi.org/10.1098/rstl.1825.0026}.

\bibitem[\citeproctext]{ref-Greub1978}
Greub, Werner. 1978. \emph{Multilinear Algebra}. Springer New York.
\url{https://doi.org/10.1007/978-1-4613-9425-9}.

\bibitem[\citeproctext]{ref-Hitchcock2021}
Hitchcock, Christopher, and MiklÃ³s RÃ©dei. 2021. {``{Reichenbach's Common
Cause Principle}.''} In \emph{The {Stanford} Encyclopedia of
Philosophy}, edited by Edward N. Zalta, {S}ummer 2021.
\url{https://plato.stanford.edu/archives/sum2021/entries/physics-Rpcc/};
Metaphysics Research Lab, Stanford University.

\bibitem[\citeproctext]{ref-Hotelling1953}
Hotelling, Harold. 1953. {``New Light on the Correlation Coefficient and
Its Transforms.''} \emph{Journal of the Royal Statistical Society:
Series B (Methodological)} 15 (2): 193--225.
\url{https://doi.org/10.1111/j.2517-6161.1953.tb00135.x}.

\bibitem[\citeproctext]{ref-Houwer2018}
Houwer, Jan De. 2018. {``Eelen 1980 Classical Conditioning.''}
\emph{Preprint}, May. \url{https://doi.org/10.31234/osf.io/vp5cu}.

\bibitem[\citeproctext]{ref-Huber1964}
Huber, Peter J. 1964. {``Robust Estimation of a Location Parameter.''}
\emph{The Annals of Mathematical Statistics} 35 (1): 73--101.
\url{https://doi.org/10.1214/aoms/1177703732}.

\bibitem[\citeproctext]{ref-Jaynes1965}
Jaynes, E. T. 1965. {``Gibbs Vs Boltzmann Entropies.''} \emph{American
Journal of Physics} 33 (5): 391--98.
\url{https://doi.org/10.1119/1.1971557}.

\bibitem[\citeproctext]{ref-Karkaria2020}
Karkaria, Behzad D., Neythen J. Treloar, Chris P. Barnes, and Alex J. H.
Fedorec. 2020. {``From Microbial Communities to Distributed Computing
Systems.''} \emph{Frontiers Bioengineering Biotechnology} 8 (July).
\url{https://doi.org/10.3389/fbioe.2020.00834}.

\bibitem[\citeproctext]{ref-Lambert2002}
Lambert, Frank L. 2002. {``Disorder - a Cracked Crutch for Supporting
Entropy Discussions.''} \emph{Journal of Chemical Education} 79 (2):
187. \url{https://doi.org/10.1021/ed079p187}.

\bibitem[\citeproctext]{ref-langford2001}
Langford, Eric, Neil Schwertman, and Margaret Owens. 2001. {``Is the
Property of Being Positively Correlated Transitive?''} \emph{The
American Statistician} 55 (4): 322--25.
\url{https://doi.org/10.1198/000313001753272286}.

\bibitem[\citeproctext]{ref-Lawson1961}
Lawson, Charlse Lawrence. 1961. {``Contribution to the Theory of Linear
Least Maximum Approximations.''} PhD thesis, University of California at
Los Angeles.

\bibitem[\citeproctext]{ref-McKerns2010}
McKerns, Michael M., and Michael A. G. Aivazis. 2010. {``Pathos: A
Framework for Heterogenous Computing.''} \emph{GitHub}.
\url{https://uqfoundation.github.io/project/pathos}.

\bibitem[\citeproctext]{ref-McKerns2011}
McKerns, Michael M., Leif Strand, Tim Sullivan, Alta Fang, and Michael
A. G. Aivazis. 2011. {``Building a Framework for Predictive Science.''}
\emph{Proceedings of the 10th Python in Science Conference}.

\bibitem[\citeproctext]{ref-Mill1843}
Mill, John Stuart. 1843. \emph{A System of Logic, Ratiocinative and
Inductive}. Cambridge University Press.
\url{https://doi.org/10.1017/cbo9781139149839}.

\bibitem[\citeproctext]{ref-miller2013}
Miller, Michael. 2013. \emph{Mathematics and Statistics for Financial
Risk Management}. Hoboken, NJ: John Wiley \& Sons.

\bibitem[\citeproctext]{ref-Monod1949}
Monod, Jacques. 1949. {``{The} {Growth} {of} {Bacterial} {Cultures}.''}
\emph{Annual Review of Microbiology} 3 (1): 371--94.
\url{https://doi.org/10.1146/annurev.mi.03.100149.002103}.

\bibitem[\citeproctext]{ref-Nantomah2017}
Nantomah, Kwara. 2017. {``Generalized HÃ¶lder's and Minkowski's
Inequalities for Jackson'sq-Integral and Some Applications to the
Incompleteq-Gamma Function.''} \emph{Abstract and Applied Analysis}
2017: 1--6. \url{https://doi.org/10.1155/2017/9796873}.

\bibitem[\citeproctext]{ref-Olkin1958}
Olkin, Ingram, and John W. Pratt. 1958. {``Unbiased Estimation of
Certain Correlation Coefficients.''} \emph{The Annals of Mathematical
Statistics} 29 (1): 201--11.
\url{https://doi.org/10.1214/aoms/1177706717}.

\bibitem[\citeproctext]{ref-pauly1987}
Pauly, D., and G. R. Morgan. 1987. \emph{Length-Based Methods in
Fisheries Research}. Makati, Metro Manila, Philippines Safat, Kuwait:
International Center for Living Aquatic Resources Management Kuwait
Institute for Scientific Research.

\bibitem[\citeproctext]{ref-PEAKALL2006}
Peakall, Rod, and Peter E. Smouse. 2006. {``Genalex 6: Genetic Analysis
in Excel. Population Genetic Software for Teaching and Research.''}
\emph{Molecular Ecology Notes} 6 (1): 288--95.
\url{https://doi.org/10.1111/j.1471-8286.2005.01155.x}.

\bibitem[\citeproctext]{ref-Pearson1895}
Pearson, Karl. 1895. {``Note on Regression and Inheritance in the Case
of Two Parents.''} \emph{Proceedings of the Royal Society of London
Series I} 58 (January): 240--42.

\bibitem[\citeproctext]{ref-piovani2007}
Piovani, Juan Ignacio. 2007. {``The Historical Construction of
Correlation as a Conceptual and Operative Instrument for Empirical
Research.''} \emph{Quality {\&} Quantity} 42 (6): 757--77.
\url{https://doi.org/10.1007/s11135-006-9066-y}.

\bibitem[\citeproctext]{ref-pypi}
PyPI. 2003. {``Python Package Index - PyPI.''} Python Software
Foundation. \url{https://pypi.org/}.

\bibitem[\citeproctext]{ref-Richards1959}
Richards, F. J. 1959. {``A Flexible Growth Function for Empirical
Use.''} \emph{Journal of Experimental Botany} 10 (2): 290--301.
\url{https://doi.org/10.1093/jxb/10.2.290}.

\bibitem[\citeproctext]{ref-Rossum2001}
Rossum, Guido van, Barry Warsaw, and Nick Coghlan. 2001. {``Style Guide
for {Python} Code.''} PEP 8. Python Software Foundation.
\url{https://www.python.org/dev/peps/pep-0008/}.

\bibitem[\citeproctext]{ref-Saab2014}
Saab, Victoria A., Quresh S. Latif, Mary M. Rowland, Tracey N. Johnson,
Anna D. Chalfoun, Steven W. Buskirk, Joslin E. Heyward, and Matthew A.
Dresser. 2014. {``Ecological Consequences of Mountain Pine Beetle
Outbreaks for Wildlife in Western North American Forests.''}
\emph{Forest Science} 60 (3): 539--59.
\url{https://doi.org/10.5849/forsci.13-022}.

\bibitem[\citeproctext]{ref-Serafino2016}
Serafino, Loris. 2016. {``On the de Finetti's Representation Theorem: An
Evergreen (and Often Misunderstood) Result at the Foundation of
Statistics.''} \url{http://philsci-archive.pitt.edu/12059/}.

\bibitem[\citeproctext]{ref-Shi2000}
Shi, Jianbo, and J. Malik. 2000. {``Normalized Cuts and Image
Segmentation.''} \emph{IEEE Transactions on Pattern Analysis and Machine
Intelligence} 22 (8): 888--905. \url{https://doi.org/10.1109/34.868688}.

\bibitem[\citeproctext]{ref-Stanley2011}
Stanley, Richard P. 2011. \emph{Cambridge Studies in Advanced
Mathematics Enumerative Combinatorics: Series Number 49: Volume 1}. 2nd
ed. Cambridge, England: Cambridge University Press.

\bibitem[\citeproctext]{ref-steinhaus1957}
Steinhaus, Hugo. 1957. {``Sur La Division Des Corps MatÃ©riels En
Parties.''} \emph{Bulletin L'AcadÃ©mie Polonaise Des Science} 4 (12):
801--4.
\url{https://scirp.org/reference/referencespapers.aspx?referenceid=2408792}.

\bibitem[\citeproctext]{ref-Sykora2009}
Sykora, Stanislav. 2009. {``Mathematical Means and Averages: Basic
Properties.''} \emph{Stan's Library}, no. Volume III (July).
\url{https://doi.org/10.3247/sl3math09.001}.

\bibitem[\citeproctext]{ref-Tabatabai2005}
Tabatabai, Mohammad, David Williams, and Zoran Bursac. 2005.
{``Hyperbolastic Growth Models: Theory and Application.''}
\emph{Theoretical Biology and Medical Modelling} 2 (1): 14.
\url{https://doi.org/10.1186/1742-4682-2-14}.

\bibitem[\citeproctext]{ref-Virtanen2020}
Virtanen, Pauli, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler
Reddy, David Cournapeau, Evgeni Burovski, et al. 2020. {``{{SciPy} 1.0:
Fundamental Algorithms for Scientific Computing in Python}.''}
\emph{Nature Methods} 17: 261--72.
\url{https://doi.org/10.1038/s41592-019-0686-2}.

\bibitem[\citeproctext]{ref-Ward1963}
Ward, Joe H. 1963. {``Hierarchical Grouping to Optimize an Objective
Function.''} \emph{Journal of the American Statistical Association} 58
(301): 236--44. \url{https://doi.org/10.1080/01621459.1963.10500845}.

\bibitem[\citeproctext]{ref-weisstein}
Weisstein, Eric W. 1999. {``Wolfram MathWorld: The Web's Most Extensive
Mathematics Resource. From MathWorld--a Wolfram Web Resource.''} Wolfram
Research, Inc. \url{https://mathworld.wolfram.com/}.

\bibitem[\citeproctext]{ref-wolfram2002}
Wolfram, Stephen. 2002. \emph{A New Kind of Science}. Champaign, IL:
Wolfram Media.

\bibitem[\citeproctext]{ref-wooldridge2009}
Wooldridge, Jeffrey. 2009. \emph{Introductory Econometrics : A Modern
Approach}. Mason, OH: South Western, Cengage Learning.

\bibitem[\citeproctext]{ref-wright1921}
Wright, Sewall. 1921. {``Correlation and Causation.''} \emph{Journal of
Agriculture Research} 20: 557--85.

\end{CSLReferences}

\bookmarksetup{startatroot}

\chapter*{Citation}\label{citation}
\addcontentsline{toc}{chapter}{Citation}

\markboth{Citation}{Citation}

If you want to cite this work you can use the following:

\begin{verbatim}
@mastersthesis{seilisthesis2022,
  author  = "Galen Seilis",
  title   = "ConAction: Efficient Implementations and Applications of Functions Inspired by the Trinity of Covariation",
  school  = "University of Northern British Columbia",
  year    = "2022",
  address = "3333 University Way, Prince George, British Columbia, V2N 4Z9, Canada",
  month   = "September",
  doi = 10.24124/2022/59312,
  url = https://doi.org/10.24124/2022/59312
}
\end{verbatim}




\end{document}
